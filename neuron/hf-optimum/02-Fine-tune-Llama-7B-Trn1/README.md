#  Dolly ë°ì´í„° ì…‹ì„ ì´ìš©í•˜ì—¬ trn1.32xlarge ì—ì„œ Llama-2-7B íŒŒì¸ íŠœë‹í•˜ê¸°

Last Update: Feb 25, 2024

---

ì´ íŠœí† ë¦¬ì–¼ì€ trn1.32xlarge ë¡œ Llama-2-7B ëª¨ë¸ì„ íŒŒì¸íŠœë‹í•˜ê³ , ì¶”ë¡ ì„ ìœ„í•œ ê°€ì´ë“œ ì…ë‹ˆë‹¤. ì•„ë˜ì˜ ë…¸íŠ¸ë¶ì„ ì‹¤í–‰ í•˜ê¸° ìœ„í•¨ ì…ë‹ˆë‹¤. 
- [Fine-tune Llama on AWS Trainium](https://github.com/huggingface/optimum-neuron/blob/main/notebooks/text-generation/llama2-7b-fine-tuning.ipynb)

ì‹¤í–‰ì„ ìœ„í•´ì„œ ìœ„ì˜ í•´ë‹¹ ë…¸íŠ¸ë¶ì„ ì‹¤í–‰ í•˜ì‹œê³ , ì—¬ê¸°ì„œëŠ” ì¤‘ìš” ê°€ì´ë“œ ë° ì¼ë¶€ ì½”ë“œ ì‹¤í–‰ ê²°ê³¼ë¥¼ ë³´ì—¬ ë“œë¦½ë‹ˆë‹¤. 


- [../01-Chatbot-Llama-2-13B-Inf2/README.md](https://huggingface.co/docs/optimum-neuron/tutorials/llama2-13b-chatbot)
<br>
<p> 

# 1. ê¸°ë³¸ ì‚¬ì „ ë‹¨ê³„
## Quota ì¤€ë¹„ 
- ë¨¼ì € AWS ê³„ì •ì— ì•„ë˜ì— í•´ë‹¹ë˜ëŠ” ê¸°ë³¸ì ì¸ [Quota](https://docs.aws.amazon.cohttps://awsdocs-neuron.readthedocs-hosted.com/en/latest/general/arch/neuron-hardware/trn1-arch.htmlm/servicequotas/latest/userguide/intro.html) ê°€ í•„ìš” í•©ë‹ˆë‹¤. [trn1.32xlarge]() ëŠ” vCPU ê°€ 128ê°œ í•„ìš”í•˜ë¯€ë¡œ Running On-Demand Trn Instances ê°€ 128 ê°œ ì´ìƒ ìˆì–´ì•¼ í•©ë‹ˆë‹¤. 
- ![quota.jpg](img/trn_quota.png)

# 2. ê¸°ë³¸ í™˜ê²½ 

##  2.1. Launch the Instance (Trn1.32xlarge EC2 ìƒì„±)
- Region: us-west-2, Subnet: us-east-2d, AZ: usw2-az4 ì—ì„œ ì§„í–‰ í–ˆìŒ.
- AMI, Instance Type ì§€ì •. 
    - ![trn_32x_hf_ec2.png](img/trn_32x_hf_ec2.png)
- [ì¤‘ìš”] <u>Storage ëŠ” 200 GB ë¡œ ìˆ˜ì •í•´ì£¼ì„¸ìš”.</u>
- Trouble Shooting: Error
    - trn1.32xlarge EC2 ëŸ°ì¹­ì‹œì— ì•„ë˜ì™€ ê°™ì€ ì—ëŸ¬:
        - Failed to start the instance i-04c1XXXXXXXX The requested configuration is currently not supported. Please check the documentation for supported configurations.
    - ì†”ë£¨ì…˜
        - ìœ„ì˜ ì´ìœ ëŠ” í•´ë‹¹ Region ë° Availability Zone (AZ) ì— í•´ë‹¹ EC2 ê°€ ë¶€ì¡±í•˜ì—¬ ë°œìƒí•˜ëŠ” ì—ëŸ¬ ì…ë‹ˆë‹¤. EC2 ìƒì„±ì‹œì— Network ë¶€ë¶„ì˜ Subnet ì„ ë°”ê¾¸ì–´ì„œ í•´ë³´ì„¸ìš”. ê·¸ë˜ë„ ì—ëŸ¬ê°€ ë°œìƒí•˜ë©´ AWS Account íŒ€ì— ë¬¸ì˜ ë°”ëë‹ˆë‹¤. 

## 2.2 EC2 Connection
- í¸í•˜ì‹  ë°©ë²•ìœ¼ë¡œ EC2 ì— SSH ë¡œ ì—°ê²°í•˜ì‹œë©´ ë©ë‹ˆë‹¤. ì €ëŠ” ë¡œì»¬ì—ì„œ VS Code Remote Connection ìœ¼ë¡œ ì—°ê²° í•˜ì˜€ìŠµë‹ˆë‹¤. 
- ![VSCode.png](../../tutorial/inference-Llama-2-13b/img/VSCode.png)

## 2.3 Start Jupyter Server and Select Kernel
- ì´ ì„¹ì…˜ì€ ë…¸íŠ¸ë¶ "AWS Inferentia ê¸°ë°˜ ìœ„ì— llama-2-13B ì´ìš©í•˜ì—¬ ì±—ë´‡ ë°ëª¨" ì˜  2.3 Start Jupyter Server and Select Kernel ë¥¼ ì°¸ì¡° í•´ì£¼ì„¸ìš”. [2.3 Start Jupyter Server and Select Kernel](../01-Chatbot-Llama-2-13B-Inf2/README.md)

<p>

# 3. ë…¸íŠ¸ë¶ ì‹¤í–‰
## 3.1. Dolly ë°ì´í„° ìƒ› ì¤€ë¹„
- ì•„ë˜ëŠ” 15,000 ê°œì˜ Dolly ë°ì´í„° ìƒ˜í”Œì˜ í•œê°œì˜ ì˜ˆì‹œ ì…ë‹ˆë‹¤. 
    - ![dolly_dataset.png](img/dolly_dataset.png)
- ì•„ë˜ëŠ” Dolly ë°ì´í„° ì…‹ì„ ë¡œë“œ í•©ë‹ˆë‹¤.
    - ![load_dataset](img/load_dataset.png)   
- ì•„ë˜ëŠ” ê¸°ë³¸ ë°ì´í„° ì…‹ì˜ í˜•íƒœë¥¼ Task ë§ì¶”ì–´ì§„ í¬ë§·ìœ¼ë¡œ ë³€ê²½ í•©ë‹ˆë‹¤.
    - ![format_dolly](img/format_dolly.png)    
- ì•„ë˜ëŠ” í›ˆë ¨ì„ íš¨ìœ¨ì ìœ¼ë¡œ í•˜ê¸° ìœ„í•´ì„œ ìµœëŒ€ ê¸¸ì´ ì»¨í…ìŠ¤íŠ¸ 2048 (ëª¨ë¸ì€ 4096 ê¹Œì§€ ì§€ì›) ë¡œ ê° ìƒ˜í”Œì„ íŒ¨í‚¹í•˜ëŠ” ì ˆì°¨ ì…ë‹ˆë‹¤. ì•„ë˜ ì› ë…¸íŠ¸ë¶ì˜ ì„¤ëª… ì…ë‹ˆë‹¤. 
"ë˜í•œ ìƒ˜í”Œ í˜•ì‹ì„ ì§€ì •í•˜ê¸° ìœ„í•´ ì—¬ëŸ¬ ìƒ˜í”Œì„ í•˜ë‚˜ì˜ ì‹œí€€ìŠ¤ë¡œ ë¬¶ì–´ ë³´ë‹¤ íš¨ìœ¨ì ì¸ í•™ìŠµì„ í•©ë‹ˆë‹¤.ì´ëŠ” ì—¬ëŸ¬ ìƒ˜í”Œì„ í•˜ë‚˜ì˜ ì‹œí€€ìŠ¤ì— ìŒ“ê³  ì´ë¥¼ EOS í† í°ìœ¼ë¡œ ë¶„í• í•œë‹¤ëŠ” ì˜ë¯¸ì…ë‹ˆë‹¤. ì´ëŠ” í›ˆë ¨ì„ ë”ìš± íš¨ìœ¨ì ìœ¼ë¡œ ë§Œë“­ë‹ˆë‹¤. ìƒ˜í”Œ íŒ¨í‚¹/ì ì¬ëŠ” í›ˆë ¨ ë„ì¤‘ì´ë‚˜ ì´ì „ì— ìˆ˜í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì‹œê°„ì„ ì ˆì•½í•˜ê¸° ìœ„í•´ í›ˆë ¨ ì „ì— í•  ê²ƒì…ë‹ˆë‹¤."
    - ![pack_sample](img/pack_sample.png)

## 3.2. ëª¨ë¸ ì»´íŒŒì¼
- ì•„ë˜ëŠ” ëª¨ë¸ì„ ì»´íŒŒì¼í•˜ëŠ” ì½”ë“œì™€ ì‹¤í–‰ ê²°ê³¼ ì…ë‹ˆë‹¤. ì•½ 25ë¶„ì´ ê±¸ë ¸ìŠµë‹ˆë‹¤.
    - ![compile_model](img/compile_model.png)

## 3.3. ëª¨ë¸ íŒŒì¸ íŠœë‹
- ì•„ë˜ëŠ” ëª¨ë¸ì„ íŒŒì¸ íŠœë‹ ì½”ë“œì™€ ì‹¤í–‰ ê²°ê³¼ ì…ë‹ˆë‹¤. ì•½ 34ë¶„ì´ ê±¸ë ¸ìŠµë‹ˆë‹¤.
    - ![train_model](img/train_model.png)
- ì•„ë˜ëŠ” í›ˆë ¨ ì¤‘ì˜ ìŠ¤í¬ë¦° ìƒ· ì…ë‹ˆë‹¤. 32 ê°œì˜ Neuron core ê°€ ëª¨ë‘ ì‚¬ìš©ì´ ë˜ê³  ìˆê³ , ì´ 16ê°œì˜ Accelerator ê°€ ìˆëŠ” ê° 1ê°œë‹¹ ì´ ë©”ëª¨ë¦¬ëŠ” 32 GB ì¸ë°ìš”, ì•½ 25 GB ë¥¼ ì‚¬ìš©í•˜ê³  ìˆìŠµë‹ˆë‹¤. 
    - ![fine_tune-llama-7b](img/fine-tune-llama-7b.png)    

## 3.4. Neuron Core ì— ëª¨ë¸ ê°€ì¤‘ì¹˜ë¥¼ ë°°ì¹˜í•˜ì—¬ ëª¨ë¸ ë¡œë”©
- Optimum Neuronì€ ì‰¬ìš´ ì¶”ë¡  ì‚¬ìš©ì„ ìœ„í•´ Transformers AutoModel í´ë˜ìŠ¤ì™€ ìœ ì‚¬í•˜ê²Œ êµ¬í˜„í•©ë‹ˆë‹¤. 'NeuronModelForCausalLM' í´ë˜ìŠ¤ë¥¼ ì‚¬ìš©í•˜ì—¬ ë°”ë‹ë¼ ë³€í™˜ê¸° ì²´í¬í¬ì¸íŠ¸ë¥¼ ë¡œë“œí•˜ê³  ì´ë¥¼ ë‰´ëŸ°ìœ¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
    - ![convert_neuron](img/convert_neuron.png)

## 3.5. ëª¨ë¸ ì¶”ë¡ 
- ì•„ë˜ëŠ” "instruction": "Can you tell me something about AWS?" ì˜ ì§ˆë¬¸ì— ì˜ ë‹µë³€ì„ í•˜ëŠ” ê²ƒì„ ë³´ì—¬ ì£¼ê³  ìˆìŠµë‹ˆë‹¤. 
    - ![inference_question](img/Inference_question.png)
- ì•„ë˜ëŠ” ì§ˆë¬¸ê³¼ Context ë¥¼ ì œê³µí•˜ì—¬ RAG ì— ëŒ€í•œ ìš”ì²­ì„ í•˜ê³ , ì •í™•í•œ ë‹µì„ ì œê³µ í•©ë‹ˆë‹¤.

    ```
    prompt = {
    "instruction": "How can train models on AWS Trainium?",
    "context": "ğŸ¤— Optimum Neuron is the interface between the ğŸ¤— Transformers library and AWS AcceleratorsÂ including [AWS Trainium](https://aws.amazon.com/machine-learning/trainium/?nc1=h_ls) and [AWS Inferentia](https://aws.amazon.com/machine-learning/inferentia/?nc1=h_ls). It provides a set of tools enabling easy model loading, training and inference on single- and multi-Accelerator settings for different downstream tasks."
    }
    ```
    - ![inference_rag](img/inference_rag.png)

    

ì—¬ê¸°ê¹Œì§€ ì˜¤ì…¨ìœ¼ë©´ ì„±ê³µ í•˜ì…¨ìŠµë‹ˆë‹¤. ì¶•í•˜ ë“œë¦½ë‹ˆë‹¤. ^^

---
Contributor: ë¬¸ê³¤ìˆ˜ (Gonsoo Moon)