import requests
import time
import statistics
from concurrent.futures import ThreadPoolExecutor
import json

def test_single_request(prompt, model_name="TinyLlama/TinyLlama-1.1B-Chat-v1.0"):
    """ë‹¨ì¼ ìš”ì²­ í…ŒìŠ¤íŠ¸"""
    start_time = time.time()
    
    try:
        # í•œê¸€ í”„ë¡¬í”„íŠ¸ë¥¼ ìœ„í•œ í—¤ë” ì„¤ì •
        headers = {
            "Content-Type": "application/json; charset=utf-8"
        }
        
        response = requests.post(
            "http://localhost:8080/v1/chat/completions",
            headers=headers,
            json={
                "model": model_name,
                "messages": [{"role": "user", "content": prompt}],
                "max_tokens": 200,  # 50ì—ì„œ 200ìœ¼ë¡œ ì¦ê°€
                "temperature": 0.7
            },
            timeout=30
        )
        
        end_time = time.time()
        response_time = end_time - start_time
        
        if response.status_code == 200:
            result = response.json()
            tokens_used = result.get("usage", {}).get("total_tokens", 0)
            return {
                "success": True,
                "response_time": response_time,
                "tokens_used": tokens_used,
                "response": result["choices"][0]["message"]["content"]
            }
        else:
            # ì—ëŸ¬ ì‘ë‹µ ë‚´ìš©ë„ í¬í•¨
            error_detail = ""
            try:
                error_detail = response.json()
            except:
                error_detail = response.text
            
            return {
                "success": False,
                "response_time": response_time,
                "error": f"HTTP {response.status_code}",
                "error_detail": error_detail
            }
            
    except Exception as e:
        return {
            "success": False,
            "response_time": time.time() - start_time,
            "error": str(e)
        }

def check_available_models():
    """ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ í™•ì¸"""
    try:
        response = requests.get("http://localhost:8080/v1/models")
        if response.status_code == 200:
            models = response.json()
            print("ğŸ“‹ ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸:")
            for model in models.get("data", []):
                print(f"  - {model['id']}")
            return models.get("data", [])
        else:
            print(f"âŒ ëª¨ë¸ ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨: HTTP {response.status_code}")
            return []
    except Exception as e:
        print(f"âŒ ëª¨ë¸ ëª©ë¡ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜: {e}")
        return []

def run_benchmark(num_requests=10, concurrent_requests=1):
    """ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰"""
    print(f"ğŸš€ ë²¤ì¹˜ë§ˆí¬ ì‹œì‘: {num_requests}ê°œ ìš”ì²­, {concurrent_requests}ê°œ ë™ì‹œ ì‹¤í–‰")
    print("=" * 50)
    
    # ë¨¼ì € ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ í™•ì¸
    available_models = check_available_models()
    if not available_models:
        print("âš ï¸  ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    # ì²« ë²ˆì§¸ ëª¨ë¸ ì‚¬ìš©
    model_name = available_models[0]["id"]
    print(f"ğŸ¯ ì‚¬ìš©í•  ëª¨ë¸: {model_name}")
    
    # í•œê¸€ í…ŒìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸ë“¤ (ì›ë˜ëŒ€ë¡œ ë³µì›)
    test_prompts = [
        "ì•ˆë…•í•˜ì„¸ìš”! ê°„ë‹¨í•œ ì¸ì‚¬ë§ì„ í•´ì£¼ì„¸ìš”.",
        "íŒŒë¦¬ëŠ” ì–´ëŠ ë‚˜ë¼ì˜ ìˆ˜ë„ì¸ê°€ìš”?",
        "1+1ì€ ë¬´ì—‡ì¸ê°€ìš”?",
        "ì¬ë¯¸ìˆëŠ” ë†ë‹´ì„ í•˜ë‚˜ í•´ì£¼ì„¸ìš”.",
        "íŒŒì´ì¬ì´ë€ ë¬´ì—‡ì¸ê°€ìš”?"
    ]
    
    results = []
    
    if concurrent_requests == 1:
        # ìˆœì°¨ ì‹¤í–‰
        for i in range(num_requests):
            prompt = test_prompts[i % len(test_prompts)]
            print(f"ìš”ì²­ {i+1}/{num_requests}: {prompt[:30]}...")
            
            result = test_single_request(prompt, model_name)
            results.append(result)
            
            if result["success"]:
                print(f"  âœ… ì„±ê³µ - {result['response_time']:.2f}ì´ˆ, {result['tokens_used']} í† í°")
                print(f"     ì‘ë‹µ: {result['response'][:50]}...")
            else:
                print(f"  âŒ ì‹¤íŒ¨ - {result['error']}")
                if "error_detail" in result:
                    print(f"     ìƒì„¸: {result['error_detail']}")
    
    else:
        # ë™ì‹œ ì‹¤í–‰
        with ThreadPoolExecutor(max_workers=concurrent_requests) as executor:
            futures = []
            for i in range(num_requests):
                prompt = test_prompts[i % len(test_prompts)]
                futures.append(executor.submit(test_single_request, prompt, model_name))
            
            for i, future in enumerate(futures):
                result = future.result()
                results.append(result)
                print(f"ìš”ì²­ {i+1}/{num_requests} ì™„ë£Œ - {result['response_time']:.2f}ì´ˆ")
    
    # ê²°ê³¼ ë¶„ì„
    successful_results = [r for r in results if r["success"]]
    failed_results = [r for r in results if not r["success"]]
    
    print("\n" + "=" * 50)
    print("ğŸ“Š ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼")
    print("=" * 50)
    
    print(f"ì´ ìš”ì²­ ìˆ˜: {len(results)}")
    print(f"ì„±ê³µ: {len(successful_results)}")
    print(f"ì‹¤íŒ¨: {len(failed_results)}")
    print(f"ì„±ê³µë¥ : {len(successful_results)/len(results)*100:.1f}%")
    
    if successful_results:
        response_times = [r["response_time"] for r in successful_results]
        tokens_used = [r["tokens_used"] for r in successful_results]
        
        print(f"\nâ±ï¸  ì‘ë‹µ ì‹œê°„:")
        print(f"  í‰ê· : {statistics.mean(response_times):.2f}ì´ˆ")
        print(f"  ì¤‘ê°„ê°’: {statistics.median(response_times):.2f}ì´ˆ")
        print(f"  ìµœì†Œ: {min(response_times):.2f}ì´ˆ")
        print(f"  ìµœëŒ€: {max(response_times):.2f}ì´ˆ")
        
        print(f"\nğŸ“ í† í° ì‚¬ìš©ëŸ‰:")
        print(f"  í‰ê· : {statistics.mean(tokens_used):.1f} í† í°")
        print(f"  ì´í•©: {sum(tokens_used)} í† í°")
        
        # ì´ˆë‹¹ ì²˜ë¦¬ëŸ‰ ê³„ì‚°
        total_time = sum(response_times)
        if total_time > 0:
            requests_per_second = len(successful_results) / total_time
            print(f"\n ì²˜ë¦¬ëŸ‰: {requests_per_second:.2f} ìš”ì²­/ì´ˆ")
    
    if failed_results:
        print(f"\nâŒ ì‹¤íŒ¨í•œ ìš”ì²­ë“¤:")
        for i, result in enumerate(failed_results):
            print(f"  {i+1}. {result['error']}")
            if "error_detail" in result:
                print(f"     ìƒì„¸: {result['error_detail']}")

if __name__ == "__main__":
    # í•œê¸€ í”„ë¡¬í”„íŠ¸ ë²¤ì¹˜ë§ˆí¬ (10ê°œ ìš”ì²­, ìˆœì°¨ ì‹¤í–‰)
    run_benchmark(num_requests=10, concurrent_requests=1)
    
    print("\n" + "=" * 50)
    
    # ë™ì‹œ ì‹¤í–‰ ë²¤ì¹˜ë§ˆí¬ (5ê°œ ìš”ì²­, 2ê°œ ë™ì‹œ ì‹¤í–‰)
    run_benchmark(num_requests=5, concurrent_requests=2)