{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "59913016-f89e-4a0e-9afe-b3a06e9112d5",
   "metadata": {},
   "source": [
    "# Run Hugging Face `upstage/SOLAR-10.7B-Instruct-v1.0` autoregressive sampling on Inf2 & Trn1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8454655-ec27-45e3-8da7-f82b744321ee",
   "metadata": {},
   "source": [
    "In this example we compile and deploy the Hugging Face [upstage/SOLAR-10.7B-Instruct-v1.0](https://huggingface.co/upstage/SOLAR-10.7B-Instruct-v1.0) model for tensor parallel inference on Neuron using the `transformers-neuronx` package.\n",
    "\n",
    "The example has the following main sections:\n",
    "1. Set up the Jupyter Notebook\n",
    "1. Install dependencies\n",
    "1. Download the model\n",
    "1. Construct the model|\n",
    "1. Split the model `state_dict` into multiple files\n",
    "1. Perform autoregressive sampling using tensor parallelism\n",
    "\n",
    "This Jupyter Notebook can be run on an Inf2 instance (`inf2.48xlarge`) or Trn1 instance (`trn1.32xlarge`)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af2b7693-2950-41fc-a038-17cba44bf003",
   "metadata": {},
   "source": [
    "## Set up the Jupyter Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c47ef383-0dea-4423-8c38-29c73927fd78",
   "metadata": {},
   "source": [
    "The following steps set up Jupyter Notebook and launch this tutorial:\n",
    "1. Clone the [AWS Neuron Samples](https://github.com/aws-neuron/aws-neuron-samples) repo to your instance using\n",
    "```\n",
    "git clone https://github.com/aws-neuron/aws-neuron-samples.git\n",
    "```\n",
    "2. Navigate to the `transformers-neuronx` inference samples folder\n",
    "```\n",
    "cd aws-neuron-samples/torch-neuronx/transformers-neuronx/inference\n",
    "```\n",
    "3. Follow the instructions in [Jupyter Notebook QuickStart](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/general/setup/notebook/setup-jupyter-notebook-steps-troubleshooting.html) to run Jupyter Notebook on your instance.\n",
    "4. Locate this tutorial in your Jupyter Notebook session (`SOLAR-10.7B-Instruct-v1.0-sampling.ipynb`) and launch it. Follow the rest of the instructions in this tutorial. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a727963e-8178-4d2a-a5cd-a4f2bf00197e",
   "metadata": {},
   "source": [
    "## Install Dependencies\n",
    "This tutorial requires the following pip packages:\n",
    "\n",
    " - `torch-neuronx`\n",
    " - `neuronx-cc`\n",
    " - `sentencepiece`\n",
    " - `transformers`\n",
    " - `transformers-neuronx`\n",
    "\n",
    "\n",
    "Most of these packages will be installed when configuring your environment using the [torch-neuronx inference setup guide](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/general/setup/torch-neuronx.html#setup-torch-neuronx). The additional dependencies must be installed here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3c4899b2-39b2-4309-b7df-48fe74b56eb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pip.repos.neuron.amazonaws.com\n",
      "Requirement already satisfied: transformers-neuronx in /home/ubuntu/aws_neuron_venv_pytorch/lib/python3.10/site-packages (0.9.474)\n",
      "Requirement already satisfied: sentencepiece in /home/ubuntu/aws_neuron_venv_pytorch/lib/python3.10/site-packages (0.2.0)\n",
      "Requirement already satisfied: accelerate in /home/ubuntu/aws_neuron_venv_pytorch/lib/python3.10/site-packages (from transformers-neuronx) (0.23.0)\n",
      "Requirement already satisfied: safetensors in /home/ubuntu/aws_neuron_venv_pytorch/lib/python3.10/site-packages (from transformers-neuronx) (0.4.2)\n",
      "Requirement already satisfied: torch-neuronx in /home/ubuntu/aws_neuron_venv_pytorch/lib/python3.10/site-packages (from transformers-neuronx) (1.13.1.1.13.1)\n",
      "Requirement already satisfied: transformers in /home/ubuntu/aws_neuron_venv_pytorch/lib/python3.10/site-packages (from transformers-neuronx) (4.36.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/ubuntu/aws_neuron_venv_pytorch/lib/python3.10/site-packages (from accelerate->transformers-neuronx) (1.25.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ubuntu/aws_neuron_venv_pytorch/lib/python3.10/site-packages (from accelerate->transformers-neuronx) (24.0)\n",
      "Requirement already satisfied: psutil in /home/ubuntu/aws_neuron_venv_pytorch/lib/python3.10/site-packages (from accelerate->transformers-neuronx) (5.9.8)\n",
      "Requirement already satisfied: pyyaml in /home/ubuntu/aws_neuron_venv_pytorch/lib/python3.10/site-packages (from accelerate->transformers-neuronx) (6.0.1)\n",
      "Requirement already satisfied: torch>=1.10.0 in /home/ubuntu/aws_neuron_venv_pytorch/lib/python3.10/site-packages (from accelerate->transformers-neuronx) (1.13.1)\n",
      "Requirement already satisfied: huggingface-hub in /home/ubuntu/aws_neuron_venv_pytorch/lib/python3.10/site-packages (from accelerate->transformers-neuronx) (0.22.1)\n",
      "Requirement already satisfied: torch-xla==1.13.1+torchneurond in /home/ubuntu/aws_neuron_venv_pytorch/lib/python3.10/site-packages (from torch-neuronx->transformers-neuronx) (1.13.1+torchneurond)\n",
      "Requirement already satisfied: libneuronxla==0.5.809 in /home/ubuntu/aws_neuron_venv_pytorch/lib/python3.10/site-packages (from torch-neuronx->transformers-neuronx) (0.5.809)\n",
      "Requirement already satisfied: protobuf<5 in /home/ubuntu/aws_neuron_venv_pytorch/lib/python3.10/site-packages (from torch-neuronx->transformers-neuronx) (3.19.6)\n",
      "Requirement already satisfied: aws-neuronx-runtime-discovery~=2.0 in /home/ubuntu/aws_neuron_venv_pytorch/lib/python3.10/site-packages (from libneuronxla==0.5.809->torch-neuronx->transformers-neuronx) (2.9)\n",
      "Requirement already satisfied: neuronx-cc~=2.0 in /home/ubuntu/aws_neuron_venv_pytorch/lib/python3.10/site-packages (from libneuronxla==0.5.809->torch-neuronx->transformers-neuronx) (2.12.68.0+4480452af)\n",
      "Requirement already satisfied: boto3~=1.26 in /home/ubuntu/aws_neuron_venv_pytorch/lib/python3.10/site-packages (from libneuronxla==0.5.809->torch-neuronx->transformers-neuronx) (1.34.72)\n",
      "Requirement already satisfied: botocore~=1.29 in /home/ubuntu/aws_neuron_venv_pytorch/lib/python3.10/site-packages (from libneuronxla==0.5.809->torch-neuronx->transformers-neuronx) (1.34.72)\n",
      "Requirement already satisfied: typing-extensions in /home/ubuntu/aws_neuron_venv_pytorch/lib/python3.10/site-packages (from torch>=1.10.0->accelerate->transformers-neuronx) (4.10.0)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /home/ubuntu/aws_neuron_venv_pytorch/lib/python3.10/site-packages (from torch>=1.10.0->accelerate->transformers-neuronx) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /home/ubuntu/aws_neuron_venv_pytorch/lib/python3.10/site-packages (from torch>=1.10.0->accelerate->transformers-neuronx) (8.5.0.96)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /home/ubuntu/aws_neuron_venv_pytorch/lib/python3.10/site-packages (from torch>=1.10.0->accelerate->transformers-neuronx) (11.10.3.66)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /home/ubuntu/aws_neuron_venv_pytorch/lib/python3.10/site-packages (from torch>=1.10.0->accelerate->transformers-neuronx) (11.7.99)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /home/ubuntu/aws_neuron_venv_pytorch/lib/python3.10/site-packages (from torch-xla==1.13.1+torchneurond->torch-neuronx->transformers-neuronx) (2.1.0)\n",
      "Requirement already satisfied: cloud-tpu-client>=0.10.0 in /home/ubuntu/aws_neuron_venv_pytorch/lib/python3.10/site-packages (from torch-xla==1.13.1+torchneurond->torch-neuronx->transformers-neuronx) (0.10)\n",
      "Requirement already satisfied: setuptools in /home/ubuntu/aws_neuron_venv_pytorch/lib/python3.10/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.10.0->accelerate->transformers-neuronx) (69.2.0)\n",
      "Requirement already satisfied: wheel in /home/ubuntu/aws_neuron_venv_pytorch/lib/python3.10/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.10.0->accelerate->transformers-neuronx) (0.43.0)\n",
      "Requirement already satisfied: filelock in /home/ubuntu/aws_neuron_venv_pytorch/lib/python3.10/site-packages (from transformers->transformers-neuronx) (3.13.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/ubuntu/aws_neuron_venv_pytorch/lib/python3.10/site-packages (from transformers->transformers-neuronx) (2023.12.25)\n",
      "Requirement already satisfied: requests in /home/ubuntu/aws_neuron_venv_pytorch/lib/python3.10/site-packages (from transformers->transformers-neuronx) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /home/ubuntu/aws_neuron_venv_pytorch/lib/python3.10/site-packages (from transformers->transformers-neuronx) (0.15.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/ubuntu/aws_neuron_venv_pytorch/lib/python3.10/site-packages (from transformers->transformers-neuronx) (4.66.2)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/ubuntu/aws_neuron_venv_pytorch/lib/python3.10/site-packages (from huggingface-hub->accelerate->transformers-neuronx) (2024.2.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/ubuntu/aws_neuron_venv_pytorch/lib/python3.10/site-packages (from requests->transformers->transformers-neuronx) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ubuntu/aws_neuron_venv_pytorch/lib/python3.10/site-packages (from requests->transformers->transformers-neuronx) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ubuntu/aws_neuron_venv_pytorch/lib/python3.10/site-packages (from requests->transformers->transformers-neuronx) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ubuntu/aws_neuron_venv_pytorch/lib/python3.10/site-packages (from requests->transformers->transformers-neuronx) (2024.2.2)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /home/ubuntu/aws_neuron_venv_pytorch/lib/python3.10/site-packages (from boto3~=1.26->libneuronxla==0.5.809->torch-neuronx->transformers-neuronx) (1.0.1)\n",
      "Requirement already satisfied: s3transfer<0.11.0,>=0.10.0 in /home/ubuntu/aws_neuron_venv_pytorch/lib/python3.10/site-packages (from boto3~=1.26->libneuronxla==0.5.809->torch-neuronx->transformers-neuronx) (0.10.1)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /home/ubuntu/aws_neuron_venv_pytorch/lib/python3.10/site-packages (from botocore~=1.29->libneuronxla==0.5.809->torch-neuronx->transformers-neuronx) (2.9.0.post0)\n",
      "Requirement already satisfied: google-api-python-client==1.8.0 in /home/ubuntu/aws_neuron_venv_pytorch/lib/python3.10/site-packages (from cloud-tpu-client>=0.10.0->torch-xla==1.13.1+torchneurond->torch-neuronx->transformers-neuronx) (1.8.0)\n",
      "Requirement already satisfied: oauth2client in /home/ubuntu/aws_neuron_venv_pytorch/lib/python3.10/site-packages (from cloud-tpu-client>=0.10.0->torch-xla==1.13.1+torchneurond->torch-neuronx->transformers-neuronx) (4.1.3)\n",
      "Requirement already satisfied: httplib2<1dev,>=0.9.2 in /home/ubuntu/aws_neuron_venv_pytorch/lib/python3.10/site-packages (from google-api-python-client==1.8.0->cloud-tpu-client>=0.10.0->torch-xla==1.13.1+torchneurond->torch-neuronx->transformers-neuronx) (0.22.0)\n",
      "Requirement already satisfied: google-auth>=1.4.1 in /home/ubuntu/aws_neuron_venv_pytorch/lib/python3.10/site-packages (from google-api-python-client==1.8.0->cloud-tpu-client>=0.10.0->torch-xla==1.13.1+torchneurond->torch-neuronx->transformers-neuronx) (2.29.0)\n",
      "Requirement already satisfied: google-auth-httplib2>=0.0.3 in /home/ubuntu/aws_neuron_venv_pytorch/lib/python3.10/site-packages (from google-api-python-client==1.8.0->cloud-tpu-client>=0.10.0->torch-xla==1.13.1+torchneurond->torch-neuronx->transformers-neuronx) (0.2.0)\n",
      "Requirement already satisfied: google-api-core<2dev,>=1.13.0 in /home/ubuntu/aws_neuron_venv_pytorch/lib/python3.10/site-packages (from google-api-python-client==1.8.0->cloud-tpu-client>=0.10.0->torch-xla==1.13.1+torchneurond->torch-neuronx->transformers-neuronx) (1.34.1)\n",
      "Requirement already satisfied: six<2dev,>=1.6.1 in /home/ubuntu/aws_neuron_venv_pytorch/lib/python3.10/site-packages (from google-api-python-client==1.8.0->cloud-tpu-client>=0.10.0->torch-xla==1.13.1+torchneurond->torch-neuronx->transformers-neuronx) (1.16.0)\n",
      "Requirement already satisfied: uritemplate<4dev,>=3.0.0 in /home/ubuntu/aws_neuron_venv_pytorch/lib/python3.10/site-packages (from google-api-python-client==1.8.0->cloud-tpu-client>=0.10.0->torch-xla==1.13.1+torchneurond->torch-neuronx->transformers-neuronx) (3.0.1)\n",
      "Requirement already satisfied: neuronx-hwm==2.12.0.0 in /home/ubuntu/aws_neuron_venv_pytorch/lib/python3.10/site-packages (from neuronx-cc~=2.0->libneuronxla==0.5.809->torch-neuronx->transformers-neuronx) (2.12.0.0+422c9037c)\n",
      "Requirement already satisfied: networkx<=2.6.3 in /home/ubuntu/aws_neuron_venv_pytorch/lib/python3.10/site-packages (from neuronx-cc~=2.0->libneuronxla==0.5.809->torch-neuronx->transformers-neuronx) (2.6.3)\n",
      "Requirement already satisfied: scipy<=1.11.2 in /home/ubuntu/aws_neuron_venv_pytorch/lib/python3.10/site-packages (from neuronx-cc~=2.0->libneuronxla==0.5.809->torch-neuronx->transformers-neuronx) (1.11.2)\n",
      "Requirement already satisfied: python-daemon>=2.2.4 in /home/ubuntu/aws_neuron_venv_pytorch/lib/python3.10/site-packages (from neuronx-cc~=2.0->libneuronxla==0.5.809->torch-neuronx->transformers-neuronx) (3.0.1)\n",
      "Requirement already satisfied: requests-unixsocket>=0.1.5 in /home/ubuntu/aws_neuron_venv_pytorch/lib/python3.10/site-packages (from neuronx-cc~=2.0->libneuronxla==0.5.809->torch-neuronx->transformers-neuronx) (0.3.0)\n",
      "Requirement already satisfied: islpy<=2023.1,>2021.1 in /home/ubuntu/aws_neuron_venv_pytorch/lib/python3.10/site-packages (from neuronx-cc~=2.0->libneuronxla==0.5.809->torch-neuronx->transformers-neuronx) (2023.1)\n",
      "Requirement already satisfied: pgzip>=0.3.0 in /home/ubuntu/aws_neuron_venv_pytorch/lib/python3.10/site-packages (from neuronx-cc~=2.0->libneuronxla==0.5.809->torch-neuronx->transformers-neuronx) (0.3.5)\n",
      "Requirement already satisfied: ec2-metadata<=2.10.0 in /home/ubuntu/aws_neuron_venv_pytorch/lib/python3.10/site-packages (from neuronx-cc~=2.0->libneuronxla==0.5.809->torch-neuronx->transformers-neuronx) (2.10.0)\n",
      "Requirement already satisfied: docutils in /home/ubuntu/aws_neuron_venv_pytorch/lib/python3.10/site-packages (from python-daemon>=2.2.4->neuronx-cc~=2.0->libneuronxla==0.5.809->torch-neuronx->transformers-neuronx) (0.16)\n",
      "Requirement already satisfied: lockfile>=0.10 in /home/ubuntu/aws_neuron_venv_pytorch/lib/python3.10/site-packages (from python-daemon>=2.2.4->neuronx-cc~=2.0->libneuronxla==0.5.809->torch-neuronx->transformers-neuronx) (0.12.2)\n",
      "Requirement already satisfied: pyasn1>=0.1.7 in /home/ubuntu/aws_neuron_venv_pytorch/lib/python3.10/site-packages (from oauth2client->cloud-tpu-client>=0.10.0->torch-xla==1.13.1+torchneurond->torch-neuronx->transformers-neuronx) (0.6.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.0.5 in /home/ubuntu/aws_neuron_venv_pytorch/lib/python3.10/site-packages (from oauth2client->cloud-tpu-client>=0.10.0->torch-xla==1.13.1+torchneurond->torch-neuronx->transformers-neuronx) (0.4.0)\n",
      "Requirement already satisfied: rsa>=3.1.4 in /home/ubuntu/aws_neuron_venv_pytorch/lib/python3.10/site-packages (from oauth2client->cloud-tpu-client>=0.10.0->torch-xla==1.13.1+torchneurond->torch-neuronx->transformers-neuronx) (4.7.2)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.56.2 in /home/ubuntu/aws_neuron_venv_pytorch/lib/python3.10/site-packages (from google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client>=0.10.0->torch-xla==1.13.1+torchneurond->torch-neuronx->transformers-neuronx) (1.63.0)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /home/ubuntu/aws_neuron_venv_pytorch/lib/python3.10/site-packages (from google-auth>=1.4.1->google-api-python-client==1.8.0->cloud-tpu-client>=0.10.0->torch-xla==1.13.1+torchneurond->torch-neuronx->transformers-neuronx) (5.3.3)\n",
      "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /home/ubuntu/aws_neuron_venv_pytorch/lib/python3.10/site-packages (from httplib2<1dev,>=0.9.2->google-api-python-client==1.8.0->cloud-tpu-client>=0.10.0->torch-xla==1.13.1+torchneurond->torch-neuronx->transformers-neuronx) (3.1.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers-neuronx sentencepiece"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14400e26-2058-44b0-b680-b1cee57203aa",
   "metadata": {},
   "source": [
    "## Download the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e233a69-5658-4180-8f6c-91f377a01001",
   "metadata": {},
   "source": [
    "Follow the steps described in [upstage/SOLAR-10.7B-Instruct-v1.0](https://huggingface.co/upstage/SOLAR-10.7B-Instruct-v1.0) to get access to the SOLLAR model from Meta and download the weights and tokenizer.\n",
    "\n",
    "Note: For the purposes of this sample we assume you have saved the SOLAR-10.7B-Instruct-v1.0 model in a directory called `SOLAR-10.7B-Instruct-v1.0` with the following format:\n",
    "```\n",
    "SOLAR-10.7B-Instruct-v1.0/\n",
    "├── config.json\n",
    "├── generation_config.json\n",
    "├── pytorch_model-00001-of-00003.bin\n",
    "├── pytorch_model-00002-of-00003.bin\n",
    "├── pytorch_model-00003-of-00003.bin\n",
    "├── pytorch_model.bin.index.json\n",
    "├── special_tokens_map.json\n",
    "├── tokenizer.json\n",
    "├── tokenizer.model\n",
    "└── tokenizer_config.json\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b06fb496-f8c6-4222-b5ad-39b3e0bc0e22",
   "metadata": {},
   "source": [
    "## Construct the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2669028-8cf1-49f6-8e93-f6ceb39588fd",
   "metadata": {},
   "source": [
    "After downloading the model and converting it to the Hugging Face format we construct the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8ea21739-a65e-4a5c-9a10-7f963a99a72a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91484c31c7ce4feb8fef3e593903d62b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import LlamaForCausalLM\n",
    "\n",
    "model = LlamaForCausalLM.from_pretrained('upstage/SOLAR-10.7B-Instruct-v1.0')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92b76098-172a-472a-a126-f0ef7606c77f",
   "metadata": {},
   "source": [
    "## Split the model state_dict into multiple files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34ef7157-da52-4a9a-9839-6394682d39ca",
   "metadata": {},
   "source": [
    "For the sake of reducing host memory usage, it is recommended to save the model `state_dict` as\n",
    "multiple files, as opposed to one monolithic file given by `torch.save`. This \"split-format\"\n",
    "`state_dict` can be created using the `save_pretrained_split` function. With this checkpoint format,\n",
    "the Neuron model loader can load parameters to the Neuron device high-bandwidth memory (HBM) directly\n",
    "by keeping at most one layer of model parameters in the CPU main memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9226f998-08a2-4c42-a8b7-75fe3626c587",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers_neuronx.module import save_pretrained_split\n",
    "\n",
    "save_pretrained_split(model, './SOLAR-10.7B-Instruct-v1.0-split')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e1ededb-e0d6-4c1d-aac8-bc3d29bd6ebe",
   "metadata": {},
   "source": [
    "## Perform autoregressive sampling using tensor parallelism"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1a87b9f-2948-4db9-946f-b618533f03a7",
   "metadata": {},
   "source": [
    "Now we have all of the necessary files for running `upstage/SOLAR-10.7B-Instruct-v1.0` autoregressive sampling. \n",
    "\n",
    "The memory required to host any model can be computed with:\n",
    "```\n",
    "total memory = bytes per parameter * number of parameters\n",
    "```\n",
    "When using `float16` casted weights for a 13 billion parameter model, this works out to `2 * 13B` or ~26GB of weights. Each NeuronCore has 16GB of memory which means that a 26GB model cannot fit on a single NeuronCore. In reality, the total space required is often greater than just the number of parameters due to caching attention layer projections (KV caching). This caching mechanism grows memory allocations linearly with sequence length and batch size.\n",
    "\n",
    "To get very large language models to fit on Inf2 & Trn1, tensor parallelism is used to split weights, data, and compute across multiple NeuronCores. The number of NeuronCores that the weights are split across can be controlled by setting the `tp_degree` parameter. This parallelism degree must be chosen to ensure that the memory usage per NeuronCore will be less than the physical 16GB limit. When configuring tensor   , the memory per NeuronCore can be computed with:\n",
    "\n",
    "```\n",
    "memory per core = (bytes per parameter * number of parameters) / tp_degree\n",
    "```\n",
    "\n",
    "This can be used to compute the minimum instance sizing by ensuring that the value selected for `tp_degree` results in less than 16GB allocated per NeuronCore.\n",
    "\n",
    "Note that increasing the `tp_degree` beyond the minimum requirement almost always results in a faster model. Increasing the tensor parallelism degree improves memory bandwidth which improves model performance. To optimize performance it's recommended to use the highest tensor parallelism degree that is supported by the instance. In this sample we use tensor parallelism degree 24 to optimize performance on `inf2.48xlarge`, but this should be changed to 32 if you are using a `trn1.32xlarge`. \n",
    "\n",
    "We will use the Neuron `LlamaForSampling` class to implement tensor parallelism for the SOLAR model. The default model config supports sampling up to sequence length 2048. Tensor parallelism is enabled through the argument `tp_degree=24`. We enable `float16` casting with the `amp='f16'` flag. The model computational graph is compiled by `neuronx-cc` for optimized inference on Neuron."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cc93079d-7d17-48fd-bf9d-7176bc061a47",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/aws_neuron_venv_pytorch/lib/python3.10/site-packages/transformers_neuronx/decoder.py:150: UserWarning: KV head replication will be enabled since the number of KV heads (8) is not evenly divisible by the tensor parallel degree (24)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-03-29 07:48:09.000876:  12995  INFO ||NEURON_CACHE||: Compile cache path: /var/tmp/neuron-compile-cache\n",
      "2024-03-29 07:48:09.000942:  12995  INFO ||NEURON_CC_WRAPPER||: Using a cached neff at /var/tmp/neuron-compile-cache/neuronxcc-2.12.68.0+4480452af/MODULE_3ccbbf8fad9f8653719c+2c2d707e/model.neff. Exiting with a successfully compiled graph.\n",
      "2024-03-29 07:48:09.000967:  12997  INFO ||NEURON_CACHE||: Compile cache path: /var/tmp/neuron-compile-cache\n",
      "2024-03-29 07:48:10.000012:  12997  INFO ||NEURON_CC_WRAPPER||: Using a cached neff at /var/tmp/neuron-compile-cache/neuronxcc-2.12.68.0+4480452af/MODULE_159bbea91adf9a015aed+2c2d707e/model.neff. Exiting with a successfully compiled graph.\n",
      "2024-03-29 07:48:10.000208:  12998  INFO ||NEURON_CACHE||: Compile cache path: /var/tmp/neuron-compile-cache\n",
      "2024-03-29 07:48:10.000253:  13000  INFO ||NEURON_CACHE||: Compile cache path: /var/tmp/neuron-compile-cache\n",
      "2024-03-29 07:48:10.000268:  12998  INFO ||NEURON_CC_WRAPPER||: Using a cached neff at /var/tmp/neuron-compile-cache/neuronxcc-2.12.68.0+4480452af/MODULE_98408c431741877be917+2c2d707e/model.neff. Exiting with a successfully compiled graph.\n",
      "2024-03-29 07:48:10.000334:  13000  INFO ||NEURON_CC_WRAPPER||: Using a cached neff at /var/tmp/neuron-compile-cache/neuronxcc-2.12.68.0+4480452af/MODULE_a1542f8e722601b49752+2c2d707e/model.neff. Exiting with a successfully compiled graph.\n",
      "2024-03-29 07:48:10.000890:  13001  INFO ||NEURON_CACHE||: Compile cache path: /var/tmp/neuron-compile-cache\n",
      "2024-03-29 07:48:11.000018:  13001  INFO ||NEURON_CC_WRAPPER||: Using a cached neff at /var/tmp/neuron-compile-cache/neuronxcc-2.12.68.0+4480452af/MODULE_38eaf08eacefe5d81f34+2c2d707e/model.neff. Exiting with a successfully compiled graph.\n",
      "2024-03-29 07:48:11.000076:  13002  INFO ||NEURON_CACHE||: Compile cache path: /var/tmp/neuron-compile-cache\n",
      "2024-03-29 07:48:11.000116:  13002  INFO ||NEURON_CC_WRAPPER||: Using a cached neff at /var/tmp/neuron-compile-cache/neuronxcc-2.12.68.0+4480452af/MODULE_16e691faa03898b124ec+2c2d707e/model.neff. Exiting with a successfully compiled graph.\n",
      "2024-03-29 07:48:11.000120:  13004  INFO ||NEURON_CACHE||: Compile cache path: /var/tmp/neuron-compile-cache\n",
      "2024-03-29 07:48:11.000161:  13004  INFO ||NEURON_CC_WRAPPER||: Using a cached neff at /var/tmp/neuron-compile-cache/neuronxcc-2.12.68.0+4480452af/MODULE_fe4597a2864fc897f83b+2c2d707e/model.neff. Exiting with a successfully compiled graph.\n",
      "2024-03-29 07:48:11.000202:  13006  INFO ||NEURON_CACHE||: Compile cache path: /var/tmp/neuron-compile-cache\n",
      "2024-03-29 07:48:11.000238:  13007  INFO ||NEURON_CACHE||: Compile cache path: /var/tmp/neuron-compile-cache\n",
      "2024-03-29 07:48:11.000257:  13006  INFO ||NEURON_CC_WRAPPER||: Using a cached neff at /var/tmp/neuron-compile-cache/neuronxcc-2.12.68.0+4480452af/MODULE_cfc674e9b3447df6f61a+2c2d707e/model.neff. Exiting with a successfully compiled graph.\n",
      "2024-03-29 07:48:11.000291:  13007  INFO ||NEURON_CC_WRAPPER||: Using a cached neff at /var/tmp/neuron-compile-cache/neuronxcc-2.12.68.0+4480452af/MODULE_53f9602d8c748fdde2f5+2c2d707e/model.neff. Exiting with a successfully compiled graph.\n",
      "2024-03-29 07:48:11.000299:  13009  INFO ||NEURON_CACHE||: Compile cache path: /var/tmp/neuron-compile-cache\n",
      "2024-03-29 07:48:11.000344:  13009  INFO ||NEURON_CC_WRAPPER||: Using a cached neff at /var/tmp/neuron-compile-cache/neuronxcc-2.12.68.0+4480452af/MODULE_a098f08124e3c8e57271+2c2d707e/model.neff. Exiting with a successfully compiled graph.\n",
      "2024-Mar-29 07:49:11.0201 12769:13069 [16] nccl_net_ofi_init:1415 CCOM WARN NET/OFI aws-ofi-nccl initialization failed\n",
      "2024-Mar-29 07:49:11.0201 12769:13069 [16] init.cc:137 CCOM WARN OFI plugin initNet() failed is EFA enabled?\n",
      "generated sequences [\"<s> Hello, I'm a language model, AI assistant designed to help you with tasks and provide answers to your questions. I can assist you with different topics, including technology, science, history, or any general interests that may come to mind. How can I assist you today?</s>\"] in 2.2680654525756836 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "from transformers_neuronx.llama.model import LlamaForSampling\n",
    "\n",
    "import os\n",
    "# Compiler flag -O1 is a workaround for “Too many instructions after unroll” in SDK 2.14\n",
    "# os.environ['NEURON_CC_FLAGS'] = '-O1'\n",
    "\n",
    "# load upstage/SOLAR-10.7B-Instruct-v1.0 to the NeuronCores with 24-way tensor parallelism and run compilation\n",
    "neuron_model = LlamaForSampling.from_pretrained('./SOLAR-10.7B-Instruct-v1.0-split', batch_size=1, tp_degree=24, amp='f16')\n",
    "neuron_model.to_neuron()\n",
    "\n",
    "# construct a tokenizer and encode prompt text\n",
    "tokenizer = AutoTokenizer.from_pretrained('upstage/SOLAR-10.7B-Instruct-v1.0')\n",
    "prompt = \"Hello, I'm a language model,\"\n",
    "input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "\n",
    "# run inference with top-k sampling\n",
    "with torch.inference_mode():\n",
    "    start = time.time()\n",
    "    generated_sequences = neuron_model.sample(input_ids, sequence_length=2048, top_k=50)\n",
    "    elapsed = time.time() - start\n",
    "\n",
    "generated_sequences = [tokenizer.decode(seq) for seq in generated_sequences]\n",
    "print(f'generated sequences {generated_sequences} in {elapsed} seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94ac4991-7606-4c2f-90af-230998b0de20",
   "metadata": {},
   "source": [
    "## Save and load the compiled model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be6a4ba9-40fd-4544-81ab-9fd249f22e4d",
   "metadata": {},
   "source": [
    "The ```save``` and ```load``` functions can be used to save and load compiled model artifacts respectively. Loading compiled model artifacts from a provided directory will avoid model recompilation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "07176c29-b30b-4d16-8291-3bd0142dc42d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated sequences tensor([[    1, 22557, 28725,   315, 28742, 28719,   264,  3842,  2229, 28725,\n",
      "           304,   315,   541,  3084,  6516, 15988,   297,   456,  2758, 28723,\n",
      "          1791,  2225,   264, 28705, 28740, 28757,   401,  5550,   356,   264,\n",
      "           808,   302,  1178,  2078,   297,   264, 28705, 28750, 28757,  2293,\n",
      "          1413, 21366, 28725,   368,   541,   938,   272,  2522,   508, 28724,\n",
      "          7607, 28742, 28713,   285,   632, 28732,  2654, 28725, 11022, 28746,\n",
      "          5364, 28731,   908, 28723,  1047,   574,  1178,   349,   297,   264,\n",
      "         28705, 28750, 28757,  2293,   970,   624,  9711, 10651,   272, 11010,\n",
      "         28725,   368,   541, 15759,   272,  2293,   304,   272, 11022,  5270,\n",
      "           298, 15759,  2267,   690, 11022,   368,   947,   272,   401,  5550,\n",
      "           298,   347, 16860, 28723,    13,    13, 15423,   349,   396,  2757,\n",
      "          1413, 15755,  6218,   304,  2522,   508, 28724, 28747,    13,    13,\n",
      "         13940, 28832, 17667,    13,  7841, 25637,   390,  7494,    13,  3211,\n",
      "           752,   508, 28724, 28723,   496, 28707,   726,   285,   632,    13,\n",
      "            13, 28771,  3563,  6212,  2787,  1178,    13,  1056,   327,  7494,\n",
      "         28723,  9121, 28723, 12219, 28732, 28740, 28734, 28734, 28725, 28705,\n",
      "         28740, 28731,    13,    13, 28771,  2744,   674, 28705, 28740, 28757,\n",
      "           401,  5550,   356,   272, 28705, 28750,   292,  9711,   325, 27902,\n",
      "         28731,    13,   496, 28707,  2440,   327,   285,   632, 28732,  1056,\n",
      "         28725, 11022, 28746, 28740, 28731,    13,    13, 28771, 27171,   272,\n",
      "          2787,   304,  3825,    13,  7841,  1610, 11096,  4472, 28723,  2834,\n",
      "         11096,   390, 17768,    13,   452, 28707, 28723, 11096, 28732,  1056,\n",
      "         28731,    13,   452, 28707, 28723, 11096, 28732,   496, 28707, 28723,\n",
      "           335,   632, 28732,   496, 28707,  2440,   743,    13,   452, 28707,\n",
      "         28723, 28744,  1559,   857,  2359,  1481,    13,   452, 28707, 28723,\n",
      "          4888,   470,    13, 13940, 28832,    13,    13, 12069,  5039,   369,\n",
      "           456,  2757, 27460,   272, 28705, 28750, 28757,  1178,  1552,  1056,\n",
      "         28832,   659,   865, 28705, 28740,  4419, 28725, 14030,   272, 11010,\n",
      "          9711, 28725,   579,   272,   401,  5550,   908, 28742, 28713,  1552,\n",
      "          8405, 28832,  5270,   349,   808,   298, 28705, 28740, 28723,  4840,\n",
      "         28725,   438,   272,   948, 28725,   272, 22668,   401,  5550,   349,\n",
      "          7589,   298,  9443,   272,  3493, 11010, 28723,     2]]) in 4.952735900878906 seconds\n"
     ]
    }
   ],
   "source": [
    "neuron_model.save('./solar_neuron_artifacts') # can be copied and used on a different neuron instance\n",
    "del neuron_model\n",
    "neuron_model = LlamaForSampling.from_pretrained('./SOLAR-10.7B-Instruct-v1.0-split', batch_size=1, tp_degree=24, amp='f16')\n",
    "neuron_model.load('solar_neuron_artifacts') # Load the compiled Neuron artifacts\n",
    "neuron_model.to_neuron() # will skip compile\n",
    "\n",
    "with torch.inference_mode():\n",
    "    start = time.time()\n",
    "    generated_sequences = neuron_model.sample(input_ids, sequence_length=2048, top_k=50)\n",
    "    elapsed = time.time() - start\n",
    "\n",
    "print(f'generated sequences {generated_sequences} in {elapsed} seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8ee1f6c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-Mar-29 08:16:13.375664 16557:16557 ERROR  TDRV:dmem_alloc_internal                     Failed to alloc DEVICE memory: 234881024\n",
      "2024-Mar-29 08:16:13.509359 16557:16557 ERROR  TDRV:dml_dump                                Wrote nrt memory alloc debug info to /tmp/nrt_mem_log_device_0_660678cd.csv\n",
      "2024-Mar-29 08:16:13.673313 16557:16557 ERROR  TDRV:log_dev_mem                             Failed to allocate 224.000MB (usage: tensors) on ND 0:NC 0, current utilization:\n",
      "\t* total: 15.782GB\n",
      "\t* tensors: 15.782GB\n",
      "\t* runtime: 1.062KB\n",
      "\t* dma rings: 32.000KB\n",
      "\n",
      "2024-Mar-29 08:16:13.979820 16557:16557 ERROR  TDRV:tensor_allocate                         Failed to allocate 234881024 bytes on DEVICE for tensor UNKNOWN.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Bad StatusOr access: RESOURCE_EXHAUSTED: AllocBuffer: error condition NRT_RESOURCE == rt_status: Not enough Neuron memory on core 0 for size=234881024",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m input_ids \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mencode(prompt, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch_neuronx\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m torch_neuronx_neuron_model \u001b[38;5;241m=\u001b[39m \u001b[43mtorch_neuronx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrace\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/aws_neuron_venv_pytorch/lib/python3.10/site-packages/torch_neuronx/xla_impl/trace.py:386\u001b[0m, in \u001b[0;36mtrace\u001b[0;34m(func, example_inputs, input_output_aliases, compiler_workdir, compiler_args, partitioner_config, *_, **kwargs)\u001b[0m\n\u001b[1;32m    381\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch_neuronx\u001b[38;5;241m.\u001b[39mpartition(\n\u001b[1;32m    382\u001b[0m         func, example_inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m(partitioner_config\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m)\n\u001b[1;32m    383\u001b[0m     )\n\u001b[1;32m    385\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context:\n\u001b[0;32m--> 386\u001b[0m     neff_filename, metaneff, flattener, packer \u001b[38;5;241m=\u001b[39m \u001b[43m_trace\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    387\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    388\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexample_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    389\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstates\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    390\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_output_aliases\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    391\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcompiler_workdir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    392\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcompiler_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    393\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    394\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    395\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m create_neuron_model(\n\u001b[1;32m    396\u001b[0m         neff_filename,\n\u001b[1;32m    397\u001b[0m         metaneff,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    401\u001b[0m         input_output_aliases,\n\u001b[1;32m    402\u001b[0m     )\n",
      "File \u001b[0;32m~/aws_neuron_venv_pytorch/lib/python3.10/site-packages/torch_neuronx/xla_impl/trace.py:447\u001b[0m, in \u001b[0;36m_trace\u001b[0;34m(func, example_inputs, states, input_output_aliases, compiler_workdir, compiler_args, options)\u001b[0m\n\u001b[1;32m    438\u001b[0m \u001b[38;5;66;03m# Convert the function to a HloProto message\u001b[39;00m\n\u001b[1;32m    439\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch_neuronx\u001b[38;5;241m.\u001b[39mcontexts\u001b[38;5;241m.\u001b[39mmock_neuron_cores(), revert_device_placement(func):\n\u001b[1;32m    440\u001b[0m     (\n\u001b[1;32m    441\u001b[0m         hlo,\n\u001b[1;32m    442\u001b[0m         input_parameter_names,\n\u001b[1;32m    443\u001b[0m         constant_parameter_tensors,\n\u001b[1;32m    444\u001b[0m         flattener,\n\u001b[1;32m    445\u001b[0m         packer,\n\u001b[1;32m    446\u001b[0m         updated_input_output_aliases,\n\u001b[0;32m--> 447\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[43mxla_trace\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    448\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    449\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexample_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    450\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstates\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    451\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_output_aliases\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    452\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    454\u001b[0m \u001b[38;5;66;03m# Create compiler directory if it does not exist\u001b[39;00m\n\u001b[1;32m    455\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(compiler_workdir):\n",
      "File \u001b[0;32m~/aws_neuron_venv_pytorch/lib/python3.10/site-packages/torch_neuronx/xla_impl/hlo_conversion.py:114\u001b[0m, in \u001b[0;36mxla_trace\u001b[0;34m(func, example_inputs, states, input_output_aliases)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m states \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    113\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m state \u001b[38;5;129;01min\u001b[39;00m states:\n\u001b[0;32m--> 114\u001b[0m         \u001b[43mplacement\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmove\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxla_device\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    116\u001b[0m aliased_inputs \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m    117\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m input_output_aliases:\n\u001b[1;32m    118\u001b[0m     \u001b[38;5;66;03m# Get the xla_tensor for the corresponding aliased input.\u001b[39;00m\n",
      "File \u001b[0;32m~/aws_neuron_venv_pytorch/lib/python3.10/site-packages/torch_neuronx/xla_impl/placement.py:51\u001b[0m, in \u001b[0;36mmove\u001b[0;34m(func, device)\u001b[0m\n\u001b[1;32m     49\u001b[0m     _move_script(func, device)\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func, torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mModule):\n\u001b[0;32m---> 51\u001b[0m     \u001b[43mfunc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func, torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mOptimizer):\n\u001b[1;32m     53\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m param_group \u001b[38;5;129;01min\u001b[39;00m func\u001b[38;5;241m.\u001b[39mparam_groups:\n",
      "File \u001b[0;32m~/aws_neuron_venv_pytorch/lib/python3.10/site-packages/transformers/modeling_utils.py:2460\u001b[0m, in \u001b[0;36mPreTrainedModel.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2455\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dtype_present_in_args:\n\u001b[1;32m   2456\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   2457\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou cannot cast a GPTQ model in a new `dtype`. Make sure to load the model using `from_pretrained` using the desired\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2458\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m `dtype` by passing the correct `torch_dtype` argument.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2459\u001b[0m         )\n\u001b[0;32m-> 2460\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/aws_neuron_venv_pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:1160\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1156\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1157\u001b[0m                     non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[1;32m   1158\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, non_blocking)\n\u001b[0;32m-> 1160\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/aws_neuron_venv_pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:810\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    808\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    809\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 810\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    812\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    813\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    814\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    815\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    820\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    821\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/aws_neuron_venv_pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:810\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    808\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    809\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 810\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    812\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    813\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    814\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    815\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    820\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    821\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "    \u001b[0;31m[... skipping similar frames: Module._apply at line 810 (2 times)]\u001b[0m\n",
      "File \u001b[0;32m~/aws_neuron_venv_pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:810\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    808\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    809\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 810\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    812\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    813\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    814\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    815\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    820\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    821\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/aws_neuron_venv_pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:833\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    829\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    830\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    831\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    832\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 833\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    834\u001b[0m should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    835\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m should_use_set_data:\n",
      "File \u001b[0;32m~/aws_neuron_venv_pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:1158\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1155\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m   1156\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1157\u001b[0m                 non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[0;32m-> 1158\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Bad StatusOr access: RESOURCE_EXHAUSTED: AllocBuffer: error condition NRT_RESOURCE == rt_status: Not enough Neuron memory on core 0 for size=234881024"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('upstage/SOLAR-10.7B-Instruct-v1.0')\n",
    "prompt = \"Hello, I'm a language model,\"\n",
    "input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "\n",
    "import torch_neuronx\n",
    "torch_neuronx_neuron_model = torch_neuronx.trace(model, input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a6c5ce0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (torch-neuronx)",
   "language": "python",
   "name": "aws_neuron_venv_pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
