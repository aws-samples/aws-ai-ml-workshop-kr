{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# [model_tuner]flan_t5_xl_with_LoRA_ml_g5_2xl\n",
    "\n",
    "ì´ sagemaker ì˜ˆì œì—ì„œëŠ” [Low-Rank Adaptation of Large Language Models (LoRA)](https://arxiv.org/abs/2106.09685)ë¥¼ ì ìš©í•˜ì—¬ ë‹¨ì¼ GPUì—ì„œ flan-t5-xlë¥¼ fine-tuní•˜ëŠ” ë°©ë²•ì— ëŒ€í•´ ì•Œì•„ë³¼ ê²ƒì…ë‹ˆë‹¤. \n",
    "Hugging Face [Transformers](https://huggingface.co/docs/transformers/index), [Accelerate](https://huggingface.co/docs/accelerate/index), [PEFT](https://github.com/huggingface/peft)ë¥¼ í™œìš©í•  ì˜ˆì •ì…ë‹ˆë‹¤.\n",
    "\n",
    "1. Setup Development Environment\n",
    "2. Load and prepare the dataset\n",
    "3. Fine-Tune flan-t5-xl with LoRA and bnb int-8 on Amazon SageMaker\n",
    "4. Deploy the model to Amazon SageMaker Endpoint\n",
    "\n",
    "### Quick intro: PEFT or Parameter Efficient Fine-tuning\n",
    "\n",
    "[PEFT](https://github.com/huggingface/peft), or Parameter Efficient Fine-tuningì€ Hugging Faceì˜ ì‹ ê·œ ì˜¤í”ˆì†ŒìŠ¤ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¡œ, ëª¨ë“  ëª¨ë¸ì˜ íŒŒë¼ë¯¸í„°ë¥¼ fine-tuningì—†ì´ ë‹¤ì–‘í•œ downstream applicationì— ëŒ€í•œ pre-trained language models (PLMs)ì„ íš¨ê³¼ì ìœ¼ë¡œ ì ìš©í•  ìˆ˜ ìˆê²Œ í•©ë‹ˆë‹¤. PEFTëŠ” í˜„ì¬ ë‹¤ìŒ techniquesì„ í¬í•¨í•˜ê³  ìˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "- LoRA:Â [LORA: LOW-RANK ADAPTATION OF LARGE LANGUAGE MODELS](https://arxiv.org/pdf/2106.09685.pdf)\n",
    "- Prefix Tuning:Â [P-Tuning v2: Prompt Tuning Can Be Comparable to Fine-tuning Universally Across Scales and Tasks](https://arxiv.org/pdf/2110.07602.pdf)\n",
    "- P-Tuning:Â [GPT Understands, Too](https://arxiv.org/pdf/2103.10385.pdf)\n",
    "- Prompt Tuning:Â [The Power of Scale for Parameter-Efficient Prompt Tuning](https://arxiv.org/pdf/2104.08691.pdf)\n",
    "- AdaLoRA: [Adaptive Budget Allocation for Parameter-Efficient Fine-Tuning](https://arxiv.org/pdf/2303.10512.pdf)\n",
    "\n",
    "[](https://github.com/huggingface/notebooks/blob/main/sagemaker/24_train_bloom_peft_lora/sagemaker-notebook.ipynb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# %%bash\n",
    "# #!/bin/bash\n",
    "\n",
    "# DAEMON_PATH=\"/etc/docker\"\n",
    "# MEMORY_SIZE=10G\n",
    "\n",
    "# FLAG=$(cat $DAEMON_PATH/daemon.json | jq 'has(\"data-root\")')\n",
    "# # echo $FLAG\n",
    "\n",
    "# if [ \"$FLAG\" == true ]; then\n",
    "#     echo \"Already revised\"\n",
    "# else\n",
    "#     echo \"Add data-root and default-shm-size=$MEMORY_SIZE\"\n",
    "#     sudo cp $DAEMON_PATH/daemon.json $DAEMON_PATH/daemon.json.bak\n",
    "#     sudo cat $DAEMON_PATH/daemon.json.bak | jq '. += {\"data-root\":\"/home/ec2-user/SageMaker/.container/docker\",\"default-shm-size\":\"'$MEMORY_SIZE'\"}' | sudo tee $DAEMON_PATH/daemon.json > /dev/null\n",
    "#     sudo service docker restart\n",
    "#     echo \"Docker Restart\"\n",
    "# fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets[s3] transformers sagemaker py7zr --upgrade --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip install \"transformers==4.26.0\" \"datasets[s3]==2.9.0\" sagemaker py7zr --upgrade --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ë¡œì»¬ í™˜ê²½ì—ì„œ SageMakerë¥¼ ì‚¬ìš©í•˜ë ¤ëŠ” ê²½ìš°. SageMakerì— í•„ìš”í•œ ê¶Œí•œì´ ìˆëŠ” IAM ì—­í• ì— ëŒ€í•œ ì•¡ì„¸ìŠ¤ ê¶Œí•œì´ í•„ìš”í•©ë‹ˆë‹¤. ìì„¸í•œ ë‚´ìš©ì€ [ì—¬ê¸°](https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-roles.html)ì—ì„œ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker role arn: arn:aws:iam::322537213286:role/service-role/AmazonSageMaker-ExecutionRole-20230528T120509\n",
      "sagemaker bucket: sagemaker-us-west-2-322537213286\n",
      "sagemaker session region: us-west-2\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "sess = sagemaker.Session()\n",
    "# sagemaker session bucket -> used for uploading data, models and logs\n",
    "# sagemaker will automatically create this bucket if it not exists\n",
    "sagemaker_session_bucket=None\n",
    "if sagemaker_session_bucket is None and sess is not None:\n",
    "    # set to default bucket if a bucket name is not given\n",
    "    sagemaker_session_bucket = sess.default_bucket()\n",
    "\n",
    "try:\n",
    "    role = sagemaker.get_execution_role()\n",
    "except ValueError:\n",
    "    iam = boto3.client('iam')\n",
    "    role = iam.get_role(RoleName='sagemaker_execution_role')['Role']['Arn']\n",
    "\n",
    "sess = sagemaker.Session(default_bucket=sagemaker_session_bucket)\n",
    "\n",
    "print(f\"sagemaker role arn: {role}\")\n",
    "print(f\"sagemaker bucket: {sess.default_bucket()}\")\n",
    "print(f\"sagemaker session region: {sess.boto_region_name}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load and prepare the dataset\n",
    "\n",
    "ì—¬ê¸°ì„œëŠ” ì•½ 16kê°œì˜ ë©”ì‹ ì € ëŒ€í™” ëª¨ìŒê³¼ ìš”ì•½ì´ í¬í•¨ëœ [samsum](https://huggingface.co/datasets/samsum) ë°ì´í„°ì…‹ì„ ì‚¬ìš©í•˜ê² ìŠµë‹ˆë‹¤. ëŒ€í™”ëŠ” ì˜ì–´ì— ëŠ¥í†µí•œ ì–¸ì–´í•™ìë“¤ì´ ì‘ì„±í•˜ê³  ê¸°ë¡í•˜ì˜€ìŠµë‹ˆë‹¤.\n",
    "\n",
    "```python\n",
    "{\n",
    "  \"id\": \"13818513\",\n",
    "  \"summary\": \"Amanda baked cookies and will bring Jerry some tomorrow.\",\n",
    "  \"dialogue\": \"Amanda: I baked cookies. Do you want some?\\r\\nJerry: Sure!\\r\\nAmanda: I'll bring you tomorrow :-)\"\n",
    "}\n",
    "```\n",
    "\n",
    "To load theÂ `samsum`Â ë°ì´í„°ì…‹ì„ ë¡œë“œí•˜ê¸° ìœ„í•´, ğŸ¤— Datasets ë¼ì´ë¸ŒëŸ¬ë¦¬ì—ì„œ `load_dataset()` ë©”ì†Œë“œë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset samsum (/root/.cache/huggingface/datasets/samsum/samsum/0.0.0/f1d7c6b7353e6de335d444e424dc002ef70d1277109031327bc9cc6af5d3d46e)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1a64ecd4b7e458ab41b4c10c3b93f15",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size: 14732\n",
      "Test dataset size: 819\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load dataset from the hub\n",
    "dataset = load_dataset(\"samsum\")\n",
    "\n",
    "print(f\"Train dataset size: {len(dataset['train'])}\")\n",
    "print(f\"Test dataset size: {len(dataset['test'])}\")\n",
    "# Train dataset size: 14732"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ëª¨ë¸ì„ í•™ìŠµí•˜ê¸° ìœ„í•´, ğŸ¤— Transformers Tokenizerë¥¼ ì´ìš©í•˜ì—¬ inputs (text)ë¥¼ token IDsë¡œ ë³€í™˜í•˜ê²Œ ë©ë‹ˆë‹¤. ì´ê²ƒì´ ì˜ë¯¸í•˜ëŠ” ê²ƒì„ ëª¨ë¥´ì‹ ë‹¤ë©´, the Hugging Face Courseì˜ **[chapter 6](https://huggingface.co/course/chapter6/1?fw=tf)**Â ì„ í™•ì¸í•˜ì‹œê¸° ë°”ëë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_id='google/flan-t5-xl'\n",
    "\n",
    "# Load tokenizer of flan-t5-xl\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "# tokenizer.model_max_length = 2048 # overwrite wrong value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "í•™ìŠµì„ ì‹œì‘í•˜ê¸° ì „ì— ë°ì´í„°ë¥¼ ì „ì²˜ë¦¬í•´ì•¼ í•©ë‹ˆë‹¤. Abstractive Summarizationê°€ í…ìŠ¤íŠ¸ ìƒì„± task ì…ë‹ˆë‹¤. ëª¨ë¸ì€ í…ìŠ¤íŠ¸ë¥¼ ì…ë ¥ìœ¼ë¡œ ë°›ì•„ summaryë¥¼ ì¶œë ¥ìœ¼ë¡œ ìƒì„±í•©ë‹ˆë‹¤. ë°ì´í„°ë¥¼ íš¨ìœ¨ì ìœ¼ë¡œ ì¼ê´„ ì²˜ë¦¬í•˜ê¸° ìœ„í•´ ì…ë ¥ê³¼ ì¶œë ¥ì— ê±¸ë¦¬ëŠ” ì‹œê°„ì„ íŒŒì•…í•˜ê³ ì í•©ë‹ˆë‹¤.\n",
    "\n",
    "ëª¨ë¸ì˜ ì„±ëŠ¥ì„ ê°œì„ í•˜ê¸° ìœ„í•´ instruct promptë¥¼ êµ¬ì„±í•˜ëŠ” ë° ì‚¬ìš©í•  `prompt_template`ë¥¼ ì •ì˜í•˜ì˜€ìŠµë‹ˆë‹¤.`prompt_template`ì€ ì‹œì‘ê³¼ ëì´ \"ê³ ì •\"ë˜ì–´ ìˆê³ , ë¬¸ì„œê°€ ì¤‘ê°„ì— ìˆìŠµë‹ˆë‹¤. ì¦‰, \"ê³ ì •ëœ\" í…œí”Œë¦¿ ë¶€ë¶„ + ë¬¸ì„œê°€ ëª¨ë¸ì˜ ìµœëŒ€ ê¸¸ì´ë¥¼ ì´ˆê³¼í•˜ì§€ ì•Šë„ë¡ í•´ì•¼ í•©ë‹ˆë‹¤. \n",
    "í•™ìŠµ ì „ì— ë°ì´í„° ì§‘í•©ì„ ì „ì²˜ë¦¬í•˜ê³  ë””ìŠ¤í¬ì— ì €ì¥í•œ ë‹¤ìŒ S3ì— ì—…ë¡œë“œí•©ë‹ˆë‹¤. ì´ ë‹¨ê³„ëŠ” ë¡œì»¬ ì»´í“¨í„° ë˜ëŠ” CPUì—ì„œ ì‹¤í–‰í•˜ê³  [Hugging Face Hub](https://huggingface.co/docs/hub/datasets-overview)ì— ì—…ë¡œë“œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/samsum/samsum/0.0.0/f1d7c6b7353e6de335d444e424dc002ef70d1277109031327bc9cc6af5d3d46e/cache-ccf0f9a49ab83084.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max source length: 255\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/14732 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max target length: 50\n"
     ]
    }
   ],
   "source": [
    "from datasets import concatenate_datasets\n",
    "import numpy as np\n",
    "\n",
    "# The maximum total input sequence length after tokenization.\n",
    "# Sequences longer than this will be truncated, sequences shorter will be padded.\n",
    "tokenized_inputs = dataset[\"train\"].map(lambda x: tokenizer(x[\"dialogue\"], truncation=True), batched=True, remove_columns=[\"dialogue\", \"summary\"])\n",
    "input_lengths = [len(x) for x in tokenized_inputs[\"input_ids\"]]\n",
    "\n",
    "# take 85 percentile of max length for better utilization\n",
    "max_source_length = int(np.percentile(input_lengths, 85))\n",
    "print(f\"Max source length: {max_source_length}\")\n",
    "\n",
    "# The maximum total sequence length for target text after tokenization.\n",
    "# Sequences longer than this will be truncated, sequences shorter will be padded.\"\n",
    "tokenized_targets = dataset[\"train\"].map(lambda x: tokenizer(x[\"summary\"], truncation=True), batched=True, remove_columns=[\"dialogue\", \"summary\"])\n",
    "target_lengths = [len(x) for x in tokenized_targets[\"input_ids\"]]\n",
    "# take 90 percentile of max length for better utilization\n",
    "max_target_length = int(np.percentile(target_lengths, 90))\n",
    "print(f\"Max target length: {max_target_length}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def preprocess_function(sample,padding=\"max_length\"):\n",
    "    # add prefix to the input for t5\n",
    "    inputs = [\"summarize: \" + item for item in sample[\"dialogue\"]]\n",
    "\n",
    "    # tokenize inputs\n",
    "    model_inputs = tokenizer(inputs, max_length=max_source_length, padding=padding, truncation=True)\n",
    "\n",
    "    # Tokenize targets with the `text_target` keyword argument\n",
    "    labels = tokenizer(text_target=sample[\"summary\"], max_length=max_target_length, padding=padding, truncation=True)\n",
    "\n",
    "    # If we are padding here, replace all tokenizer.pad_token_id in the labels by -100 when we want to ignore\n",
    "    # padding in the loss.\n",
    "    if padding == \"max_length\":\n",
    "        labels[\"input_ids\"] = [\n",
    "            [(l if l != tokenizer.pad_token_id else -100) for l in label] for label in labels[\"input_ids\"]\n",
    "        ]\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/samsum/samsum/0.0.0/f1d7c6b7353e6de335d444e424dc002ef70d1277109031327bc9cc6af5d3d46e/cache-e4c84224a7b3f37d.arrow\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/819 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/samsum/samsum/0.0.0/f1d7c6b7353e6de335d444e424dc002ef70d1277109031327bc9cc6af5d3d46e/cache-9fb0724aee9b2408.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keys of tokenized dataset: ['input_ids', 'attention_mask', 'labels']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/14732 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/819 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/819 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/819 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/819 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/819 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "uploaded data to:\n",
      "dataset to: s3://sagemaker-us-west-2-322537213286/processed/samsum-sagemaker/data\n"
     ]
    }
   ],
   "source": [
    "tokenized_dataset = dataset.map(preprocess_function, batched=True, remove_columns=[\"dialogue\", \"summary\", \"id\"])\n",
    "print(f\"Keys of tokenized dataset: {list(tokenized_dataset['train'].features)}\")\n",
    "\n",
    "# save train_dataset to s3\n",
    "training_input_path = f's3://{sess.default_bucket()}/processed/samsum-sagemaker/data'\n",
    "\n",
    "# save datasets to disk for later easy loading\n",
    "tokenized_dataset[\"train\"].save_to_disk(f\"{training_input_path}/tokenized_train\")\n",
    "\n",
    "## save test dataset without preprocessing to evaluate in the training script\n",
    "tokenized_dataset[\"test\"].save_to_disk(f\"{training_input_path}/tokenized_test\")\n",
    "dataset[\"test\"].save_to_disk(f\"{training_input_path}/test\")\n",
    "\n",
    "# save datasets to disk for local debugging\n",
    "tokenized_dataset[\"test\"].save_to_disk(\"./data/tokenized_train\")\n",
    "tokenized_dataset[\"test\"].save_to_disk(\"./data/tokenized_test\")\n",
    "dataset[\"test\"].save_to_disk(\"./data/test\")\n",
    "\n",
    "print(\"uploaded data to:\")\n",
    "print(f\"dataset to: {training_input_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ë°ì´í„°ì…‹ì„ ì²˜ë¦¬í•œ í›„ ìƒˆë¡œìš´ [FileSystem integration](https://huggingface.co/docs/datasets/filesystems)ì„ ì‚¬ìš©í•˜ì—¬ ë°ì´í„°ì…‹ì„ S3ì— ì—…ë¡œë“œí•  ê²ƒì…ë‹ˆë‹¤. ì—¬ê¸°ì„œëŠ” `sess.default_bucket()`ì„ ì‚¬ìš©í•˜ê³  ìˆìœ¼ë©°, ë°ì´í„°ì…‹ì„ ë‹¤ë¥¸ S3 ë²„í‚·ì— ì €ì¥í•˜ë ¤ë©´ ì´ ê°’ì„ ì¡°ì •í•©ë‹ˆë‹¤. ì´í›„ í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸ì—ì„œ S3 ê²½ë¡œë¥¼ ì‚¬ìš©í•  ê²ƒì…ë‹ˆë‹¤.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 3. Fine-Tune flan-t5-xl with LoRA and bnb int-8 on Amazon SageMaker\n",
    "\n",
    "LoRA ê¸°ë²• ì™¸ì—ë„ [bitsandbytes LLM.int8()](https://huggingface.co/blog/hf-bitsandbytes-integration)ì„ ì‚¬ìš©í•˜ì—¬ frozenëœ LLMì„ int8ë¡œ quantizeí•©ë‹ˆë‹¤. ì´ë¥¼ í†µí•´ flan-t5-xlì— í•„ìš”í•œ ë©”ëª¨ë¦¬ë¥¼ ìµœëŒ€ 4ë°°ê¹Œì§€ ì¤„ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. \n",
    "\n",
    "PEFTë¥¼ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ì„ í•™ìŠµí•˜ëŠ” [run_clm.py](./scripts/run_clm.py)ë¥¼ ì¤€ë¹„í–ˆìŠµë‹ˆë‹¤. ì–´ë–»ê²Œ ì‘ë™í•˜ëŠ”ì§€ ê¶ê¸ˆí•˜ë‹¤ë©´ [Efficient Large Language Model training with LoRA and Hugging Face](https://www.philschmid.de/fine-tune-flan-t5-peft) ë¸”ë¡œê·¸ì—ì„œ í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸ì— ëŒ€í•´ ìì„¸íˆ ì„¤ëª…ë˜ì–´ ìˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "SageMaker í•™ìŠµ ì‘ì—…ì„ ìƒì„±í•˜ê¸° ìœ„í•´ `HuggingFace` Estimatorê°€ í•„ìš”í•©ë‹ˆë‹¤. EstimatorëŠ” end-to-end Amazon SageMaker í•™ìŠµê³¼ ë°°í¬ ì‘ì—…ì„ ì²˜ë¦¬í•©ë‹ˆë‹¤. EstimatorëŠ” ì¸í”„ë¼ ì‚¬ìš©ì„ ê´€ë¦¬í•©ë‹ˆë‹¤. \n",
    "SageMakerëŠ” í•„ìš”í•œ ëª¨ë“  ec2 ì¸ìŠ¤í„´ìŠ¤ë¥¼ ì‹œì‘í•˜ê³  ê´€ë¦¬í•˜ë©°, ì˜¬ë°”ë¥¸ huggingface ì»¨í…Œì´ë„ˆë¥¼ ì œê³µí•˜ê³ , ì œê³µëœ ìŠ¤í¬ë¦½íŠ¸ë¥¼ ì—…ë¡œë“œí•˜ê³ , `/opt/ml/input/data`ì˜ ì»¨í…Œì´ë„ˆì— S3 ë²„í‚·ì—ì„œ ë°ì´í„°ë¥¼ ë‹¤ìš´ë¡œë“œí•©ë‹ˆë‹¤. ê·¸ëŸ° ë‹¤ìŒ ì‹¤í–‰ì„ í†µí•´ í•™ìŠµ ì‘ì—…ì„ ì‹œì‘í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "instance_type='ml.g5.2xlarge'\n",
    "# instance_type='local_gpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if instance_type in ['local', 'local_gpu']:\n",
    "    from sagemaker.local import LocalSession\n",
    "    sagemaker_session = LocalSession()\n",
    "    sagemaker_session.config = {'local': {'local_code': True}}\n",
    "    data_path=f'file://{Path.cwd()}/data'\n",
    "else:\n",
    "    sagemaker_session = sagemaker.session.Session()\n",
    "    data_path=training_input_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# define Training Job Name \n",
    "job_name = f'huggingface-peft-{time.strftime(\"%Y-%m-%d-%H-%M-%S\", time.localtime())}'\n",
    "\n",
    "from sagemaker.pytorch.estimator import PyTorch\n",
    "\n",
    "# hyperparameters, which are passed into the training job\n",
    "hyperparameters ={\n",
    "  'model_id': model_id,                                 # pre-trained model\n",
    "  # 'model_id': \"philschmid/flan-t5-xxl-sharded-fp16\",\n",
    "  'epochs': 1,                                          # number of training epochs\n",
    "  'per_device_train_batch_size': 100, #50, #15,         # batch size for training\n",
    "  'eval_sample': 50,                                    # batch size for evaluation\n",
    "  'lr': 2e-4,                                           # learning rate used during training\n",
    "}\n",
    "\n",
    "# create the Estimator\n",
    "estimator = PyTorch(\n",
    "    entry_point          = 'run_clm.py',               # train script\n",
    "    source_dir           = f'{Path.cwd()}/flan_t5_xl_with_LoRA',  # directory which includes all the files needed for training\n",
    "    instance_type        = instance_type,              # instances type used for the training job\n",
    "    instance_count       = 1,                          # the number of instances used for training\n",
    "    base_job_name        = job_name,                   # the name of the training job\n",
    "    role                 = role,                       # Iam role used in training job to access AWS ressources, e.g. S3\n",
    "    volume_size          = 300,                        # the size of the EBS volume in GB\n",
    "    framework_version    = '2.0',                      # the pytorch_version version used in the training job\n",
    "    py_version           = 'py310',                    # the python version used in the training job\n",
    "    sagemaker_session    = sagemaker_session,\n",
    "    hyperparameters      = hyperparameters\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ì´ì œ `.fit()` ë©”ì„œë“œê°€ í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸ì— S3 ê²½ë¡œë¥¼ ì „ë‹¬í•˜ì—¬ í•™ìŠµ ì‘ì—…ì„ ì‹œì‘í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:image_uri is not presented, retrieving image_uri based on instance_type, framework etc.\n",
      "INFO:sagemaker:Creating training-job with name: huggingface-peft-2023-06-26-01-45-31-2023-06-26-01-45-31-106\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using provided s3_resource\n"
     ]
    }
   ],
   "source": [
    "# define a data input dictonary with our uploaded s3 uris\n",
    "data = {\n",
    "    'tokenized_train': data_path+'/tokenized_train', \n",
    "    'tokenized_test': data_path+'/tokenized_test',\n",
    "    'test': data_path+'/test',\n",
    "}\n",
    "\n",
    "# starting the train job with our uploaded datasets as input\n",
    "estimator.fit(data, wait=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-06-26 01:45:31 Starting - Starting the training job...\n",
      "2023-06-26 01:45:48 Starting - Preparing the instances for training......\n",
      "2023-06-26 01:46:45 Downloading - Downloading input data...\n",
      "2023-06-26 01:47:06 Training - Downloading the training image...........................\n",
      "2023-06-26 01:51:42 Training - Training image download completed. Training in progress....\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2023-06-26 01:52:19,282 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2023-06-26 01:52:19,294 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-06-26 01:52:19,303 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2023-06-26 01:52:19,304 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2023-06-26 01:52:20,617 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.10 -m pip install -r requirements.txt\u001b[0m\n",
      "\u001b[34mCollecting peft==0.2.0 (from -r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34mDownloading peft-0.2.0-py3-none-any.whl (40 kB)\u001b[0m\n",
      "\u001b[34mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 40.3/40.3 kB 4.4 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting transformers==4.27.1 (from -r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[34mDownloading transformers-4.27.1-py3-none-any.whl (6.7 MB)\u001b[0m\n",
      "\u001b[34mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 6.7/6.7 MB 75.7 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting accelerate==0.17.1 (from -r requirements.txt (line 3))\u001b[0m\n",
      "\u001b[34mDownloading accelerate-0.17.1-py3-none-any.whl (212 kB)\u001b[0m\n",
      "\u001b[34mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 212.8/212.8 kB 53.8 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting bitsandbytes==0.37.1 (from -r requirements.txt (line 4))\u001b[0m\n",
      "\u001b[34mDownloading bitsandbytes-0.37.1-py3-none-any.whl (76.3 MB)\u001b[0m\n",
      "\u001b[34mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 76.3/76.3 MB 31.7 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting datasets (from -r requirements.txt (line 5))\u001b[0m\n",
      "\u001b[34mDownloading datasets-2.13.1-py3-none-any.whl (486 kB)\u001b[0m\n",
      "\u001b[34mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 486.2/486.2 kB 79.1 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting evaluate (from -r requirements.txt (line 6))\u001b[0m\n",
      "\u001b[34mDownloading evaluate-0.4.0-py3-none-any.whl (81 kB)\u001b[0m\n",
      "\u001b[34mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 81.4/81.4 kB 27.6 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting rouge_score (from -r requirements.txt (line 7))\u001b[0m\n",
      "\u001b[34mDownloading rouge_score-0.1.2.tar.gz (17 kB)\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): started\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from peft==0.2.0->-r requirements.txt (line 1)) (1.23.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from peft==0.2.0->-r requirements.txt (line 1)) (23.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from peft==0.2.0->-r requirements.txt (line 1)) (5.9.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from peft==0.2.0->-r requirements.txt (line 1)) (5.4.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: torch>=1.13.0 in /opt/conda/lib/python3.10/site-packages (from peft==0.2.0->-r requirements.txt (line 1)) (2.0.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers==4.27.1->-r requirements.txt (line 2)) (3.12.0)\u001b[0m\n",
      "\u001b[34mCollecting huggingface-hub<1.0,>=0.11.0 (from transformers==4.27.1->-r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[34mDownloading huggingface_hub-0.15.1-py3-none-any.whl (236 kB)\u001b[0m\n",
      "\u001b[34mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 236.8/236.8 kB 60.4 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting regex!=2019.12.17 (from transformers==4.27.1->-r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[34mDownloading regex-2023.6.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (770 kB)\u001b[0m\n",
      "\u001b[34mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 770.4/770.4 kB 97.8 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers==4.27.1->-r requirements.txt (line 2)) (2.28.2)\u001b[0m\n",
      "\u001b[34mCollecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers==4.27.1->-r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[34mDownloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\u001b[0m\n",
      "\u001b[34mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 7.8/7.8 MB 111.8 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers==4.27.1->-r requirements.txt (line 2)) (4.65.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyarrow>=8.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets->-r requirements.txt (line 5)) (12.0.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: dill<0.3.7,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets->-r requirements.txt (line 5)) (0.3.6)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets->-r requirements.txt (line 5)) (2.0.1)\u001b[0m\n",
      "\u001b[34mCollecting xxhash (from datasets->-r requirements.txt (line 5))\u001b[0m\n",
      "\u001b[34mDownloading xxhash-3.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (212 kB)\u001b[0m\n",
      "\u001b[34mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 212.5/212.5 kB 49.4 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets->-r requirements.txt (line 5)) (0.70.14)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: fsspec[http]>=2021.11.1 in /opt/conda/lib/python3.10/site-packages (from datasets->-r requirements.txt (line 5)) (2023.5.0)\u001b[0m\n",
      "\u001b[34mCollecting aiohttp (from datasets->-r requirements.txt (line 5))\u001b[0m\n",
      "\u001b[34mDownloading aiohttp-3.8.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\u001b[0m\n",
      "\u001b[34mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 1.0/1.0 MB 92.4 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting responses<0.19 (from evaluate->-r requirements.txt (line 6))\u001b[0m\n",
      "\u001b[34mDownloading responses-0.18.0-py3-none-any.whl (38 kB)\u001b[0m\n",
      "\u001b[34mCollecting absl-py (from rouge_score->-r requirements.txt (line 7))\u001b[0m\n",
      "\u001b[34mDownloading absl_py-1.4.0-py3-none-any.whl (126 kB)\u001b[0m\n",
      "\u001b[34mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 126.5/126.5 kB 32.2 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting nltk (from rouge_score->-r requirements.txt (line 7))\u001b[0m\n",
      "\u001b[34mDownloading nltk-3.8.1-py3-none-any.whl (1.5 MB)\u001b[0m\n",
      "\u001b[34mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 1.5/1.5 MB 110.1 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: six>=1.14.0 in /opt/conda/lib/python3.10/site-packages (from rouge_score->-r requirements.txt (line 7)) (1.16.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->-r requirements.txt (line 5)) (22.2.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: charset-normalizer<4.0,>=2.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->-r requirements.txt (line 5)) (3.1.0)\u001b[0m\n",
      "\u001b[34mCollecting multidict<7.0,>=4.5 (from aiohttp->datasets->-r requirements.txt (line 5))\u001b[0m\n",
      "\u001b[34mDownloading multidict-6.0.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB)\u001b[0m\n",
      "\u001b[34mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 114.5/114.5 kB 28.2 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting async-timeout<5.0,>=4.0.0a3 (from aiohttp->datasets->-r requirements.txt (line 5))\u001b[0m\n",
      "\u001b[34mDownloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\u001b[0m\n",
      "\u001b[34mCollecting yarl<2.0,>=1.0 (from aiohttp->datasets->-r requirements.txt (line 5))\u001b[0m\n",
      "\u001b[34mDownloading yarl-1.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (268 kB)\u001b[0m\n",
      "\u001b[34mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 268.8/268.8 kB 45.3 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting frozenlist>=1.1.1 (from aiohttp->datasets->-r requirements.txt (line 5))\u001b[0m\n",
      "\u001b[34mDownloading frozenlist-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (149 kB)\u001b[0m\n",
      "\u001b[34mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 149.6/149.6 kB 35.4 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting aiosignal>=1.1.2 (from aiohttp->datasets->-r requirements.txt (line 5))\u001b[0m\n",
      "\u001b[34mDownloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.11.0->transformers==4.27.1->-r requirements.txt (line 2)) (4.5.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.27.1->-r requirements.txt (line 2)) (3.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.27.1->-r requirements.txt (line 2)) (1.26.15)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.27.1->-r requirements.txt (line 2)) (2023.5.7)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.2.0->-r requirements.txt (line 1)) (1.11.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.2.0->-r requirements.txt (line 1)) (3.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.2.0->-r requirements.txt (line 1)) (3.1.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: click in /opt/conda/lib/python3.10/site-packages (from nltk->rouge_score->-r requirements.txt (line 7)) (8.1.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: joblib in /opt/conda/lib/python3.10/site-packages (from nltk->rouge_score->-r requirements.txt (line 7)) (1.2.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->-r requirements.txt (line 5)) (2.8.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->-r requirements.txt (line 5)) (2023.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->-r requirements.txt (line 5)) (2023.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.13.0->peft==0.2.0->-r requirements.txt (line 1)) (2.1.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.13.0->peft==0.2.0->-r requirements.txt (line 1)) (1.3.0)\u001b[0m\n",
      "\u001b[34mBuilding wheels for collected packages: rouge_score\u001b[0m\n",
      "\u001b[34mBuilding wheel for rouge_score (setup.py): started\u001b[0m\n",
      "\u001b[34mBuilding wheel for rouge_score (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCreated wheel for rouge_score: filename=rouge_score-0.1.2-py3-none-any.whl size=24934 sha256=f4d53622f0d6ef6154871e013fecb107add6d41c1e8b1013b1500988a945308e\u001b[0m\n",
      "\u001b[34mStored in directory: /root/.cache/pip/wheels/5f/dd/89/461065a73be61a532ff8599a28e9beef17985c9e9c31e541b4\u001b[0m\n",
      "\u001b[34mSuccessfully built rouge_score\u001b[0m\n",
      "\u001b[34mInstalling collected packages: tokenizers, bitsandbytes, xxhash, regex, multidict, frozenlist, async-timeout, absl-py, yarl, responses, nltk, huggingface-hub, aiosignal, transformers, rouge_score, aiohttp, accelerate, peft, datasets, evaluate\u001b[0m\n",
      "\u001b[34mAttempting uninstall: accelerate\u001b[0m\n",
      "\u001b[34mFound existing installation: accelerate 0.19.0\u001b[0m\n",
      "\u001b[34mUninstalling accelerate-0.19.0:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled accelerate-0.19.0\u001b[0m\n",
      "\u001b[34mSuccessfully installed absl-py-1.4.0 accelerate-0.17.1 aiohttp-3.8.4 aiosignal-1.3.1 async-timeout-4.0.2 bitsandbytes-0.37.1 datasets-2.13.1 evaluate-0.4.0 frozenlist-1.3.3 huggingface-hub-0.15.1 multidict-6.0.4 nltk-3.8.1 peft-0.2.0 regex-2023.6.3 responses-0.18.0 rouge_score-0.1.2 tokenizers-0.13.3 transformers-4.27.1 xxhash-3.2.0 yarl-1.9.2\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m2023-06-26 01:52:34,871 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[34m2023-06-26 01:52:34,871 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[34m2023-06-26 01:52:34,884 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-06-26 01:52:34,905 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-06-26 01:52:34,926 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-06-26 01:52:34,936 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[34mTraining Env:\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"test\": \"/opt/ml/input/data/test\",\n",
      "        \"tokenized_test\": \"/opt/ml/input/data/tokenized_test\",\n",
      "        \"tokenized_train\": \"/opt/ml/input/data/tokenized_train\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"current_instance_group\": \"homogeneousCluster\",\n",
      "    \"current_instance_group_hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"current_instance_type\": \"ml.g5.2xlarge\",\n",
      "    \"distribution_hosts\": [],\n",
      "    \"distribution_instance_groups\": [],\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"epochs\": 1,\n",
      "        \"eval_sample\": 50,\n",
      "        \"lr\": 0.0002,\n",
      "        \"model_id\": \"google/flan-t5-xl\",\n",
      "        \"per_device_train_batch_size\": 100\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"test\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        \"tokenized_test\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        \"tokenized_train\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"instance_groups\": [\n",
      "        \"homogeneousCluster\"\n",
      "    ],\n",
      "    \"instance_groups_dict\": {\n",
      "        \"homogeneousCluster\": {\n",
      "            \"instance_group_name\": \"homogeneousCluster\",\n",
      "            \"instance_type\": \"ml.g5.2xlarge\",\n",
      "            \"hosts\": [\n",
      "                \"algo-1\"\n",
      "            ]\n",
      "        }\n",
      "    },\n",
      "    \"is_hetero\": false,\n",
      "    \"is_master\": true,\n",
      "    \"is_modelparallel_enabled\": null,\n",
      "    \"is_smddpmprun_installed\": true,\n",
      "    \"job_name\": \"huggingface-peft-2023-06-26-01-45-31-2023-06-26-01-45-31-106\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-west-2-322537213286/huggingface-peft-2023-06-26-01-45-31-2023-06-26-01-45-31-106/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"run_clm\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 8,\n",
      "    \"num_gpus\": 1,\n",
      "    \"num_neurons\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.g5.2xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.g5.2xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"run_clm.py\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mEnvironment variables:\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"epochs\":1,\"eval_sample\":50,\"lr\":0.0002,\"model_id\":\"google/flan-t5-xl\",\"per_device_train_batch_size\":100}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=run_clm.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.g5.2xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.2xlarge\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"test\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"tokenized_test\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"tokenized_train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"test\",\"tokenized_test\",\"tokenized_train\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_TYPE=ml.g5.2xlarge\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP=homogeneousCluster\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS=[\"homogeneousCluster\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS_DICT={\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.2xlarge\"}}\u001b[0m\n",
      "\u001b[34mSM_DISTRIBUTION_INSTANCE_GROUPS=[]\u001b[0m\n",
      "\u001b[34mSM_IS_HETERO=false\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=run_clm\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=8\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=1\u001b[0m\n",
      "\u001b[34mSM_NUM_NEURONS=0\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-us-west-2-322537213286/huggingface-peft-2023-06-26-01-45-31-2023-06-26-01-45-31-106/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"test\":\"/opt/ml/input/data/test\",\"tokenized_test\":\"/opt/ml/input/data/tokenized_test\",\"tokenized_train\":\"/opt/ml/input/data/tokenized_train\"},\"current_host\":\"algo-1\",\"current_instance_group\":\"homogeneousCluster\",\"current_instance_group_hosts\":[\"algo-1\"],\"current_instance_type\":\"ml.g5.2xlarge\",\"distribution_hosts\":[],\"distribution_instance_groups\":[],\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"epochs\":1,\"eval_sample\":50,\"lr\":0.0002,\"model_id\":\"google/flan-t5-xl\",\"per_device_train_batch_size\":100},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"test\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"tokenized_test\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"tokenized_train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"instance_groups\":[\"homogeneousCluster\"],\"instance_groups_dict\":{\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.2xlarge\"}},\"is_hetero\":false,\"is_master\":true,\"is_modelparallel_enabled\":null,\"is_smddpmprun_installed\":true,\"job_name\":\"huggingface-peft-2023-06-26-01-45-31-2023-06-26-01-45-31-106\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-west-2-322537213286/huggingface-peft-2023-06-26-01-45-31-2023-06-26-01-45-31-106/source/sourcedir.tar.gz\",\"module_name\":\"run_clm\",\"network_interface_name\":\"eth0\",\"num_cpus\":8,\"num_gpus\":1,\"num_neurons\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.g5.2xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.2xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"run_clm.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--epochs\",\"1\",\"--eval_sample\",\"50\",\"--lr\",\"0.0002\",\"--model_id\",\"google/flan-t5-xl\",\"--per_device_train_batch_size\",\"100\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TEST=/opt/ml/input/data/test\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TOKENIZED_TEST=/opt/ml/input/data/tokenized_test\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TOKENIZED_TRAIN=/opt/ml/input/data/tokenized_train\u001b[0m\n",
      "\u001b[34mSM_HP_EPOCHS=1\u001b[0m\n",
      "\u001b[34mSM_HP_EVAL_SAMPLE=50\u001b[0m\n",
      "\u001b[34mSM_HP_LR=0.0002\u001b[0m\n",
      "\u001b[34mSM_HP_MODEL_ID=google/flan-t5-xl\u001b[0m\n",
      "\u001b[34mSM_HP_PER_DEVICE_TRAIN_BATCH_SIZE=100\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python310.zip:/opt/conda/lib/python3.10:/opt/conda/lib/python3.10/lib-dynload:/opt/conda/lib/python3.10/site-packages\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.10 run_clm.py --epochs 1 --eval_sample 50 --lr 0.0002 --model_id google/flan-t5-xl --per_device_train_batch_size 100\u001b[0m\n",
      "\u001b[34m2023-06-26 01:52:34,962 sagemaker-training-toolkit INFO     Exceptions not imported for SageMaker TF as Tensorflow is not installed.\u001b[0m\n",
      "\u001b[34m===================================BUG REPORT===================================\u001b[0m\n",
      "\u001b[34mWelcome to bitsandbytes. For bug reports, please submit your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\u001b[0m\n",
      "\u001b[34m================================================================================\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/local/nvidia/lib'), PosixPath('/usr/local/nvidia/lib64')}\n",
      "  warn(msg)\u001b[0m\n",
      "\u001b[34mCUDA SETUP: CUDA runtime path found: /opt/conda/lib/libcudart.so\u001b[0m\n",
      "\u001b[34mCUDA SETUP: Highest compute capability among GPUs detected: 8.6\u001b[0m\n",
      "\u001b[34mCUDA SETUP: Detected CUDA version 118\u001b[0m\n",
      "\u001b[34mCUDA SETUP: Loading binary /opt/conda/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda118.so...\u001b[0m\n",
      "\u001b[34mTrain_sagemaker\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)lve/main/config.json: 0.00B [00:00, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)lve/main/config.json: 1.44kB [00:00, 7.36MB/s]\u001b[0m\n",
      "\u001b[34mOverriding torch_dtype=None with `torch_dtype=torch.float16` due to requirements of `bitsandbytes` to enable model loading in mixed int8. Either pass torch_dtype=torch.float16 or don't pass this argument at all to remove this warning.\u001b[0m\n",
      "\u001b[34mOverriding torch_dtype=None with `torch_dtype=torch.float16` due to requirements of `bitsandbytes` to enable model loading in mixed int8. Either pass torch_dtype=torch.float16 or don't pass this argument at all to remove this warning.\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)model.bin.index.json: 0.00B [00:00, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)model.bin.index.json: 50.8kB [00:00, 148MB/s]\u001b[0m\n",
      "\u001b[34mDownloading shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)l-00001-of-00002.bin:   0%|          | 0.00/9.45G [00:00<?, ?B/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)l-00001-of-00002.bin:   0%|          | 21.0M/9.45G [00:00<01:01, 153MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)l-00001-of-00002.bin:   1%|          | 52.4M/9.45G [00:00<00:40, 230MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)l-00001-of-00002.bin:   1%|          | 105M/9.45G [00:00<00:27, 334MB/s] #033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)l-00001-of-00002.bin:   2%|â–         | 157M/9.45G [00:00<00:24, 384MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)l-00001-of-00002.bin:   2%|â–         | 210M/9.45G [00:00<00:22, 411MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)l-00001-of-00002.bin:   3%|â–         | 262M/9.45G [00:00<00:21, 427MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)l-00001-of-00002.bin:   3%|â–         | 315M/9.45G [00:00<00:21, 429MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)l-00001-of-00002.bin:   4%|â–         | 367M/9.45G [00:00<00:20, 442MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)l-00001-of-00002.bin:   4%|â–         | 419M/9.45G [00:01<00:20, 447MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)l-00001-of-00002.bin:   5%|â–         | 472M/9.45G [00:01<00:19, 451MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)l-00001-of-00002.bin:   6%|â–Œ         | 524M/9.45G [00:01<00:19, 453MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)l-00001-of-00002.bin:   6%|â–Œ         | 577M/9.45G [00:01<00:19, 456MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)l-00001-of-00002.bin:   7%|â–‹         | 629M/9.45G [00:01<00:19, 455MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)l-00001-of-00002.bin:   7%|â–‹         | 682M/9.45G [00:01<00:19, 456MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)l-00001-of-00002.bin:   8%|â–Š         | 734M/9.45G [00:01<00:19, 454MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)l-00001-of-00002.bin:   8%|â–Š         | 786M/9.45G [00:01<00:20, 429MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)l-00001-of-00002.bin:   9%|â–‰         | 839M/9.45G [00:01<00:20, 426MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)l-00001-of-00002.bin:   9%|â–‰         | 891M/9.45G [00:02<00:20, 421MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)l-00001-of-00002.bin:  10%|â–‰         | 944M/9.45G [00:02<00:20, 417MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)l-00001-of-00002.bin:  10%|â–ˆ         | 986M/9.45G [00:02<00:20, 415MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)l-00001-of-00002.bin:  11%|â–ˆ         | 1.04G/9.45G [00:02<00:19, 429MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)l-00001-of-00002.bin:  12%|â–ˆâ–        | 1.09G/9.45G [00:02<00:19, 438MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)l-00001-of-00002.bin:  12%|â–ˆâ–        | 1.14G/9.45G [00:02<00:18, 445MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)l-00001-of-00002.bin:  13%|â–ˆâ–        | 1.20G/9.45G [00:02<00:18, 449MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)l-00001-of-00002.bin:  13%|â–ˆâ–        | 1.25G/9.45G [00:02<00:18, 443MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)l-00001-of-00002.bin:  14%|â–ˆâ–        | 1.30G/9.45G [00:03<00:18, 448MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)l-00001-of-00002.bin:  14%|â–ˆâ–        | 1.35G/9.45G [00:03<00:17, 451MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)l-00001-of-00002.bin:  15%|â–ˆâ–        | 1.41G/9.45G [00:03<00:17, 454MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)l-00001-of-00002.bin:  15%|â–ˆâ–Œ        | 1.46G/9.45G [00:03<00:17, 454MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)l-00001-of-00002.bin:  16%|â–ˆâ–Œ        | 1.51G/9.45G [00:03<00:17, 454MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)l-00001-of-00002.bin:  17%|â–ˆâ–‹        | 1.56G/9.45G [00:03<00:17, 455MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)l-00001-of-00002.bin:  17%|â–ˆâ–‹        | 1.61G/9.45G [00:03<00:17, 453MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)l-00001-of-00002.bin:  18%|â–ˆâ–Š        | 1.67G/9.45G [00:03<00:17, 454MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)l-00001-of-00002.bin:  18%|â–ˆâ–Š        | 1.72G/9.45G [00:03<00:17, 453MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)l-00001-of-00002.bin:  19%|â–ˆâ–‰        | 1.77G/9.45G [00:04<00:16, 454MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)l-00001-of-00002.bin:  19%|â–ˆâ–‰        | 1.82G/9.45G [00:04<00:16, 455MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)l-00001-of-00002.bin:  20%|â–ˆâ–‰        | 1.88G/9.45G [00:04<00:16, 456MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)l-00001-of-00002.bin:  20%|â–ˆâ–ˆ        | 1.93G/9.45G [00:04<00:16, 455MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)l-00001-of-00002.bin:  21%|â–ˆâ–ˆ        | 1.98G/9.45G [00:04<00:16, 455MB/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)l-00001-of-00002.bin:  22%|â–ˆâ–ˆâ–       | 2.03G/9.45G [00:04<00:16, 451MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)l-00001-of-00002.bin:  22%|â–ˆâ–ˆâ–       | 2.09G/9.45G [00:04<00:16, 454MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)l-00001-of-00002.bin:  23%|â–ˆâ–ˆâ–       | 2.14G/9.45G [00:04<00:16, 454MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)l-00001-of-00002.bin:  23%|â–ˆâ–ˆâ–       | 2.19G/9.45G [00:05<00:16, 450MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)l-00001-of-00002.bin:  24%|â–ˆâ–ˆâ–       | 2.24G/9.45G [00:05<00:16, 444MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)l-00001-of-00002.bin:  24%|â–ˆâ–ˆâ–       | 2.30G/9.45G [00:05<00:15, 447MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)l-00001-of-00002.bin:  25%|â–ˆâ–ˆâ–       | 2.35G/9.45G [00:05<00:15, 449MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)l-00001-of-00002.bin:  25%|â–ˆâ–ˆâ–Œ       | 2.40G/9.45G [00:05<00:15, 450MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)l-00001-of-00002.bin:  26%|â–ˆâ–ˆâ–Œ       | 2.45G/9.45G [00:05<00:15, 453MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)l-00001-of-00002.bin:  27%|â–ˆâ–ˆâ–‹       | 2.51G/9.45G [00:05<00:15, 454MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)l-00001-of-00002.bin:  27%|â–ˆâ–ˆâ–‹       | 2.56G/9.45G [00:05<00:15, 450MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)l-00001-of-00002.bin:  28%|â–ˆâ–ˆâ–Š       | 2.61G/9.45G [00:05<00:15, 451MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)l-00001-of-00002.bin:  28%|â–ˆâ–ˆâ–Š       | 2.66G/9.45G [00:06<00:14, 455MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)l-00001-of-00002.bin:  29%|â–ˆâ–ˆâ–Š       | 2.72G/9.45G [00:06<00:14, 456MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)l-00001-of-00002.bin:  29%|â–ˆâ–ˆâ–‰       | 2.77G/9.45G [00:06<00:14, 454MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)l-00001-of-00002.bin:  30%|â–ˆâ–ˆâ–‰       | 2.82G/9.45G [00:06<00:15, 433MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)l-00001-of-00002.bin:  30%|â–ˆâ–ˆâ–ˆ       | 2.87G/9.45G [00:06<00:15, 419MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)l-00001-of-00002.bin:  31%|â–ˆâ–ˆâ–ˆ       | 2.93G/9.45G [00:06<00:15, 429MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)l-00001-of-00002.bin:  32%|â–ˆâ–ˆâ–ˆâ–      | 2.98G/9.45G [00:06<00:14, 436MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)l-00001-of-00002.bin:  32%|â–ˆâ–ˆâ–ˆâ–      | 3.03G/9.45G [00:06<00:14, 443MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)l-00001-of-00002.bin:  33%|â–ˆâ–ˆâ–ˆâ–      | 3.08G/9.45G [00:07<00:14, 448MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)l-00001-of-00002.bin:  33%|â–ˆâ–ˆâ–ˆâ–      | 3.14G/9.45G [00:07<00:14, 435MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)l-00001-of-00002.bin:  34%|â–ˆâ–ˆâ–ˆâ–      | 3.19G/9.45G [00:07<00:15, 409MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)l-00001-of-00002.bin:  34%|â–ˆâ–ˆâ–ˆâ–      | 3.24G/9.45G [00:07<00:14, 422MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)l-00001-of-00002.bin:  35%|â–ˆâ–ˆâ–ˆâ–      | 3.29G/9.45G [00:07<00:14, 431MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)l-00001-of-00002.bin:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 3.34G/9.45G [00:07<00:13, 436MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)l-00001-of-00002.bin:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 3.40G/9.45G [00:07<00:13, 443MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)l-00001-of-00002.bin:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 3.45G/9.45G [00:07<00:14, 412MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)l-00001-of-00002.bin:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 3.50G/9.45G [00:08<00:14, 420MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)l-00001-of-00002.bin:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 3.55G/9.45G [00:08<00:13, 423MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)l-00001-of-00002.bin:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 3.61G/9.45G [00:08<00:13, 435MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)l-00001-of-00002.bin:  39%|â–ˆâ–ˆâ–ˆâ–Š      | 3.66G/9.45G [00:08<00:13, 443MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)l-00001-of-00002.bin:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 3.71G/9.45G [00:08<00:12, 451MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)l-00001-of-00002.bin:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 3.76G/9.45G [00:08<00:12, 455MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)l-00001-of-00002.bin:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 3.82G/9.45G [00:08<00:12, 455MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)l-00001-of-00002.bin:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 3.87G/9.45G [00:08<00:12, 456MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)l-00001-of-00002.bin:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 3.92G/9.45G [00:08<00:12, 457MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)l-00001-of-00002.bin:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 3.97G/9.45G [00:09<00:11, 457MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)l-00001-of-00002.bin:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 4.03G/9.45G [00:09<00:11, 455MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)l-00001-of-00002.bin:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 4.08G/9.45G [00:09<00:11, 456MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)l-00001-of-00002.bin:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 4.13G/9.45G [00:09<00:11, 457MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)l-00001-of-00002.bin:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 4.18G/9.45G [00:09<00:11, 460MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)l-00001-of-00002.bin:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 4.24G/9.45G [00:09<00:11, 459MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)l-00001-of-00002.bin:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 4.29G/9.45G [00:09<00:11, 458MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)l-00001-of-00002.bin:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 4.34G/9.45G [00:09<00:11, 458MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)l-00001-of-00002.bin:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 4.39G/9.45G [00:09<00:11, 457MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)l-00001-of-00002.bin:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 4.45G/9.45G [00:10<00:10, 456MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)l-00001-of-00002.bin:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 4.50G/9.45G [00:10<00:10, 454MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)l-00001-of-00002.bin:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 4.55G/9.45G [00:10<00:13, 372MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)l-00001-of-00002.bin:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 4.59G/9.45G [00:10<00:12, 377MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)l-00001-of-00002.bin:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 4.65G/9.45G [00:10<00:12, 398MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)l-00001-of-00002.bin:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 4.70G/9.45G [00:10<00:11, 415MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)l-00001-of-00002.bin:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 4.75G/9.45G [00:10<00:11, 427MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)l-00001-of-00002.bin:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 4.80G/9.45G [00:10<00:10, 436MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)l-00001-of-00002.bin:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 4.85G/9.45G [00:11<00:10, 443MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)l-00001-of-00002.bin:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 4.91G/9.45G [00:11<00:10, 449MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)l-00001-of-00002.bin:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 4.96G/9.45G [00:11<00:09, 453MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)l-00001-of-00002.bin:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 5.01G/9.45G [00:11<00:09, 454MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)l-00001-of-00002.bin:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 5.06G/9.45G [00:11<00:09, 457MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)l-00001-of-00002.bin:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 5.12G/9.45G [00:11<00:09, 455MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)l-00001-of-00002.bin:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 5.17G/9.45G [00:11<00:09, 453MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)l-00001-of-00002.bin:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 5.22G/9.45G [00:11<00:09, 453MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)l-00001-of-00002.bin:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 5.27G/9.45G [00:11<00:09, 454MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)l-00001-of-00002.bin:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 5.33G/9.45G [00:12<00:09, 455MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)l-00001-of-00002.bin:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 5.38G/9.45G [00:12<00:08, 454MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)l-00001-of-00002.bin:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 5.43G/9.45G [00:12<00:08, 454MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)l-00001-of-00002.bin:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 5.48G/9.45G [00:12<00:08, 453MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)l-00001-of-00002.bin:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 5.54G/9.45G [00:12<00:08, 454MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)l-00001-of-00002.bin:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 5.59G/9.45G [00:12<00:08, 456MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)l-00001-of-00002.bin:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 5.64G/9.45G [00:12<00:08, 453MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)l-00001-of-00002.bin:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 5.69G/9.45G [00:12<00:08, 452MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)l-00001-of-00002.bin:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 5.75G/9.45G [00:13<00:08, 439MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)l-00001-of-00002.bin:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 5.80G/9.45G [00:13<00:09, 373MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)l-00001-of-00002.bin:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 5.84G/9.45G [00:13<00:11, 324MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)l-00001-of-00002.bin:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 5.88G/9.45G [00:13<00:12, 297MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)l-00001-of-00002.bin:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 5.91G/9.45G [00:13<00:12, 286MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)l-00001-of-00002.bin:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 5.95G/9.45G [00:13<00:12, 271MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)l-00001-of-00002.bin:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 5.98G/9.45G [00:13<00:13, 266MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)l-00001-of-00002.bin:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 6.01G/9.45G [00:14<00:13, 258MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)l-00001-of-00002.bin:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 6.04G/9.45G [00:14<00:13, 252MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)l-00001-of-00002.bin:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 6.07G/9.45G [00:14<00:13, 246MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)l-00001-of-00002.bin:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 6.10G/9.45G [00:14<00:13, 248MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)l-00001-of-00002.bin:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 6.13G/9.45G [00:14<00:13, 247MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)l-00001-of-00002.bin:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 6.17G/9.45G [00:14<00:13, 243MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)l-00001-of-00002.bin:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 6.20G/9.45G [00:14<00:13, 239MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)l-00001-of-00002.bin:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 6.23G/9.45G [00:15<00:13, 242MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)l-00001-of-00002.bin:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 6.26G/9.45G [00:15<00:13, 242MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)l-00001-of-00002.bin:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 6.29G/9.45G [00:15<00:13, 241MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)l-00001-of-00002.bin:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 6.32G/9.45G [00:15<00:12, 242MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)l-00001-of-00002.bin:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 6.35G/9.45G [00:15<00:12, 242MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)l-00001-of-00002.bin:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 6.39G/9.45G [00:15<00:12, 246MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)l-00001-of-00002.bin:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 6.42G/9.45G [00:15<00:12, 244MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)l-00001-of-00002.bin:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 6.45G/9.45G [00:15<00:12, 244MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)l-00001-of-00002.bin:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 6.48G/9.45G [00:16<00:12, 241MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)l-00001-of-00002.bin:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 6.51G/9.45G [00:16<00:13, 214MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)l-00001-of-00002.bin:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 6.54G/9.45G [00:16<00:14, 203MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)l-00001-of-00002.bin:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 6.56G/9.45G [00:16<00:14, 193MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)l-00001-of-00002.bin:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 6.59G/9.45G [00:16<00:16, 173MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)l-00001-of-00002.bin:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 6.61G/9.45G [00:16<00:17, 166MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)l-00001-of-00002.bin:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 6.63G/9.45G [00:16<00:17, 164MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)l-00001-of-00002.bin:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 6.65G/9.45G [00:17<00:18, 149MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)l-00001-of-00002.bin:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 6.67G/9.45G [00:17<00:18, 149MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)l-00001-of-00002.bin:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 6.69G/9.45G [00:17<00:18, 152MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)l-00001-of-00002.bin:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 6.71G/9.45G [00:17<00:19, 140MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)l-00001-of-00002.bin:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 6.73G/9.45G [00:17<00:19, 141MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)l-00001-of-00002.bin:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 6.75G/9.45G [00:17<00:18, 144MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)l-00001-of-00002.bin:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 6.77G/9.45G [00:18<00:19, 136MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)l-00001-of-00002.bin:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 6.79G/9.45G [00:18<00:19, 136MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)l-00001-of-00002.bin:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 6.82G/9.45G [00:18<00:18, 143MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)l-00001-of-00002.bin:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 6.84G/9.45G [00:18<00:19, 135MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)l-00001-of-00002.bin:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 6.86G/9.45G [00:18<00:19, 130MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)l-00001-of-00002.bin:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 6.88G/9.45G [00:18<00:18, 140MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)l-00001-of-00002.bin:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 6.90G/9.45G [00:18<00:19, 133MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)l-00001-of-00002.bin:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 6.92G/9.45G [00:19<00:19, 128MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)l-00001-of-00002.bin:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 6.94G/9.45G [00:19<00:17, 142MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)l-00001-of-00002.bin:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 6.96G/9.45G [00:19<00:18, 137MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)l-00001-of-00002.bin:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 6.98G/9.45G [00:19<00:18, 131MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)l-00001-of-00002.bin:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 7.00G/9.45G [00:19<00:18, 135MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)l-00001-of-00002.bin:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 7.03G/9.45G [00:19<00:18, 134MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)l-00001-of-00002.bin:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 7.05G/9.45G [00:20<00:18, 129MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)l-00001-of-00002.bin:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 7.07G/9.45G [00:20<00:17, 135MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)l-00001-of-00002.bin:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 7.09G/9.45G [00:20<00:17, 133MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)l-00001-of-00002.bin:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 7.11G/9.45G [00:20<00:18, 128MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)l-00001-of-00002.bin:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 7.13G/9.45G [00:20<00:17, 134MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)l-00001-of-00002.bin:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 7.15G/9.45G [00:20<00:16, 137MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)l-00001-of-00002.bin:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 7.17G/9.45G [00:21<00:17, 131MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)l-00001-of-00002.bin:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 7.19G/9.45G [00:21<00:17, 132MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)l-00001-of-00002.bin:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 7.21G/9.45G [00:21<00:16, 136MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)l-00001-of-00002.bin:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 7.24G/9.45G [00:21<00:16, 131MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)l-00001-of-00002.bin:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 7.26G/9.45G [00:21<00:17, 127MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)l-00001-of-00002.bin:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 7.28G/9.45G [00:21<00:16, 135MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)l-00001-of-00002.bin:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 7.30G/9.45G [00:22<00:16, 131MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)l-00001-of-00002.bin:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 7.32G/9.45G [00:22<00:16, 127MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)l-00001-of-00002.bin:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 7.34G/9.45G [00:22<00:14, 141MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)l-00001-of-00002.bin:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 7.36G/9.45G [00:22<00:15, 136MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)l-00001-of-00002.bin:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 7.38G/9.45G [00:22<00:15, 130MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)l-00001-of-00002.bin:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 7.40G/9.45G [00:22<00:15, 135MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)l-00001-of-00002.bin:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 7.42G/9.45G [00:22<00:15, 133MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)l-00001-of-00002.bin:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 7.44G/9.45G [00:23<00:18, 107MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)l-00001-of-00002.bin:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 7.49G/9.45G [00:23<00:12, 161MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)l-00001-of-00002.bin:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 7.51G/9.45G [00:23<00:13, 148MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)l-00001-of-00002.bin:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 7.53G/9.45G [00:23<00:13, 140MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)l-00001-of-00002.bin:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 7.55G/9.45G [00:23<00:13, 136MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)l-00001-of-00002.bin:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 7.57G/9.45G [00:24<00:14, 134MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)l-00001-of-00002.bin:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 7.59G/9.45G [00:24<00:14, 129MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)l-00001-of-00002.bin:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 7.61G/9.45G [00:24<00:14, 126MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)l-00001-of-00002.bin:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 7.63G/9.45G [00:24<00:15, 120MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)l-00001-of-00002.bin:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 7.65G/9.45G [00:24<00:15, 119MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)l-00001-of-00002.bin:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 7.68G/9.45G [00:24<00:14, 122MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)l-00001-of-00002.bin:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 7.70G/9.45G [00:25<00:14, 119MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)l-00001-of-00002.bin:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 7.72G/9.45G [00:25<00:14, 121MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)l-00001-of-00002.bin:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 7.74G/9.45G [00:25<00:14, 121MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)l-00001-of-00002.bin:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 7.76G/9.45G [00:25<00:14, 121MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)l-00001-of-00002.bin:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 7.78G/9.45G [00:25<00:13, 120MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)l-00001-of-00002.bin:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 7.80G/9.45G [00:25<00:13, 123MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)l-00001-of-00002.bin:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 7.82G/9.45G [00:26<00:13, 122MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)l-00001-of-00002.bin:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 7.84G/9.45G [00:26<00:13, 121MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)l-00001-of-00002.bin:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 7.86G/9.45G [00:26<00:13, 120MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)l-00001-of-00002.bin:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 7.89G/9.45G [00:26<00:13, 120MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)l-00001-of-00002.bin:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 7.91G/9.45G [00:26<00:13, 116MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)l-00001-of-00002.bin:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 7.93G/9.45G [00:26<00:12, 123MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)l-00001-of-00002.bin:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 7.95G/9.45G [00:27<00:12, 121MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)l-00001-of-00002.bin:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 7.97G/9.45G [00:27<00:12, 121MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)l-00001-of-00002.bin:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 7.99G/9.45G [00:27<00:12, 120MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)l-00001-of-00002.bin:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 8.01G/9.45G [00:27<00:11, 123MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)l-00001-of-00002.bin:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 8.03G/9.45G [00:27<00:11, 122MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)l-00001-of-00002.bin:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 8.05G/9.45G [00:28<00:11, 121MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)l-00001-of-00002.bin:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 8.07G/9.45G [00:28<00:11, 121MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)l-00001-of-00002.bin:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 8.10G/9.45G [00:28<00:10, 130MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)l-00001-of-00002.bin:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 8.12G/9.45G [00:28<00:10, 128MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)l-00001-of-00002.bin:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 8.14G/9.45G [00:28<00:10, 124MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)l-00001-of-00002.bin:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 8.16G/9.45G [00:28<00:09, 140MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)l-00001-of-00002.bin:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 8.18G/9.45G [00:28<00:09, 133MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)l-00001-of-00002.bin:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 8.20G/9.45G [00:29<00:09, 129MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)l-00001-of-00002.bin:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 8.22G/9.45G [00:29<00:08, 143MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)l-00001-of-00002.bin:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 8.24G/9.45G [00:29<00:08, 144MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)l-00001-of-00002.bin:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 8.26G/9.45G [00:29<00:08, 135MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)l-00001-of-00002.bin:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 8.28G/9.45G [00:29<00:08, 138MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)l-00001-of-00002.bin:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 8.30G/9.45G [00:29<00:07, 152MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)l-00001-of-00002.bin:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 8.33G/9.45G [00:29<00:08, 140MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)l-00001-of-00002.bin:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 8.35G/9.45G [00:30<00:08, 133MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)l-00001-of-00002.bin:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 8.38G/9.45G [00:30<00:06, 154MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)l-00001-of-00002.bin:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 8.40G/9.45G [00:30<00:07, 143MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)l-00001-of-00002.bin:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 8.42G/9.45G [00:30<00:07, 140MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)l-00001-of-00002.bin:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 8.44G/9.45G [00:30<00:06, 153MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)l-00001-of-00002.bin:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 8.46G/9.45G [00:30<00:06, 145MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)l-00001-of-00002.bin:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 8.48G/9.45G [00:31<00:07, 136MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)l-00001-of-00002.bin:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 8.50G/9.45G [00:31<00:06, 142MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)l-00001-of-00002.bin:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 8.52G/9.45G [00:31<00:06, 146MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)l-00001-of-00002.bin:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 8.55G/9.45G [00:31<00:06, 136MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)l-00001-of-00002.bin:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 8.57G/9.45G [00:31<00:06, 137MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)l-00001-of-00002.bin:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 8.59G/9.45G [00:31<00:05, 149MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)l-00001-of-00002.bin:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 8.61G/9.45G [00:31<00:06, 139MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)l-00001-of-00002.bin:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 8.63G/9.45G [00:32<00:06, 136MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)l-00001-of-00002.bin:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 8.65G/9.45G [00:32<00:05, 146MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)l-00001-of-00002.bin:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 8.67G/9.45G [00:32<00:05, 137MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)l-00001-of-00002.bin:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 8.69G/9.45G [00:32<00:05, 132MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)l-00001-of-00002.bin:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 8.71G/9.45G [00:32<00:05, 144MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)l-00001-of-00002.bin:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 8.73G/9.45G [00:32<00:04, 147MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)l-00001-of-00002.bin:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 8.76G/9.45G [00:33<00:05, 137MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)l-00001-of-00002.bin:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 8.78G/9.45G [00:33<00:04, 137MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)l-00001-of-00002.bin:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 8.80G/9.45G [00:33<00:04, 150MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)l-00001-of-00002.bin:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 8.82G/9.45G [00:33<00:04, 138MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)l-00001-of-00002.bin:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 8.84G/9.45G [00:33<00:04, 137MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)l-00001-of-00002.bin:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 8.86G/9.45G [00:33<00:04, 146MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)l-00001-of-00002.bin:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 8.88G/9.45G [00:33<00:04, 137MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)l-00001-of-00002.bin:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 8.90G/9.45G [00:34<00:04, 134MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)l-00001-of-00002.bin:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 8.92G/9.45G [00:34<00:03, 139MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)l-00001-of-00002.bin:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 8.94G/9.45G [00:34<00:03, 141MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)l-00001-of-00002.bin:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 8.97G/9.45G [00:34<00:03, 134MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)l-00001-of-00002.bin:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 8.99G/9.45G [00:34<00:03, 135MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)l-00001-of-00002.bin:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 9.01G/9.45G [00:34<00:03, 145MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)l-00001-of-00002.bin:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 9.03G/9.45G [00:35<00:03, 136MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)l-00001-of-00002.bin:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 9.05G/9.45G [00:35<00:02, 135MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)l-00001-of-00002.bin:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 9.07G/9.45G [00:35<00:02, 147MB/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)l-00001-of-00002.bin:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 9.09G/9.45G [00:35<00:02, 137MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)l-00001-of-00002.bin:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 9.11G/9.45G [00:35<00:02, 131MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)l-00001-of-00002.bin:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 9.13G/9.45G [00:35<00:02, 144MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)l-00001-of-00002.bin:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 9.15G/9.45G [00:35<00:02, 136MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)l-00001-of-00002.bin:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 9.18G/9.45G [00:36<00:02, 130MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)l-00001-of-00002.bin:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 9.20G/9.45G [00:36<00:01, 139MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)l-00001-of-00002.bin:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 9.22G/9.45G [00:36<00:01, 147MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)l-00001-of-00002.bin:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 9.24G/9.45G [00:36<00:01, 137MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)l-00001-of-00002.bin:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 9.26G/9.45G [00:36<00:01, 135MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)l-00001-of-00002.bin:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 9.28G/9.45G [00:36<00:01, 144MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)l-00001-of-00002.bin:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 9.30G/9.45G [00:37<00:01, 112MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)l-00001-of-00002.bin:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 9.33G/9.45G [00:37<00:00, 142MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)l-00001-of-00002.bin:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 9.35G/9.45G [00:37<00:00, 142MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)l-00001-of-00002.bin:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 9.37G/9.45G [00:37<00:00, 134MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)l-00001-of-00002.bin:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 9.40G/9.45G [00:37<00:00, 135MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)l-00001-of-00002.bin: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 9.42G/9.45G [00:37<00:00, 143MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)l-00001-of-00002.bin: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 9.44G/9.45G [00:38<00:00, 135MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)l-00001-of-00002.bin: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9.45G/9.45G [00:38<00:00, 248MB/s]\u001b[0m\n",
      "\u001b[34mDownloading shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:38<00:38, 38.32s/it]\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)l-00002-of-00002.bin:   0%|          | 0.00/1.95G [00:00<?, ?B/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)l-00002-of-00002.bin:   2%|â–         | 41.9M/1.95G [00:00<00:05, 360MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)l-00002-of-00002.bin:   4%|â–         | 83.9M/1.95G [00:00<00:10, 184MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)l-00002-of-00002.bin:   6%|â–Œ         | 115M/1.95G [00:00<00:11, 163MB/s] #033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)l-00002-of-00002.bin:   7%|â–‹         | 136M/1.95G [00:00<00:12, 148MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)l-00002-of-00002.bin:   8%|â–Š         | 157M/1.95G [00:00<00:11, 154MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)l-00002-of-00002.bin:   9%|â–‰         | 178M/1.95G [00:01<00:12, 147MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)l-00002-of-00002.bin:  10%|â–ˆ         | 199M/1.95G [00:01<00:12, 138MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)l-00002-of-00002.bin:  11%|â–ˆâ–        | 220M/1.95G [00:01<00:12, 138MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)l-00002-of-00002.bin:  12%|â–ˆâ–        | 241M/1.95G [00:01<00:12, 140MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)l-00002-of-00002.bin:  13%|â–ˆâ–        | 262M/1.95G [00:01<00:11, 146MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)l-00002-of-00002.bin:  15%|â–ˆâ–        | 283M/1.95G [00:01<00:12, 137MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)l-00002-of-00002.bin:  16%|â–ˆâ–Œ        | 304M/1.95G [00:02<00:11, 145MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)l-00002-of-00002.bin:  17%|â–ˆâ–‹        | 325M/1.95G [00:02<00:11, 136MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)l-00002-of-00002.bin:  18%|â–ˆâ–Š        | 346M/1.95G [00:02<00:12, 131MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)l-00002-of-00002.bin:  19%|â–ˆâ–‰        | 367M/1.95G [00:02<00:11, 140MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)l-00002-of-00002.bin:  20%|â–ˆâ–‰        | 388M/1.95G [00:02<00:11, 133MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)l-00002-of-00002.bin:  21%|â–ˆâ–ˆ        | 409M/1.95G [00:02<00:11, 129MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)l-00002-of-00002.bin:  22%|â–ˆâ–ˆâ–       | 430M/1.95G [00:02<00:11, 136MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)l-00002-of-00002.bin:  23%|â–ˆâ–ˆâ–       | 451M/1.95G [00:03<00:11, 130MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)l-00002-of-00002.bin:  24%|â–ˆâ–ˆâ–       | 472M/1.95G [00:03<00:11, 127MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)l-00002-of-00002.bin:  25%|â–ˆâ–ˆâ–Œ       | 493M/1.95G [00:03<00:10, 136MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)l-00002-of-00002.bin:  26%|â–ˆâ–ˆâ–‹       | 514M/1.95G [00:03<00:10, 135MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)l-00002-of-00002.bin:  27%|â–ˆâ–ˆâ–‹       | 535M/1.95G [00:03<00:10, 130MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)l-00002-of-00002.bin:  29%|â–ˆâ–ˆâ–Š       | 556M/1.95G [00:03<00:10, 132MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)l-00002-of-00002.bin:  30%|â–ˆâ–ˆâ–‰       | 577M/1.95G [00:04<00:09, 143MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)l-00002-of-00002.bin:  31%|â–ˆâ–ˆâ–ˆ       | 598M/1.95G [00:04<00:10, 135MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)l-00002-of-00002.bin:  32%|â–ˆâ–ˆâ–ˆâ–      | 619M/1.95G [00:04<00:09, 141MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)l-00002-of-00002.bin:  33%|â–ˆâ–ˆâ–ˆâ–      | 640M/1.95G [00:04<00:09, 142MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)l-00002-of-00002.bin:  34%|â–ˆâ–ˆâ–ˆâ–      | 661M/1.95G [00:04<00:09, 135MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)l-00002-of-00002.bin:  35%|â–ˆâ–ˆâ–ˆâ–      | 682M/1.95G [00:04<00:09, 129MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)l-00002-of-00002.bin:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 703M/1.95G [00:04<00:08, 139MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)l-00002-of-00002.bin:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 724M/1.95G [00:05<00:09, 133MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)l-00002-of-00002.bin:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 744M/1.95G [00:05<00:09, 128MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)l-00002-of-00002.bin:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 765M/1.95G [00:05<00:08, 141MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)l-00002-of-00002.bin:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 786M/1.95G [00:05<00:08, 134MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)l-00002-of-00002.bin:  41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 807M/1.95G [00:05<00:08, 129MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)l-00002-of-00002.bin:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 828M/1.95G [00:05<00:08, 135MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)l-00002-of-00002.bin:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 849M/1.95G [00:06<00:08, 134MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)l-00002-of-00002.bin:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 870M/1.95G [00:06<00:08, 129MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)l-00002-of-00002.bin:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 891M/1.95G [00:06<00:08, 131MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)l-00002-of-00002.bin:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 912M/1.95G [00:06<00:07, 138MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)l-00002-of-00002.bin:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 933M/1.95G [00:06<00:07, 131MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)l-00002-of-00002.bin:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 954M/1.95G [00:06<00:07, 127MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)l-00002-of-00002.bin:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 975M/1.95G [00:07<00:06, 142MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)l-00002-of-00002.bin:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 996M/1.95G [00:07<00:07, 134MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)l-00002-of-00002.bin:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1.02G/1.95G [00:07<00:07, 129MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)l-00002-of-00002.bin:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1.04G/1.95G [00:07<00:06, 140MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)l-00002-of-00002.bin:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1.06G/1.95G [00:07<00:06, 134MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)l-00002-of-00002.bin:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1.08G/1.95G [00:07<00:06, 128MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)l-00002-of-00002.bin:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1.10G/1.95G [00:07<00:06, 140MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)l-00002-of-00002.bin:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1.12G/1.95G [00:08<00:06, 137MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)l-00002-of-00002.bin:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1.14G/1.95G [00:08<00:06, 131MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)l-00002-of-00002.bin:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1.16G/1.95G [00:08<00:05, 135MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)l-00002-of-00002.bin:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1.18G/1.95G [00:08<00:05, 139MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)l-00002-of-00002.bin:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1.21G/1.95G [00:08<00:05, 133MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)l-00002-of-00002.bin:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1.23G/1.95G [00:08<00:05, 133MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)l-00002-of-00002.bin:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1.25G/1.95G [00:09<00:06, 108MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)l-00002-of-00002.bin:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1.28G/1.95G [00:09<00:04, 138MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)l-00002-of-00002.bin:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 1.30G/1.95G [00:09<00:04, 147MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)l-00002-of-00002.bin:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 1.32G/1.95G [00:09<00:04, 143MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)l-00002-of-00002.bin:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 1.34G/1.95G [00:09<00:04, 135MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)l-00002-of-00002.bin:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 1.36G/1.95G [00:09<00:04, 135MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)l-00002-of-00002.bin:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 1.38G/1.95G [00:10<00:04, 140MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)l-00002-of-00002.bin:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1.41G/1.95G [00:10<00:04, 133MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)l-00002-of-00002.bin:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1.43G/1.95G [00:10<00:03, 133MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)l-00002-of-00002.bin:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1.45G/1.95G [00:10<00:03, 138MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)l-00002-of-00002.bin:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1.47G/1.95G [00:10<00:03, 132MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)l-00002-of-00002.bin:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1.49G/1.95G [00:10<00:03, 128MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)l-00002-of-00002.bin:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1.51G/1.95G [00:11<00:03, 143MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)l-00002-of-00002.bin:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1.53G/1.95G [00:11<00:03, 135MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)l-00002-of-00002.bin:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1.55G/1.95G [00:11<00:03, 129MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)l-00002-of-00002.bin:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 1.57G/1.95G [00:11<00:02, 142MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)l-00002-of-00002.bin:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1.59G/1.95G [00:11<00:02, 135MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)l-00002-of-00002.bin:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1.61G/1.95G [00:11<00:02, 129MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)l-00002-of-00002.bin:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1.64G/1.95G [00:11<00:02, 138MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)l-00002-of-00002.bin:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1.66G/1.95G [00:12<00:02, 135MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)l-00002-of-00002.bin:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1.68G/1.95G [00:12<00:02, 130MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)l-00002-of-00002.bin:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 1.70G/1.95G [00:12<00:01, 136MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)l-00002-of-00002.bin:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 1.72G/1.95G [00:12<00:01, 134MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)l-00002-of-00002.bin:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 1.74G/1.95G [00:12<00:01, 132MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)l-00002-of-00002.bin:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 1.76G/1.95G [00:12<00:01, 132MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)l-00002-of-00002.bin:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1.78G/1.95G [00:13<00:01, 139MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)l-00002-of-00002.bin:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1.80G/1.95G [00:13<00:01, 136MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)l-00002-of-00002.bin:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1.82G/1.95G [00:13<00:00, 130MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)l-00002-of-00002.bin:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1.85G/1.95G [00:13<00:00, 143MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)l-00002-of-00002.bin:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 1.87G/1.95G [00:13<00:00, 135MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)l-00002-of-00002.bin:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 1.89G/1.95G [00:13<00:00, 130MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)l-00002-of-00002.bin:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 1.91G/1.95G [00:13<00:00, 139MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)l-00002-of-00002.bin:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 1.93G/1.95G [00:14<00:00, 133MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)l-00002-of-00002.bin: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.95G/1.95G [00:14<00:00, 130MB/s]#033[A\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)l-00002-of-00002.bin: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.95G/1.95G [00:14<00:00, 136MB/s]\u001b[0m\n",
      "\u001b[34mDownloading shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:52<00:00, 24.29s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:52<00:00, 26.40s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [01:17<01:17, 77.41s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [01:26<00:00, 37.22s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [01:26<00:00, 43.25s/it]\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)neration_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)neration_config.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 147/147 [00:00<00:00, 1.70MB/s]\u001b[0m\n",
      "\u001b[34mtrainable params: 9437184 || all params: 2859194368 || trainable%: 0.33006444422319176\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)okenizer_config.json: 0.00B [00:00, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)okenizer_config.json: 2.54kB [00:00, 16.5MB/s]\u001b[0m\n",
      "\u001b[34mDownloading spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading spiece.model: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 792k/792k [00:00<00:00, 401MB/s]\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)/main/tokenizer.json: 0.00B [00:00, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)/main/tokenizer.json: 2.42MB [00:00, 33.1MB/s]\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)cial_tokens_map.json: 0.00B [00:00, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading (â€¦)cial_tokens_map.json: 2.20kB [00:00, 15.8MB/s]\u001b[0m\n",
      "\u001b[34m0%|          | 0/74 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mYou're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\u001b[0m\n",
      "\u001b[34mYou're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:298: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:298: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\u001b[0m\n",
      "\u001b[34m1%|â–         | 1/74 [00:40<49:03, 40.33s/it]\u001b[0m\n",
      "\u001b[34m3%|â–         | 2/74 [01:12<42:26, 35.36s/it]\u001b[0m\n",
      "\u001b[34m4%|â–         | 3/74 [01:44<39:57, 33.77s/it]\u001b[0m\n",
      "\u001b[34m5%|â–Œ         | 4/74 [02:15<38:31, 33.02s/it]\u001b[0m\n",
      "\u001b[34m7%|â–‹         | 5/74 [02:47<37:29, 32.61s/it]\u001b[0m\n",
      "\u001b[34m8%|â–Š         | 6/74 [03:19<36:41, 32.37s/it]\u001b[0m\n",
      "\u001b[34m9%|â–‰         | 7/74 [03:51<35:58, 32.21s/it]\u001b[0m\n",
      "\u001b[34m11%|â–ˆ         | 8/74 [04:23<35:19, 32.11s/it]\u001b[0m\n",
      "\u001b[34m12%|â–ˆâ–        | 9/74 [04:55<34:43, 32.05s/it]\u001b[0m\n",
      "\u001b[34m14%|â–ˆâ–        | 10/74 [05:27<34:07, 31.99s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2028, 'learning_rate': 0.000172972972972973, 'epoch': 0.14}\u001b[0m\n",
      "\u001b[34m14%|â–ˆâ–        | 10/74 [05:27<34:07, 31.99s/it]\u001b[0m\n",
      "\u001b[34m15%|â–ˆâ–        | 11/74 [05:59<33:33, 31.96s/it]\u001b[0m\n",
      "\u001b[34m16%|â–ˆâ–Œ        | 12/74 [06:31<32:59, 31.93s/it]\u001b[0m\n",
      "\u001b[34m18%|â–ˆâ–Š        | 13/74 [07:02<32:26, 31.92s/it]\u001b[0m\n",
      "\u001b[34m19%|â–ˆâ–‰        | 14/74 [07:34<31:54, 31.91s/it]\u001b[0m\n",
      "\u001b[34m20%|â–ˆâ–ˆ        | 15/74 [08:06<31:22, 31.91s/it]\u001b[0m\n",
      "\u001b[34m22%|â–ˆâ–ˆâ–       | 16/74 [08:38<30:49, 31.89s/it]\u001b[0m\n",
      "\u001b[34m23%|â–ˆâ–ˆâ–       | 17/74 [09:10<30:17, 31.89s/it]\u001b[0m\n",
      "\u001b[34m24%|â–ˆâ–ˆâ–       | 18/74 [09:42<29:45, 31.88s/it]\u001b[0m\n",
      "\u001b[34m26%|â–ˆâ–ˆâ–Œ       | 19/74 [10:14<29:13, 31.88s/it]\u001b[0m\n",
      "\u001b[34m27%|â–ˆâ–ˆâ–‹       | 20/74 [10:46<28:41, 31.89s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1626, 'learning_rate': 0.00014594594594594595, 'epoch': 0.27}\u001b[0m\n",
      "\u001b[34m27%|â–ˆâ–ˆâ–‹       | 20/74 [10:46<28:41, 31.89s/it]\u001b[0m\n",
      "\u001b[34m28%|â–ˆâ–ˆâ–Š       | 21/74 [11:17<28:09, 31.88s/it]\u001b[0m\n",
      "\u001b[34m30%|â–ˆâ–ˆâ–‰       | 22/74 [11:49<27:38, 31.88s/it]\u001b[0m\n",
      "\u001b[34m31%|â–ˆâ–ˆâ–ˆ       | 23/74 [12:21<27:06, 31.89s/it]\u001b[0m\n",
      "\u001b[34m32%|â–ˆâ–ˆâ–ˆâ–      | 24/74 [12:53<26:34, 31.90s/it]\u001b[0m\n",
      "\u001b[34m34%|â–ˆâ–ˆâ–ˆâ–      | 25/74 [13:25<26:02, 31.89s/it]\u001b[0m\n",
      "\u001b[34m35%|â–ˆâ–ˆâ–ˆâ–Œ      | 26/74 [13:57<25:30, 31.88s/it]\u001b[0m\n",
      "\u001b[34m36%|â–ˆâ–ˆâ–ˆâ–‹      | 27/74 [14:29<24:58, 31.89s/it]\u001b[0m\n",
      "\u001b[34m38%|â–ˆâ–ˆâ–ˆâ–Š      | 28/74 [15:01<24:27, 31.89s/it]\u001b[0m\n",
      "\u001b[34m39%|â–ˆâ–ˆâ–ˆâ–‰      | 29/74 [15:33<23:55, 31.90s/it]\u001b[0m\n",
      "\u001b[34m41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 30/74 [16:05<23:23, 31.89s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1571, 'learning_rate': 0.00011891891891891893, 'epoch': 0.41}\u001b[0m\n",
      "\u001b[34m41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 30/74 [16:05<23:23, 31.89s/it]\u001b[0m\n",
      "\u001b[34m42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 31/74 [16:36<22:51, 31.89s/it]\u001b[0m\n",
      "\u001b[34m43%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 32/74 [17:08<22:19, 31.89s/it]\u001b[0m\n",
      "\u001b[34m45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 33/74 [17:40<21:47, 31.89s/it]\u001b[0m\n",
      "\u001b[34m46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 34/74 [18:12<21:15, 31.89s/it]\u001b[0m\n",
      "\u001b[34m47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 35/74 [18:44<20:43, 31.89s/it]\u001b[0m\n",
      "\u001b[34m49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 36/74 [19:16<20:11, 31.89s/it]\u001b[0m\n",
      "\u001b[34m50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 37/74 [19:48<19:39, 31.88s/it]\u001b[0m\n",
      "\u001b[34m51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 38/74 [20:20<19:07, 31.89s/it]\u001b[0m\n",
      "\u001b[34m53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 39/74 [20:51<18:35, 31.88s/it]\u001b[0m\n",
      "\u001b[34m54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 40/74 [21:23<18:04, 31.89s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1438, 'learning_rate': 9.18918918918919e-05, 'epoch': 0.54}\u001b[0m\n",
      "\u001b[34m54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 40/74 [21:23<18:04, 31.89s/it]\u001b[0m\n",
      "\u001b[34m55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 41/74 [21:55<17:31, 31.87s/it]\u001b[0m\n",
      "\u001b[34m57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 42/74 [22:27<16:59, 31.87s/it]\u001b[0m\n",
      "\u001b[34m58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 43/74 [22:59<16:28, 31.88s/it]\u001b[0m\n",
      "\u001b[34m59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 44/74 [23:31<15:56, 31.88s/it]\u001b[0m\n",
      "\u001b[34m61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 45/74 [24:03<15:24, 31.88s/it]\u001b[0m\n",
      "\u001b[34m62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 46/74 [24:35<14:52, 31.88s/it]\u001b[0m\n",
      "\u001b[34m64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 47/74 [25:07<14:20, 31.89s/it]\u001b[0m\n",
      "\u001b[34m65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 48/74 [25:38<13:49, 31.89s/it]\u001b[0m\n",
      "\u001b[34m66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 49/74 [26:10<13:17, 31.89s/it]\u001b[0m\n",
      "\u001b[34m68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 50/74 [26:42<12:45, 31.89s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1257, 'learning_rate': 6.486486486486487e-05, 'epoch': 0.68}\u001b[0m\n",
      "\u001b[34m68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 50/74 [26:42<12:45, 31.89s/it]\u001b[0m\n",
      "\u001b[34m69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 51/74 [27:14<12:13, 31.89s/it]\u001b[0m\n",
      "\u001b[34m70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 52/74 [27:46<11:41, 31.89s/it]\u001b[0m\n",
      "\u001b[34m72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 53/74 [28:18<11:09, 31.88s/it]\u001b[0m\n",
      "\u001b[34m73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 54/74 [28:50<10:37, 31.88s/it]\u001b[0m\n",
      "\u001b[34m74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 55/74 [29:22<10:06, 31.90s/it]\u001b[0m\n",
      "\u001b[34m76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 56/74 [29:54<09:34, 31.89s/it]\u001b[0m\n",
      "\u001b[34m77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 57/74 [30:25<09:02, 31.89s/it]\u001b[0m\n",
      "\u001b[34m78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 58/74 [30:57<08:30, 31.88s/it]\u001b[0m\n",
      "\u001b[34m80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 59/74 [31:29<07:58, 31.89s/it]\u001b[0m\n",
      "\u001b[34m81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 60/74 [32:01<07:26, 31.88s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1171, 'learning_rate': 3.783783783783784e-05, 'epoch': 0.81}\u001b[0m\n",
      "\u001b[34m81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 60/74 [32:01<07:26, 31.88s/it]\u001b[0m\n",
      "\u001b[34m82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 61/74 [32:33<06:54, 31.88s/it]\u001b[0m\n",
      "\u001b[34m84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 62/74 [33:05<06:22, 31.88s/it]\u001b[0m\n",
      "\u001b[34m85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 63/74 [33:37<05:50, 31.88s/it]\u001b[0m\n",
      "\u001b[34m86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 64/74 [34:09<05:18, 31.88s/it]\u001b[0m\n",
      "\u001b[34m88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 65/74 [34:40<04:46, 31.89s/it]\u001b[0m\n",
      "\u001b[34m89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 66/74 [35:12<04:15, 31.89s/it]\u001b[0m\n",
      "\u001b[34m91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 67/74 [35:44<03:43, 31.89s/it]\u001b[0m\n",
      "\u001b[34m92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 68/74 [36:16<03:11, 31.88s/it]\u001b[0m\n",
      "\u001b[34m93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 69/74 [36:48<02:39, 31.88s/it]\u001b[0m\n",
      "\u001b[34m95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 70/74 [37:20<02:07, 31.88s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1241, 'learning_rate': 1.0810810810810812e-05, 'epoch': 0.95}\u001b[0m\n",
      "\u001b[34m95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 70/74 [37:20<02:07, 31.88s/it]\u001b[0m\n",
      "\u001b[34m96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 71/74 [37:52<01:35, 31.89s/it]\u001b[0m\n",
      "\u001b[34m97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 72/74 [38:24<01:03, 31.89s/it]\u001b[0m\n",
      "\u001b[34m99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 73/74 [38:56<00:31, 31.89s/it]\u001b[0m\n",
      "\u001b[34m100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 74/74 [39:17<00:00, 28.71s/it]\u001b[0m\n",
      "\u001b[34m{'train_runtime': 2357.3767, 'train_samples_per_second': 6.249, 'train_steps_per_second': 0.031, 'train_loss': 1.14574054125193, 'epoch': 1.0}\u001b[0m\n",
      "\u001b[34m100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 74/74 [39:17<00:00, 28.71s/it]\u001b[0m\n",
      "\u001b[34m100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 74/74 [39:17<00:00, 31.86s/it]\u001b[0m\n",
      "\u001b[34mOverriding torch_dtype=None with `torch_dtype=torch.float16` due to requirements of `bitsandbytes` to enable model loading in mixed int8. Either pass torch_dtype=torch.float16 or don't pass this argument at all to remove this warning.\u001b[0m\n",
      "\u001b[34mOverriding torch_dtype=None with `torch_dtype=torch.float16` due to requirements of `bitsandbytes` to enable model loading in mixed int8. Either pass torch_dtype=torch.float16 or don't pass this argument at all to remove this warning.\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:08<00:08,  8.29s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:08<00:00,  3.79s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:08<00:00,  4.46s/it]\u001b[0m\n",
      "\u001b[34m#################### model ####################\u001b[0m\n",
      "\u001b[34minput sentence: Richie: Pogba#015\u001b[0m\n",
      "\u001b[34mClay: Pogboom#015\u001b[0m\n",
      "\u001b[34mRichie: what a s strike yoh!#015\u001b[0m\n",
      "\u001b[34mClay: was off the seat the moment he chopped the ball back to his right foot#015\u001b[0m\n",
      "\u001b[34mRichie: me too dude#015\u001b[0m\n",
      "\u001b[34mClay: hope his form lasts#015\u001b[0m\n",
      "\u001b[34mRichie: This season he's more mature#015\u001b[0m\n",
      "\u001b[34mClay: Yeah, Jose has his trust in him#015\u001b[0m\n",
      "\u001b[34mRichie: everyone does#015\u001b[0m\n",
      "\u001b[34mClay: yeah, he really deserved to score after his first 60 minutes#015\u001b[0m\n",
      "\u001b[34mRichie: reward#015\u001b[0m\n",
      "\u001b[34mClay: yeah man#015\u001b[0m\n",
      "\u001b[34mRichie: cool then #015\u001b[0m\n",
      "\u001b[34mClay: cool\u001b[0m\n",
      "\u001b[34m------------------------------------------------------------\u001b[0m\n",
      "\u001b[34msummary:\u001b[0m\n",
      "\u001b[34mPogba scored after his first 60 minutes\u001b[0m\n",
      "\u001b[34mDownloading builder script: 0.00B [00:00, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading builder script: 6.27kB [00:00, 6.01MB/s]\u001b[0m\n",
      "\u001b[34m0%|          | 0/819 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m0%|          | 1/819 [00:02<34:33,  2.54s/it]\u001b[0m\n",
      "\u001b[34m0%|          | 2/819 [00:04<32:28,  2.39s/it]\u001b[0m\n",
      "\u001b[34m0%|          | 3/819 [00:07<34:09,  2.51s/it]\u001b[0m\n",
      "\u001b[34m0%|          | 4/819 [00:10<38:19,  2.82s/it]\u001b[0m\n",
      "\u001b[34m1%|          | 5/819 [00:14<41:10,  3.03s/it]\u001b[0m\n",
      "\u001b[34m1%|          | 6/819 [00:18<46:58,  3.47s/it]\u001b[0m\n",
      "\u001b[34m1%|          | 7/819 [00:21<45:33,  3.37s/it]\u001b[0m\n",
      "\u001b[34m1%|          | 8/819 [00:23<38:06,  2.82s/it]\u001b[0m\n",
      "\u001b[34m1%|          | 9/819 [00:29<53:33,  3.97s/it]\u001b[0m\n",
      "\u001b[34m1%|          | 10/819 [00:34<54:56,  4.07s/it]\u001b[0m\n",
      "\u001b[34m1%|â–         | 11/819 [00:38<57:52,  4.30s/it]\u001b[0m\n",
      "\u001b[34m1%|â–         | 12/819 [00:43<57:47,  4.30s/it]\u001b[0m\n",
      "\u001b[34m2%|â–         | 13/819 [00:46<54:36,  4.07s/it]\u001b[0m\n",
      "\u001b[34m2%|â–         | 14/819 [00:48<46:13,  3.44s/it]\u001b[0m\n",
      "\u001b[34m2%|â–         | 15/819 [00:55<58:18,  4.35s/it]\u001b[0m\n",
      "\u001b[34m2%|â–         | 16/819 [00:59<59:02,  4.41s/it]\u001b[0m\n",
      "\u001b[34m2%|â–         | 17/819 [01:01<49:23,  3.69s/it]\u001b[0m\n",
      "\u001b[34m2%|â–         | 18/819 [01:07<56:51,  4.26s/it]\u001b[0m\n",
      "\u001b[34m2%|â–         | 19/819 [01:13<1:05:25,  4.91s/it]\u001b[0m\n",
      "\u001b[34m2%|â–         | 20/819 [01:17<59:22,  4.46s/it]\u001b[0m\n",
      "\u001b[34m3%|â–         | 21/819 [01:20<56:34,  4.25s/it]\u001b[0m\n",
      "\u001b[34m3%|â–         | 22/819 [01:27<1:05:16,  4.91s/it]\u001b[0m\n",
      "\u001b[34m3%|â–         | 23/819 [01:31<1:02:17,  4.70s/it]\u001b[0m\n",
      "\u001b[34m3%|â–         | 24/819 [01:32<49:04,  3.70s/it]\u001b[0m\n",
      "\u001b[34m3%|â–         | 25/819 [01:38<55:25,  4.19s/it]\u001b[0m\n",
      "\u001b[34m3%|â–         | 26/819 [01:41<50:44,  3.84s/it]\u001b[0m\n",
      "\u001b[34m3%|â–         | 27/819 [01:43<45:56,  3.48s/it]\u001b[0m\n",
      "\u001b[34m3%|â–         | 28/819 [01:49<52:12,  3.96s/it]\u001b[0m\n",
      "\u001b[34m4%|â–         | 29/819 [01:53<54:34,  4.14s/it]\u001b[0m\n",
      "\u001b[34m4%|â–         | 30/819 [01:57<51:36,  3.92s/it]\u001b[0m\n",
      "\u001b[34m4%|â–         | 31/819 [01:59<46:59,  3.58s/it]\u001b[0m\n",
      "\u001b[34m4%|â–         | 32/819 [02:03<48:18,  3.68s/it]\u001b[0m\n",
      "\u001b[34m4%|â–         | 33/819 [02:05<39:42,  3.03s/it]\u001b[0m\n",
      "\u001b[34m4%|â–         | 34/819 [02:07<35:42,  2.73s/it]\u001b[0m\n",
      "\u001b[34m4%|â–         | 35/819 [02:10<38:51,  2.97s/it]\u001b[0m\n",
      "\u001b[34m4%|â–         | 36/819 [02:16<48:58,  3.75s/it]\u001b[0m\n",
      "\u001b[34m5%|â–         | 37/819 [02:20<52:05,  4.00s/it]\u001b[0m\n",
      "\u001b[34m5%|â–         | 38/819 [02:24<50:43,  3.90s/it]\u001b[0m\n",
      "\u001b[34m5%|â–         | 39/819 [02:28<50:48,  3.91s/it]\u001b[0m\n",
      "\u001b[34m5%|â–         | 40/819 [02:31<47:51,  3.69s/it]\u001b[0m\n",
      "\u001b[34m5%|â–Œ         | 41/819 [02:38<58:32,  4.52s/it]\u001b[0m\n",
      "\u001b[34m5%|â–Œ         | 42/819 [02:41<52:14,  4.03s/it]\u001b[0m\n",
      "\u001b[34m5%|â–Œ         | 43/819 [02:45<52:15,  4.04s/it]\u001b[0m\n",
      "\u001b[34m5%|â–Œ         | 44/819 [02:47<44:51,  3.47s/it]\u001b[0m\n",
      "\u001b[34m5%|â–Œ         | 45/819 [02:50<42:04,  3.26s/it]\u001b[0m\n",
      "\u001b[34m6%|â–Œ         | 46/819 [02:52<38:42,  3.00s/it]\u001b[0m\n",
      "\u001b[34m6%|â–Œ         | 47/819 [02:56<43:37,  3.39s/it]\u001b[0m\n",
      "\u001b[34m6%|â–Œ         | 48/819 [02:59<40:12,  3.13s/it]\u001b[0m\n",
      "\u001b[34m6%|â–Œ         | 49/819 [03:01<37:51,  2.95s/it]\u001b[0m\n",
      "\u001b[34m6%|â–Œ         | 50/819 [03:04<38:06,  2.97s/it]\u001b[0m\n",
      "\u001b[34m96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 785/819 [03:04<00:00, 104.74it/s]\u001b[0m\n",
      "\u001b[34m100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 819/819 [03:04<00:00,  4.43it/s]\u001b[0m\n",
      "\u001b[34mRogue1: 46.108224%\u001b[0m\n",
      "\u001b[34mrouge2: 19.772860%\u001b[0m\n",
      "\u001b[34mrougeL: 36.792934%\u001b[0m\n",
      "\u001b[34mrougeLsum: 36.796880%\u001b[0m\n",
      "\n",
      "2023-06-26 02:37:48 Uploading - Uploading generated training model\u001b[34m2023-06-26 02:37:40,122 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[34m2023-06-26 02:37:40,122 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[34m2023-06-26 02:37:40,123 sagemaker-training-toolkit INFO     Reporting training SUCCESS\u001b[0m\n",
      "\n",
      "2023-06-26 02:37:59 Completed - Training job completed\n",
      "Training seconds: 3073\n",
      "Billable seconds: 3073\n"
     ]
    }
   ],
   "source": [
    "estimator.logs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ì´ ì˜ˆì œì—ì„œ SageMaker í•™ìŠµ ì‘ì—…ì€ `7946 seconds`ê°€ ì†Œìš”ë˜ì—ˆìœ¼ë©°, ì´ëŠ” ì•½ `2.2 hours`ì…ë‹ˆë‹¤. ìš°ë¦¬ê°€ ì‚¬ìš©í•œ ml.g5.2xlarge ì¸ìŠ¤í„´ìŠ¤ëŠ” ì˜¨ë””ë§¨ë“œ ì‚¬ìš© ì‹œ ì‹œê°„ë‹¹ `$1.515 per hour` (US region ê¸°ì¤€) ì…ë‹ˆë‹¤. ê·¸ ê²°ê³¼, fine-tunedëœ flan-t5-xl ëª¨ë¸ì„ í•™ìŠµí•˜ëŠ” ë° ë“œëŠ” ì´ ë¹„ìš©ì€ `$3.34`ì— ë¶ˆê³¼í–ˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "Spot ì¸ìŠ¤í„´ìŠ¤ë¥¼ ì‚¬ìš©í•˜ë©´ í•™ìŠµ ë¹„ìš©ì„ ë” ì¤„ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ Spot ì¸ìŠ¤í„´ìŠ¤ ì¤‘ë‹¨ìœ¼ë¡œ ì¸í•´ ì´ í•™ìŠµ ì‹œê°„ì´ ëŠ˜ì–´ë‚  ê°€ëŠ¥ì„±ì´ ìˆìŠµë‹ˆë‹¤. ì¸ìŠ¤í„´ìŠ¤ ê°€ê²©ì— ëŒ€í•œ ìì„¸í•œ ë‚´ìš©ì€ [SageMaker ê°€ê²© í˜ì´ì§€](https://aws.amazon.com/sagemaker/pricing/)ë¥¼ ì°¸ì¡°í•˜ì„¸ìš”.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Deploy the model to Amazon SageMaker Endpoint\n",
    "\n",
    "í•™ìŠµì— `peft`ë¥¼ ì‚¬ìš©í•  ë•ŒëŠ” ì¼ë°˜ì ìœ¼ë¡œ adapter weightsë¥¼ ì‚¬ìš©í•˜ê²Œ ë©ë‹ˆë‹¤. ëª¨ë¸ì„ ë” ì‰½ê²Œ ë°°í¬í•  ìˆ˜ ìˆë„ë¡ ê¸°ë³¸ ëª¨ë¸ê³¼ adatperë¥¼ ë³‘í•©í•˜ëŠ” `merge_and_unload()` ë©”ì„œë“œë¥¼ ì¶”ê°€í–ˆìŠµë‹ˆë‹¤. ì´ì œ `transformers` ë¼ì´ë¸ŒëŸ¬ë¦¬ì˜ `pipelines` ê¸°ëŠ¥ì„ ì‚¬ìš©í•  ìˆ˜ ìˆê²Œ ë˜ì—ˆìŠµë‹ˆë‹¤. \n",
    "\n",
    "SageMakerëŠ” SageMaker Endpoint Configurationê³¼ SageMaker Endpointë¥¼ ìƒì„±í•˜ì—¬ ë°°í¬ í”„ë¡œì„¸ìŠ¤ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤. Endpoint Configurationì€ ëª¨ë¸ê³¼ instance typeì„ ì •ì˜í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "download: s3://sagemaker-us-west-2-322537213286/huggingface-peft-2023-06-26-01-45-31-2023-06-26-01-45-31-106/output/model.tar.gz to model/model.tar.gz\n",
      "tokenizer_config.json\n",
      "rogue.pickle\n",
      "tokenizer.json\n",
      "adapter_config.json\n",
      "adapter_model.bin\n",
      "special_tokens_map.json\n"
     ]
    }
   ],
   "source": [
    "!rm -rf ./model\n",
    "!aws s3 cp {estimator.model_data} ./model/model.tar.gz\n",
    "!tar -xzvf ./model/model.tar.gz -C ./model/ # && mv ./model/model.tar.gz ./"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "instance_type= \"ml.g5.4xlarge\"\n",
    "# instance_type= \"local_gpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# source_dir=f\"file://{Path.cwd()}/src\"\n",
    "\n",
    "if instance_type in ['local', 'local_gpu']:\n",
    "    from sagemaker.local import LocalSession\n",
    "    \n",
    "    sagemaker_session = LocalSession()\n",
    "    sagemaker_session.config = {'local': {'local_code': True}}\n",
    "    model_data=f\"file://{Path.cwd()}/model/model.tar.gz\"\n",
    "else:\n",
    "    sagemaker_session = sagemaker.session.Session()\n",
    "    model_data=estimator.model_data\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "env = {\n",
    "    'SAGEMAKER_MODEL_SERVER_TIMEOUT': str(3600),\n",
    "    'MODEL_CACHE_ROOT': '/opt/ml/model', \n",
    "    'SAGEMAKER_ENV': '1',\n",
    "    'SAGEMAKER_SUBMIT_DIRECTORY': '/opt/ml/code',\n",
    "    'TS_DEFAULT_WORKERS_PER_MODEL': '1', \n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from sagemaker.huggingface import HuggingFaceModel\n",
    "from sagemaker.pytorch.model import PyTorchModel\n",
    "\n",
    "# create Hugging Face Model Class\n",
    "model = PyTorchModel(\n",
    "    entry_point='inference.py',\n",
    "    source_dir=f'{Path.cwd()}/flan_t5_xl_with_LoRA',\n",
    "    model_data=model_data,\n",
    "    role=role, \n",
    "    framework_version=\"2.0\", \n",
    "    py_version=\"py310\",\n",
    "    model_server_workers=1,\n",
    "    sagemaker_session=sagemaker_session,\n",
    "    env=env\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ì´ì œ ì›í•˜ëŠ” ì¸ìŠ¤í„´ìŠ¤ ìˆ˜ì™€ ì¸ìŠ¤í„´ìŠ¤ ìœ í˜•ì„ ì „ë‹¬í•˜ì—¬ HuggingFace estimator ê°ì²´ì—ì„œ `deploy()`ë¥¼ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ì„ ë°°í¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Repacking model artifact (s3://sagemaker-us-west-2-322537213286/huggingface-peft-2023-06-26-01-45-31-2023-06-26-01-45-31-106/output/model.tar.gz), script artifact (/root/aws-ai-ml-workshop-kr/genai/jumpstart/text_to_text/flan_t5_xl_with_LoRA), and dependencies ([]) into single tar.gz file located at s3://sagemaker-us-west-2-322537213286/pytorch-inference-2023-06-26-03-22-15-557/model.tar.gz. This may take some time depending on model size...\n",
      "INFO:sagemaker:Creating model with name: pytorch-inference-2023-06-26-03-22-19-571\n",
      "INFO:sagemaker:Creating endpoint-config with name huggingface-peft-2023-06-26-03-22-15\n",
      "INFO:sagemaker:Creating endpoint with name huggingface-peft-2023-06-26-03-22-15\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------!"
     ]
    }
   ],
   "source": [
    "import time\n",
    "endpoint_name = f'huggingface-peft-{time.strftime(\"%Y-%m-%d-%H-%M-%S\", time.localtime())}'\n",
    "\n",
    "# deploy model to SageMaker Inference\n",
    "predictor = model.deploy(\n",
    "    endpoint_name=endpoint_name,\n",
    "    initial_instance_count=1,\n",
    "    instance_type=instance_type\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: SageMaker endpointê°€ ì¶”ë¡  ìš”ì²­ì„ í—ˆìš©í•˜ê¸° ìœ„í•´ ì¸ìŠ¤í„´ìŠ¤ë¥¼ ì˜¨ë¼ì¸\u001d",
    "ë¡œ ì „í™˜í•˜ê³  ëª¨ë¸ì„ ë‹¤ìš´ë¡œë“œí•˜ëŠ” ë° 5~10ë¶„ì´ ì†Œìš”ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`test` ë¶„í• ì˜ ì˜ˆì œë¥¼ ì‚¬ìš©í•˜ì—¬ í…ŒìŠ¤íŠ¸í•´ ë³´ê² ìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.builder:Found cached dataset samsum (/root/.cache/huggingface/datasets/samsum/samsum/0.0.0/f1d7c6b7353e6de335d444e424dc002ef70d1277109031327bc9cc6af5d3d46e)\n"
     ]
    }
   ],
   "source": [
    "from random import randint\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Load dataset from the hub\n",
    "test_dataset = load_dataset(\"samsum\", split=\"test\")\n",
    "\n",
    "# select a random test sample\n",
    "sample = test_dataset[randint(0,len(test_dataset))]\n",
    "\n",
    "# format sample\n",
    "prompt_template = f\"Summarize the chat dialogue:\\n{{dialogue}}\\n---\\nSummary:\\n\"\n",
    "\n",
    "fomatted_sample = {\n",
    "  \"inputs\": prompt_template.format(dialogue=sample[\"dialogue\"]),\n",
    "  \"parameters\": {\n",
    "    \"do_sample\": True, # sample output predicted probabilities\n",
    "    \"top_p\": 0.9, # sampling technique Fan et. al (2018)\n",
    "    \"temperature\": 0.1, # increasing the likelihood of high probability words and decreasing the likelihood of low probability words\n",
    "    \"max_new_tokens\": 100, # The maximum numbers of tokens to generate, ignoring the number of tokens in the prompt\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sean has decided that his spirit animal is a tortoise. Tiffany thinks Sean is a wasp.\n"
     ]
    }
   ],
   "source": [
    "# predict\n",
    "res = predictor.predict(fomatted_sample)\n",
    "\n",
    "\n",
    "print(res[0][\"generated_text\"].split(\"Summary:\")[-1])\n",
    "\n",
    "# Sample model output: Kirsten and Alex are going bowling this Friday at 7 pm. They will meet up and then go together."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ì´ì œ ëª¨ë¸ ìš”ì•½ë˜ dialogì˜ ê²°ê³¼ì™€ í…ŒìŠ¤íŠ¸ sample summaryë¥¼ ë¹„êµí•´ ë³´ê² ìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sean believes his spirit animal is a tortoise and Tiffany's could be a wasp. \n"
     ]
    }
   ],
   "source": [
    "print(sample[\"summary\"])\n",
    "\n",
    "# Test sample summary: Kirsten reminds Alex that the youth group meets this Friday at 7 pm to go bowling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ë§ˆì§€ë§‰ìœ¼ë¡œ endpointë¥¼ ë‹¤ì‹œ ì‚­ì œí•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Deleting model with name: pytorch-inference-2023-06-26-03-22-19-571\n",
      "INFO:sagemaker:Deleting endpoint configuration with name: huggingface-peft-2023-06-26-03-22-15\n",
      "INFO:sagemaker:Deleting endpoint with name: huggingface-peft-2023-06-26-03-22-15\n"
     ]
    }
   ],
   "source": [
    "predictor.delete_model()\n",
    "predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-west-2:236514542706:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "2d58e898dde0263bc564c6968b04150abacfd33eed9b19aaa8e45c040360e146"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
