{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  04 Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Amazon Bedrock의 fine tuning된 모델 가져오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "import json\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "region = sess.boto_region_name\n",
    "client = boto3.client(\"bedrock-runtime\", region_name=region)\n",
    "model_id = \"<ENTER_YOUR_MODEL_ARN_HERE>\"\n",
    "\n",
    "assert model_id != \"<ENTER_YOUR_MODEL_ARN_HERE>\", \"ERROR: Please enter your model id\"\n",
    "\n",
    "def get_sql_query(system_prompt, user_question):\n",
    "    \"\"\"\n",
    "    Generate a SQL query using Llama 3 8B\n",
    "    Remember to use the same template used in fine tuning\n",
    "    \"\"\"\n",
    "    formatted_prompt = f\"<s>[INST] <<SYS>>{system_prompt}<</SYS>>\\n\\n[INST]Human: Return the SQL query that answers the following question: {user_question}[/INST]\\n\\nAssistant:\"\n",
    "    native_request = {\n",
    "        \"prompt\": formatted_prompt,\n",
    "        \"max_tokens\": 100,\n",
    "        \"top_p\": 0.9,\n",
    "        \"temperature\": 0.1\n",
    "    }\n",
    "    response = client.invoke_model(modelId=model_id,\n",
    "                                   body=json.dumps(native_request))\n",
    "    response_text = json.loads(response.get('body').read())[\"outputs\"][0][\"text\"]\n",
    "\n",
    "    return response_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 테스트 데이터셋 가져오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "test_df = pd.read_json(\"../datasets/ko_test_dataset.json\", lines=True)[\"messages\"]\n",
    "\n",
    "def extract_content(dicts, role):\n",
    "    for d in dicts:\n",
    "        if d['role'] == role:\n",
    "            return d['content']\n",
    "    return None\n",
    "\n",
    "df = pd.DataFrame()\n",
    "for role in ['system', 'user', 'assistant']:\n",
    "    df[role] = test_df.apply(lambda x: extract_content(x, role))\n",
    "del test_df\n",
    "\n",
    "df = df[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['llama'] = df.apply(lambda row: get_sql_query(row['system'], row['user']), axis=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- pandas로 데이터셋 미리보기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Claude 3.5 Sonnet을 이용해 정확도 측정하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function because Claude requires the Messages API\n",
    "\n",
    "#for connecting with Bedrock, use Boto3\n",
    "import boto3, time, json\n",
    "from botocore.config import Config\n",
    "\n",
    "my_config = Config(connect_timeout=60*3, read_timeout=60*3)\n",
    "bedrock = boto3.client(service_name='bedrock-runtime',config=my_config)\n",
    "bedrock_service = boto3.client(service_name='bedrock',config=my_config)\n",
    "\n",
    "MAX_ATTEMPTS = 3 #how many times to retry if Claude is not working.\n",
    "\n",
    "def ask_claude(messages,system=\"\", model_version=\"haiku\"):\n",
    "    '''\n",
    "    Send a prompt to Bedrock, and return the response\n",
    "    '''\n",
    "    raw_prompt_text = str(messages)\n",
    "    \n",
    "    if type(messages)==str:\n",
    "        messages = [{\"role\": \"user\", \"content\": messages}]\n",
    "    \n",
    "    promt_json = {\n",
    "        \"system\":system,\n",
    "        \"messages\": messages,\n",
    "        \"max_tokens\": 3000,\n",
    "        \"temperature\": 0.7,\n",
    "        \"anthropic_version\":\"\",\n",
    "        \"top_k\": 250,\n",
    "        \"top_p\": 0.7,\n",
    "        \"stop_sequences\": [\"\\n\\nHuman:\"]\n",
    "    }\n",
    "    \n",
    "    modelId = 'anthropic.claude-3-5-sonnet-20240620-v1:0'\n",
    "    #Sonnet 3.0 \n",
    "    # modelId = 'anthropic.claude-3-sonnet-20240229-v1:0'\n",
    "\n",
    "    \n",
    "    attempt = 1\n",
    "    while True:\n",
    "        try:\n",
    "            response = bedrock.invoke_model(body=json.dumps(promt_json), modelId=modelId, accept='application/json', contentType='application/json')\n",
    "            response_body = json.loads(response.get('body').read())\n",
    "            results = response_body.get(\"content\")[0].get(\"text\")\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(\"Error with calling Bedrock: \"+str(e))\n",
    "            attempt+=1\n",
    "            if attempt>MAX_ATTEMPTS:\n",
    "                print(\"Max attempts reached!\")\n",
    "                results = str(e)\n",
    "                break\n",
    "            else: #retry in 2 seconds\n",
    "                time.sleep(2)\n",
    "    return [raw_prompt_text,results]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def get_score(system, user, assistant, llama):\n",
    "    db_schema = system[139:] # Remove generic instructions\n",
    "    question = user[58:] # Remove generic instructions\n",
    "    correct_answer = assistant\n",
    "    test_answer = llama\n",
    "    formatted_prompt = f\"\"\"You are a data science teacher that is introducing students to SQL. Consider the following question and schema:\n",
    "<question>{question}</question>\n",
    "<schema>{db_schema}</schema>\n",
    "    \n",
    "Here is the correct answer:\n",
    "<correct_answer>{correct_answer}</correct_answer>\n",
    "    \n",
    "Here is the student's answer:\n",
    "<student_answer>{test_answer}<student_answer>\n",
    "\n",
    "Please provide a numeric score from 0 to 100 on how well the student's answer matches the correct answer for this question.\n",
    "The score should be high if the answers say essentially the same thing.\n",
    "The score should be lower if some parts are missing, or if extra unnecessary parts have been included.\n",
    "The score should be 0 for an entirely wrong answer. Put the score in <SCORE> XML tags.\n",
    "Do not consider your own answer to the question, but instead score based only on the correct answer above.\n",
    "\"\"\"\n",
    "    _, result = ask_claude(formatted_prompt, model_version=\"sonnet\")\n",
    "    pattern = r'<SCORE>(.*?)</SCORE>'\n",
    "    match = re.search (pattern, result)\n",
    "    \n",
    "    return match.group(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Score 보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import textwrap\n",
    "\n",
    "scores = []\n",
    "print('=' * 80)\n",
    "print('평가 결과'.center(80))\n",
    "print('=' * 80)\n",
    "\n",
    "for ix in range(len(df)):\n",
    "    user_input = df['user'][ix]\n",
    "    assistant_output = df['assistant'][ix]\n",
    "    llama_output = df['llama'][ix]\n",
    "    response = float(get_score(df['system'][ix], user_input, assistant_output, llama_output))\n",
    "    scores.append(response)\n",
    "    \n",
    "    print(f'항목 #{ix+1}'.center(80, '-'))\n",
    "    print(f'User 질문:')\n",
    "    print(textwrap.fill(user_input, width=80, initial_indent='  ', subsequent_indent='  '))\n",
    "    print(f'\\nAssistant 정답:')\n",
    "    print(textwrap.fill(assistant_output, width=80, initial_indent='  ', subsequent_indent='  '))\n",
    "    print(f'\\nLlama 모델 답변:')\n",
    "    print(textwrap.fill(llama_output, width=80, initial_indent='  ', subsequent_indent='  '))\n",
    "    print(f'\\n점수: {response:.2f}')\n",
    "    print('=' * 80)\n",
    "\n",
    "average_score = sum(scores) / len(scores)\n",
    "print('총 평가 결과'.center(80, '-'))\n",
    "print(f'전체 점수: {scores}')\n",
    "print(f'평균 점수: {average_score:.2f}')\n",
    "print('=' * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 평가 예시\n",
    "```\n",
    "------------------------------------항목 #1--------------------------------------\n",
    "System 컨텍스트:\n",
    "  You are a powerful text-to-SQL model. Your job is to answer questions about a\n",
    "  database.You can use the following table schema for context: CREATE TABLE\n",
    "  table_name_87 (profession VARCHAR, city VARCHAR)\n",
    "\n",
    "User 질문:\n",
    "  Return the SQL query that answers the following question: Leskovac시 출신의 하우스\n",
    "  메이트의 직업은 무엇입니까?\n",
    "\n",
    "Assistant 정답:\n",
    "  SELECT profession FROM table_name_87 WHERE city = \"leskovac\"\n",
    "\n",
    "Llama 모델 답변:\n",
    "   SELECT profession FROM table_name_87 WHERE city = \"leskovac\"\n",
    "\n",
    "점수: 100.00\n",
    "================================================================================\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 참고: 테스트 결과 아래와 같이 평균 90.45%의 정확도를 보였습니다.\n",
    "    ```\n",
    "    Assigned scores:  [80.0, 90.0, 90.0, 95.0, 50.0, 40.0, 100.0, 90.0, 100.0, 75.0, 80.0, 100.0, 100.0, 80.0, 80.0, 100.0, 60.0, 100.0, 90.0, 100.0, 100.0, 100.0, 80.0, 60.0, 100.0, 100.0, 100.0, 95.0, 75.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 95.0, 75.0, 60.0, 95.0, 100.0, 100.0, 100.0, 100.0, 90.0, 40.0, 100.0, 0.0, 90.0, 100.0, 100.0, 100.0, 100.0, 75.0, 80.0, 100.0, 100.0, 100.0, 100.0, 60.0, 95.0, 100.0, 60.0, 100.0, 100.0, 100.0, 100.0, 100.0, 50.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 80.0, 100.0, 100.0, 90.0, 100.0, 100.0, 100.0, 100.0, 100.0, 90.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 50.0, 100.0, 60.0, 100.0, 100.0, 100.0]\n",
    "    Average score: 90.45\n",
    "\n",
    "    ```"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
