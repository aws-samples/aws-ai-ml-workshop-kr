# script parameters
#model_id:  "meta-llama/Meta-Llama-3-8B" # Hugging Face model id
model_id: "MLP-KTLim/llama-3-Korean-Bllossom-8B"
#model_id: "meta-llama/Meta-Llama-3.1-8B-Instruct"
###########################
# small samples for Debug
###########################
train_dataset_path: "../data/naver-news-summarization-ko/train"                      # path to dataset
validation_dataset_path: "../data/naver-news-summarization-ko/validation"                      # path to dataset
#test_dataset_path: "../data/naver-news-summarization-ko/test"                      # path to dataset
per_device_train_batch_size: 1         # batch size per device during training
per_device_eval_batch_size: 1          # batch size for evaluation
gradient_accumulation_steps: 2         # number of steps before performing a backward/update pass
###########################
# large samples for evaluation
###########################
# train_dataset_path: "../data/naver-news-summarization-ko/full_train"                      # path to dataset
# validation_dataset_path: "../data/naver-news-summarization-ko/full_validation"                      # path to dataset
# test_dataset_path: "../data/naver-news-summarization-ko/full_test"                      # path to dataset
# per_device_train_batch_size: 16         # batch size per device during training
# per_device_eval_batch_size: 1          # batch size for evaluation
# gradient_accumulation_steps: 2         # number of steps before performing a backward/update pass
###########################
#max_seq_len:  2048              # max sequence length for model and packing of the dataset


# training parameters
output_dir: "/home/ec2-user/SageMaker/models/llama-3-8b-naver-news" # Temporary output directory for model checkpoints
report_to: "tensorboard"               # report metrics to tensorboard
logging_dir: "/home/ec2-user/SageMaker/logs/llama-3-8b-naver-news" # log folder for tensorboard
learning_rate: 0.0002                  # learning rate 2e-4
lr_scheduler_type: "constant"          # learning rate scheduler
num_train_epochs: 1                    # number of training epochs
optim: adamw_torch                     # use torch adamw optimizer
logging_steps: 10                      # log every 10 steps
save_strategy: epoch                   # save checkpoint every epoch
evaluation_strategy: epoch             # evaluate every epoch
max_grad_norm: 0.3                     # max gradient norm
warmup_ratio: 0.03                     # warmup ratio
bf16: true                             # use bfloat16 precision
tf32: true                             # use tf32 precision
#gradient_checkpointing: true          # use gradient checkpointing to save memory
#activation_checkpointing: true         # use gradient checkpointing to save memory in FSDP config

# FSDP parameters: https://huggingface.co/docs/transformers/main/en/fsdp
fsdp: "full_shard auto_wrap offload" # remove offload if enough GPU memory
fsdp_config:
    backward_prefetch: "backward_pre"
    forward_prefetch: "false"
    use_orig_params: "false"
    activation_checkpointing: "true"
    #fsdp_limit_all_gathers: "true"
    #fsdp_sync_module_states: "true"
    #fsdp_offload_params: "false"
