{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe4872e4-c90b-434b-bfe5-f88292fba385",
   "metadata": {},
   "source": [
    "# ë¡œì»¬ì—ì„œ í›ˆë ¨ í•˜ê¸°\n",
    "- ì´ ë…¸íŠ¸ë¶ì€ ë¡œì»¬ (í˜„ì¬ ë¨¸ì‹ ) ì—ì„œ Hugging Face Accelerator + PyTorch FSDP ë¡œ íŒŒì¸ íŠœë‹ í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5da7aa7-1a2d-4db1-b011-59217c32a83a",
   "metadata": {},
   "source": [
    "## 1. í™˜ê²½ ì…‹ì—…"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c268a663-63a3-4d9e-8bd3-8025dec88254",
   "metadata": {},
   "source": [
    "### Hugging Face Token ì…ë ¥\n",
    "- [ì¤‘ìš”] HF Key ê°€ ë…¸ì¶œì´ ì•ˆë˜ë„ë¡ ì¡°ì‹¬í•˜ì„¸ìš”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3ee8f554-0ac4-499a-bc35-7fbae4b96b98",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "datasets                  3.0.0\n",
      "sagemaker                 2.232.1\n",
      "sagemaker-core            1.0.9\n",
      "torch                     2.4.1\n",
      "transformers              4.40.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip list | grep -E \"torch|datasets|transformers|sagemaker\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68adfaae-6c9e-4f39-b08e-9ca3d176cb89",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install -U transformers==4.40.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18b93c6e-60cd-45e9-8640-e11fa4e62167",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install flash-attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "542f1cbe-5b7e-4fae-bf2d-4a6ac2f3dfe1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to /home/ec2-user/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def set_hf_key_env_vars(hf_key_name, key_val):\n",
    "    os.environ[hf_key_name] = key_val\n",
    "\n",
    "def get_hf_key_env_vars(hf_key_name):\n",
    "    HF_key_value = os.environ.get(hf_key_name)\n",
    "\n",
    "    return HF_key_value\n",
    "\n",
    "\n",
    "is_sagemaker_notebook = True\n",
    "if is_sagemaker_notebook:\n",
    "    hf_key_name = \"HF_KEY\"\n",
    "    key_val = \"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\"#\"<Type Your HF Key>\"\n",
    "    set_hf_key_env_vars(hf_key_name, key_val)\n",
    "    HF_TOKEN = get_hf_key_env_vars(hf_key_name)\n",
    "else: # VS Code\n",
    "    from dotenv import load_dotenv\n",
    "    HF_TOKEN = os.getenv('HF_TOKEN')\n",
    "\n",
    "\n",
    "# Log in to HF\n",
    "!huggingface-cli login --token {HF_TOKEN}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebcc7a0b-4829-4b2c-88b6-fd423732c53a",
   "metadata": {},
   "source": [
    "### ì €ì¥ëœ ë³€ìˆ˜ ë¡œë”©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "24bce431",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_folder:  ../data/naver-news-summarization-ko\n",
      "train_data_json:  ../data/naver-news-summarization-ko/train/train_dataset.json\n",
      "validation_data_json:  ../data/naver-news-summarization-ko/validation/validation_dataset.json\n",
      "test_data_json:  ../data/naver-news-summarization-ko/test/test_dataset.json\n",
      "full_train_data_json:  ../data/naver-news-summarization-ko/full_train/train_dataset.json\n",
      "full_validation_data_json:  ../data/naver-news-summarization-ko/full_validation/validation_dataset.json\n",
      "full_test_data_json:  ../data/naver-news-summarization-ko/full_test/test_dataset.json\n"
     ]
    }
   ],
   "source": [
    "%store -r data_folder\n",
    "%store -r train_data_json \n",
    "%store -r validation_data_json \n",
    "%store -r test_data_json \n",
    "%store -r full_train_data_json \n",
    "%store -r full_validation_data_json \n",
    "%store -r full_test_data_json\n",
    "\n",
    "\n",
    "print(\"data_folder: \", data_folder)\n",
    "print(\"train_data_json: \", train_data_json)\n",
    "print(\"validation_data_json: \", validation_data_json)\n",
    "print(\"test_data_json: \", test_data_json)\n",
    "print(\"full_train_data_json: \", full_train_data_json)\n",
    "print(\"full_validation_data_json: \", full_validation_data_json)\n",
    "print(\"full_test_data_json: \", full_test_data_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a396bc4f-0b0a-4ffb-8c59-18ed6d0a968d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from datasets import Dataset, load_dataset\n",
    "from datasets import load_dataset\n",
    "from transformers import set_seed\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a78f53c-d21d-4fca-b69d-6a85d966353c",
   "metadata": {},
   "source": [
    "## 2. ë² ì´ìŠ¤ ëª¨ë¸ ì¤€ë¹„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d514e3df",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#model_id = \"meta-llama/Meta-Llama-3-8B\"\n",
    "model_id = \"MLP-KTLim/llama-3-Korean-Bllossom-8B\"\n",
    "\n",
    "\n",
    "output_dir = \"/home/ec2-user/SageMaker/models/llama-3-8b-naver-news\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2403f6b8",
   "metadata": {},
   "source": [
    "### Config YAML íŒŒì¼ ìƒì„±"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "72d30120",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting accelerator_config/local_llama_3_8b_fsdp_qlora.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile accelerator_config/local_llama_3_8b_fsdp_qlora.yaml\n",
    "# script parameters\n",
    "model_id:  \"meta-llama/Meta-Llama-3-8B\" # Hugging Face model id\n",
    "#model_id: \"MLP-KTLim/llama-3-Korean-Bllossom-8B\"\n",
    "#model_id: \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
    "###########################\n",
    "# small samples for Debug\n",
    "###########################\n",
    "train_dataset_path: \"../data/naver-news-summarization-ko/train\"                      # path to dataset\n",
    "validation_dataset_path: \"../data/naver-news-summarization-ko/validation\"                      # path to dataset\n",
    "#test_dataset_path: \"../data/naver-news-summarization-ko/test\"                      # path to dataset\n",
    "per_device_train_batch_size: 1         # batch size per device during training\n",
    "per_device_eval_batch_size: 1          # batch size for evaluation\n",
    "gradient_accumulation_steps: 2         # number of steps before performing a backward/update pass\n",
    "###########################\n",
    "# large samples for evaluation\n",
    "###########################\n",
    "# train_dataset_path: \"../data/naver-news-summarization-ko/full_train\"                      # path to dataset\n",
    "# validation_dataset_path: \"../data/naver-news-summarization-ko/full_validation\"                      # path to dataset\n",
    "# test_dataset_path: \"../data/naver-news-summarization-ko/full_test\"                      # path to dataset\n",
    "# per_device_train_batch_size: 16         # batch size per device during training\n",
    "# per_device_eval_batch_size: 1          # batch size for evaluation\n",
    "# gradient_accumulation_steps: 2         # number of steps before performing a backward/update pass\n",
    "###########################\n",
    "max_seq_length:  2048              # max sequence length for model and packing of the dataset\n",
    "\n",
    "\n",
    "# training parameters\n",
    "output_dir: \"/home/ec2-user/SageMaker/models/llama-3-8b-naver-news\" # Temporary output directory for model checkpoints\n",
    "report_to: \"tensorboard\"               # report metrics to tensorboard\n",
    "logging_dir: \"/home/ec2-user/SageMaker/logs/llama-3-8b-naver-news\" # log folder for tensorboard\n",
    "learning_rate: 0.0002                  # learning rate 2e-4\n",
    "lr_scheduler_type: \"constant\"          # learning rate scheduler\n",
    "num_train_epochs: 1                    # number of training epochs\n",
    "optim: adamw_torch                     # use torch adamw optimizer\n",
    "logging_steps: 10                      # log every 10 steps\n",
    "save_strategy: epoch                   # save checkpoint every epoch\n",
    "evaluation_strategy: epoch             # evaluate every epoch\n",
    "max_grad_norm: 0.3                     # max gradient norm\n",
    "warmup_ratio: 0.03                     # warmup ratio\n",
    "bf16: true                             # use bfloat16 precision\n",
    "#tf32: true                             # use tf32 precision\n",
    "gradient_checkpointing: true          # use gradient checkpointing to save memory\n",
    "#activation_checkpointing: true         # use gradient checkpointing to save memory in FSDP config\n",
    "\n",
    "# FSDP parameters: https://huggingface.co/docs/transformers/main/en/fsd\n",
    "fsdp: \"full_shard auto_wrap offload\" # remove offload if enough GPU memory\n",
    "fsdp_config:\n",
    "    backward_prefetch: \"backward_pre\"\n",
    "    forward_prefetch: \"false\"\n",
    "    use_orig_params: \"false\"\n",
    "    #activation_checkpointing: \"true\"\n",
    "    #fsdp_limit_all_gathers: \"true\"\n",
    "    #fsdp_sync_module_states: \"true\"\n",
    "    #fsdp_offload_params: \"false\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c851f75d",
   "metadata": {},
   "source": [
    "## 3. í›ˆë ¨ Script ì‹¤í–‰\n",
    "\n",
    "ì•„ë˜ëŠ” Hugging Face ì˜ Accelerator ë¥¼ ì´ìš©í•œ PyTorch FSDP ë¥¼ ì‹¤í–‰í•˜ê¸° ìœ„í•œ ëª…ë ¹ì–´ ì…ë‹ˆë‹¤.\n",
    "- í˜„ì¬ ë¨¸ì‹ ì— 4ê°œì˜ GPU ê°€ ìˆëŠ” ê²½ìš° ì…ë‹ˆë‹¤. GPU ê°€ 1ê°œ ì´ë©´ nproc_per_node=1 ë¡œ ìˆ˜ì •í•´ì„œ ì‹¤í–‰ í•˜ì„¸ìš”. \n",
    "\n",
    "```\n",
    "!ACCELERATE_USE_FSDP=1 FSDP_CPU_RAM_EFFICIENT_LOADING=1 torchrun --nproc_per_node=4 \\\n",
    "../../scripts/local_run_fsdp_qlora.py \\\n",
    "--config config_folder_name/local_llama_3_8b_fsdp_qlora.yaml\n",
    "```\n",
    "- ì°¸ê³ \n",
    "    - Launching your ğŸ¤— Accelerate scripts, [Link](https://huggingface.co/docs/accelerate/en/basic_tutorials/launch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "650a43f8-666d-4176-94b5-885f2bcb58cd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "config_folder_name = \"accelerator_config\"\n",
    "os.makedirs(config_folder_name, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cb9be15-e2bc-4b03-a80b-8849e77d9e19",
   "metadata": {},
   "source": [
    "### Hugging Face  Accelerator ì— ì œê³µí•  config.yaml ì…ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4c36914-f44a-419c-8300-cab5bc402a1b",
   "metadata": {},
   "source": [
    "# shape ì•ˆë§ë‹¤ê³  í•˜ëŠ”ê±´ \"gradient_checkpointing\" ë¬¸ì œì´ë‹¤"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "11932e90-758a-4e78-af75-0045f1e87f0b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.0.0\n",
      "2.17.1\n",
      "0.25.1\n",
      "0.11.1\n",
      "0.44.0\n",
      "4.40.2\n",
      "0.12.0\n",
      "0.34.2\n",
      "2.232.1\n",
      "2.4.1+cu121\n"
     ]
    }
   ],
   "source": [
    "import datasets, tensorboard, huggingface_hub, trl, bitsandbytes, transformers, peft, accelerate, sagemaker, torch\n",
    "\n",
    "print (datasets.__version__)\n",
    "print (tensorboard.__version__)\n",
    "print (huggingface_hub.__version__)\n",
    "print (trl.__version__)\n",
    "print (bitsandbytes.__version__)\n",
    "print (transformers.__version__)\n",
    "print (peft.__version__)\n",
    "print (accelerate.__version__)\n",
    "print (sagemaker.__version__)\n",
    "print (torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6ae80df0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## script_args: \n",
      " ScriptArguments(train_dataset_path='../data/naver-news-summarization-ko/train', validation_dataset_path='../data/naver-news-summarization-ko/validation', model_id='meta-llama/Meta-Llama-3-8B', max_seq_length=2048)\n",
      "## training_args: \n",
      " TrainingArguments(\n",
      "_n_gpu=1,\n",
      "accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'gradient_accumulation_kwargs': None},\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "auto_find_batch_size=False,\n",
      "bf16=True,\n",
      "bf16_full_eval=False,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_persistent_workers=False,\n",
      "dataloader_pin_memory=True,\n",
      "dataloader_prefetch_factor=None,\n",
      "ddp_backend=None,\n",
      "ddp_broadcast_buffers=None,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "ddp_timeout=1800,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "dispatch_batches=None,\n",
      "do_eval=True,\n",
      "do_predict=False,\n",
      "do_train=False,\n",
      "eval_accumulation_steps=None,\n",
      "eval_delay=0,\n",
      "eval_do_concat_batches=True,\n",
      "eval_steps=None,\n",
      "evaluation_strategy=epoch,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "fsdp=[<FSDPOption.FULL_SHARD: 'full_shard'>, <FSDPOption.AUTO_WRAP: 'auto_wrap'>, <FSDPOption.OFFLOAD: 'offload'>],\n",
      "fsdp_config={'backward_prefetch': 'backward_pre', 'forward_prefetch': 'false', 'use_orig_params': 'false', 'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\n",
      "fsdp_min_num_params=0,\n",
      "fsdp_transformer_layer_cls_to_wrap=None,\n",
      "full_determinism=False,\n",
      "gradient_accumulation_steps=2,\n",
      "gradient_checkpointing=True,\n",
      "gradient_checkpointing_kwargs=None,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_always_push=False,\n",
      "hub_model_id=None,\n",
      "hub_private_repo=False,\n",
      "hub_strategy=every_save,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "include_inputs_for_metrics=False,\n",
      "include_num_input_tokens_seen=False,\n",
      "include_tokens_per_second=False,\n",
      "jit_mode_eval=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=0.0002,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=0,\n",
      "log_level=passive,\n",
      "log_level_replica=warning,\n",
      "log_on_each_node=True,\n",
      "logging_dir=/home/ec2-user/SageMaker/logs/llama-3-8b-naver-news,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=10,\n",
      "logging_strategy=steps,\n",
      "lr_scheduler_kwargs={},\n",
      "lr_scheduler_type=constant,\n",
      "max_grad_norm=0.3,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "neftune_noise_alpha=None,\n",
      "no_cuda=False,\n",
      "num_train_epochs=1,\n",
      "optim=adamw_torch,\n",
      "optim_args=None,\n",
      "optim_target_modules=None,\n",
      "output_dir=/home/ec2-user/SageMaker/models/llama-3-8b-naver-news,\n",
      "overwrite_output_dir=False,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=1,\n",
      "per_device_train_batch_size=1,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "ray_scope=last,\n",
      "remove_unused_columns=True,\n",
      "report_to=['tensorboard'],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=/home/ec2-user/SageMaker/models/llama-3-8b-naver-news,\n",
      "save_on_each_node=False,\n",
      "save_only_model=False,\n",
      "save_safetensors=True,\n",
      "save_steps=500,\n",
      "save_strategy=epoch,\n",
      "save_total_limit=None,\n",
      "seed=42,\n",
      "skip_memory_metrics=True,\n",
      "split_batches=None,\n",
      "tf32=None,\n",
      "torch_compile=False,\n",
      "torch_compile_backend=None,\n",
      "torch_compile_mode=None,\n",
      "torchdynamo=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_cpu=False,\n",
      "use_ipex=False,\n",
      "use_legacy_prediction_loop=False,\n",
      "use_mps_device=False,\n",
      "warmup_ratio=0.03,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      ")\n",
      "/home/ec2-user/SageMaker/.cs/conda/envs/finetune-image/lib/python3.10/site-packages/huggingface_hub/file_download.py:1142: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "You are an AI assistant specialized in news articles.Your role is to provide accurate summaries and insights in Korean. Please analyze the given text and provide concise, informative summaries that highlight the key goals and findings.\n",
      "\n",
      "Human: Please summarize the goals for journalist in this text:\n",
      "\n",
      "êµ­ë‚´ ìµœëŒ€ê·œëª¨ ë‚˜ë…¸ ì „ì‹œíšŒì´ì ì„¸ê³„ 3ëŒ€ ë‚˜ë…¸í–‰ì‚¬ ë‚˜ë…¸ì½”ë¦¬ì•„ 2022 ê°€ 6ì¼ë¶€í„° 3ì¼ê°„ ê²½ê¸°ë„ í‚¨í…ìŠ¤ ì œ1ì „ì‹œì¥ 4Â·5í™€ ì—ì„œ ê°œìµœëœë‹¤. ì˜¬í•´ 20íšŒë¥¼ ë§ëŠ” ë‚˜ë…¸ì½”ë¦¬ì•„ëŠ” ì‚°ì—…í†µìƒìì›ë¶€ ê³¼í•™ê¸°ìˆ ì •ë³´í†µì‹ ë¶€ê°€ ê³µë™ ì£¼ìµœí•œë‹¤. ë‚˜ë…¸ìœµí•©ì‚°ì—…ì—°êµ¬ì¡°í•©ê³¼ ë‚˜ë…¸ê¸°ìˆ ì—°êµ¬í˜‘ì˜íšŒê°€ ì£¼ê´€í•œë‹¤. ë‚˜ë…¸ê¸°ìˆ ê³¼ ì‚°ì—…ì˜ í˜„ì¬ ë¯¸ë˜ íŠ¸ëœë“œë¥¼ ì¡°ë§í•˜ëŠ” ê¸°ì¡° ê°•ì—°ì„ ì‹œì‘ìœ¼ë¡œ ë‚˜ë…¸ ìœµí•© ì „ì‹œí™”ì™€ êµ­ì œ ì‹¬í¬ì§€ì—„ í–‰ì‚¬ê°€ ë‹¤ì–‘í•˜ê²Œ ê°œìµœëœë‹¤. ê°•ë¯¼ì„ LGì´ë…¸í… ë¶€ì‚¬ì¥ì´ ììœ¨ì£¼í–‰ì‚°ì—… ë™í–¥ì— ë”°ë¥¸ ë‚˜ë…¸ê¸°ìˆ ê³¼ ì¸ê³µì§€ëŠ¥ AI ì˜ í™œìš© ì„ ì£¼ì œë¡œ ê¸°ì¡°ê°•ì—°ì„ í•œë‹¤. ì•Œë² ë¥´í˜ë¥´ í”„ë‘ìŠ¤ íŒŒë¦¬ ìŠˆë“œëŒ€ êµìˆ˜ë„ ê¸°ì¡° ê°•ì—°ì„ ë§¡ëŠ”ë‹¤. ì „ì‹œê·œëª¨ëŠ” ì½”ë¡œë‚˜19 ì´ì „ ìˆ˜ì¤€ìœ¼ë¡œ íšŒë³µëë‹¤. ì‚¼ì„±ì „ì LG ë“± ì£¼ìš” ê¸°ì—…ê³¼ ë‚˜ë…¸ ê¸°ìˆ  ê¸°ì—… ë“± ì´ 360ê°œì‚¬ê°€ ì°¸ì—¬í•œë‹¤. ë‚˜ë…¸ 20ì£¼ë…„ íŠ¹ë³„ ê¸°ë…ê´€ë„ ë§ˆë ¨ëœë‹¤. 20ì£¼ë…„ íŠ¹ë³„ ê¸°ë…ê´€ì—ëŠ” ì°¨ì„¸ëŒ€ ë°˜ë„ì²´ ë¯¸ë˜ì°¨ 6ì„¸ëŒ€ 6G ì´ë™í†µì‹  íƒ„ì†Œì¤‘ë¦½ ë””ì§€í„¸ ë°”ì´ì˜¤ ë“± 6ê°œ ë¶„ì•¼ í˜ì‹  ê¸°ìˆ ì´ ì†Œê°œëœë‹¤. ì‚°ì—…í™” ì„¸ì…˜ì—ì„œëŠ” ì§€ì† ê°€ëŠ¥ ì„±ì¥ì„ ìœ„í•œ ESG ë‚˜ë…¸ìœµí•©ê¸°ìˆ ì„ ì£¼ì œë¡œ ì´ˆì²­ ê°•ì—°ì´ ì—´ë¦°ë‹¤. ë‚˜ë…¸ì œí’ˆê±°ë˜ìƒë‹´íšŒ ì „ì‹œíšŒí…Œí¬ë‹ˆì»¬íˆ¬ì–´ ìµœì‹ ê¸°ìˆ ë°œí‘œíšŒ ë“± ë‹¤ì–‘í•œ ë‚˜ë…¸ ê´€ë ¨ ë¶€ëŒ€ í–‰ì‚¬ë„ ì¤€ë¹„ëœë‹¤.<|end_of_text|>\n",
      "\n",
      "Assistant: êµ­ë‚´ 3ëŒ€ ë‚˜ë…¸í–‰ì‚¬ ì¤‘ í•˜ë‚˜ì¸ ë‚˜ë…¸ì½”ë¦¬ì•„ 2022 ê°€ 6ì¼ë¶€í„° 3ì¼ê°„ ê²½ê¸°ë„ í‚¨í…ìŠ¤ ì œ1ì „ì‹œì¥ 4Â·5í™€ì—ì„œ ê°œìµœë˜ëŠ”ë°, ì‚¼ì„±ì „ì ë“± ì£¼ìš” ê¸°ì—…ê³¼ ë‚˜ë…¸ ê¸°ìˆ  ê¸°ì—… ë“± ì´ 360ê°œì‚¬ê°€ ì°¸ì—¬í•  ì˜ˆì •ì´ë©° ë‚˜ë…¸ê¸°ìˆ ê³¼ ì‚°ì—…ì˜ í˜„ì¬ ë¯¸ë˜ íŠ¸ëœë“œë¥¼ ì¡°ë§í•˜ëŠ” ê¸°ì¡° ê°•ì—°ì„ ì‹œì‘ìœ¼ë¡œ ë‚˜ë…¸ ìœµí•© ì „ì‹œí™”ì™€ êµ­ì œ ì‹¬í¬ì§€ì—„ í–‰ì‚¬ê°€ ë‹¤ì–‘í•˜ê²Œ ê°œìµœëœë‹¤.<|end_of_text|>\n",
      "You are an AI assistant specialized in news articles.Your role is to provide accurate summaries and insights in Korean. Please analyze the given text and provide concise, informative summaries that highlight the key goals and findings.\n",
      "\n",
      "Human: Please summarize the goals for journalist in this text:\n",
      "\n",
      "4ì¼ë¶€í„° ì„ë‹¬ê°„ ì¦ê¶Œì‚¬ ì‹ ìš©ìœµìë‹´ë³´ë¹„ìœ¨ ìœ ì§€ì˜ë¬´ ë©´ì œ ê¸ˆìœµë‹¹êµ­ì´ ì½”ìŠ¤í”¼ì§€ìˆ˜ê°€ ì¥ì¤‘ 2300 ì•„ë˜ê¹Œì§€ ë–¨ì–´ì§€ì ì£¼ì‹ì‹œì¥ ë³€ë™ì„±ì„ ì™„í™”í•˜ëŠ” ì¡°ì¹˜ë¥¼ ì‹œí–‰í•˜ê¸°ë¡œ í–ˆë‹¤. ê¸ˆìœµìœ„ì›íšŒëŠ” 1ì¼ ì£¼ì‹ì‹œì¥ ë§ˆê° í›„ ê¹€ì†Œì˜ ë¶€ìœ„ì›ì¥ ì£¼ì¬ë¡œ ì¦ê¶Œ ìœ ê´€ê¸°ê´€ê³¼ ê¸ˆìœµì‹œì¥í•©ë™ì ê²€íšŒì˜ë¥¼ ì—´ê³  ë³€ë™ì„± ì™„í™”ì¡°ì¹˜ë¥¼ ì‹œí–‰í•˜ê¸°ë¡œ í–ˆë‹¤ê³  ë°í˜”ë‹¤. ì´ì— ë”°ë¼ ë‹¤ìŒ ì£¼ì‹ì‹œì¥ ê°œì¥ì¼ì¸ ì˜¤ëŠ” 4ì¼ë¶€í„° 9ì›”30ì¼ê¹Œì§€ 3ê°œì›”ê°„ ì¦ê¶Œì‚¬ì˜ ì‹ ìš©ìœµìë‹´ë³´ë¹„ìœ¨ ìœ ì§€ì˜ë¬´ê°€ ë©´ì œëœë‹¤. ì‹ ìš©ìœµìë‹´ë³´ë¹„ìœ¨ ìœ ì§€ì˜ë¬´ë€ ì¦ê¶Œì‚¬ê°€ ì¼ëª… â€˜ë¹šíˆ¬â€™ ë¹šë‚´ì„œ íˆ¬ì ìê¸ˆì¸ ì‹ ìš©ìœµìë¥¼ ì‹œí–‰í•  ë•Œ ë‹´ë³´ë¥¼ 140% ì´ìƒ í™•ë³´í•˜ê³  ë‚´ê·œì—ì„œ ì •í•œ ë‹´ë³´ë¹„ìœ¨ì„ ìœ ì§€í•  ê²ƒì„ ìš”êµ¬í•˜ëŠ” ê·œì œë‹¤. ìœ ì§€ì˜ë¬´ ë©´ì œëŠ” ì¦ê¶Œì‚¬ì˜ ì‹ ìš©ìœµì ë‹´ë³´ì£¼ì‹ì— ëŒ€í•œ ê³¼ë„í•œ ë°˜ëŒ€ë§¤ë§¤ë¥¼ ì–µì œí•˜ê¸° ìœ„í•œ ì¡°ì¹˜ë‹¤. ê¸ˆìœµë‹¹êµ­ì€ ì½”ë¡œë‚˜19 ì‚¬íƒœê°€ ë³¸ê²©í™”í•œ 2020ë…„ 3ì›”ì—ë„ ì´ ê°™ì€ ì¡°ì¹˜ë¥¼ 6ê°œì›”ê°„ ì‹œí–‰í•œ ë°” ìˆë‹¤. 7ì¼ë¶€í„° 10ì›”6ì¼ê¹Œì§€ëŠ” ìƒì¥ê¸°ì—…ì˜ í•˜ë£¨ ìê¸°ì£¼ì‹ ë§¤ìˆ˜ ì£¼ë¬¸ ìˆ˜ëŸ‰ í•œë„ ì œí•œë„ ì™„í™”ëœë‹¤. ì‹ íƒì·¨ë“ ì£¼ì‹ë„ ë°œí–‰ì£¼ì‹ì´ìˆ˜ì˜ 1% ì´ë‚´ì—ì„œ ì‹ íƒì¬ì‚° ì´ì•¡ ë²”ìœ„ ë‚´ë¡œ í•œì‹œì ìœ¼ë¡œ í™•ëŒ€ëœë‹¤. ì´ì™€ í•¨ê»˜ ê¸ˆìœµìœ„ëŠ” ê¸ˆìœµê°ë…ì›ê³¼ í•œêµ­ê±°ë˜ì†Œ í•©ë™ìœ¼ë¡œ ê³µë§¤ë„ íŠ¹ë³„ì ê²€ì„ ì‹¤ì‹œí•´ ê³µë§¤ë„ í˜„í™©ê³¼ ì‹œì¥êµë€ ê°€ëŠ¥ì„± ë“±ì„ ì‚´í´ë³´ê¸°ë¡œ í–ˆë‹¤. í˜„ì¬ ê³µë§¤ë„ëŠ” ì§€ë‚œí•´ 5ì›” ì´í›„ ì œí•œì ìœ¼ë¡œ ì‹¤ì‹œë˜ê³  ìˆë‹¤. ê¸ˆìœµìœ„ì™€ ê¸ˆê°ì›ì€ ë§¤ì£¼ ê¸ˆìš”ì¼ ê¸ˆìœµì‹œì¥í•©ë™ì ê²€íšŒì˜ë¥¼ ì—´ì–´ ì¦ì‹œ ë“± ê¸ˆìœµì‹œì¥ìƒí™©ì„ ì ê²€í•  ì˜ˆì •ì´ë‹¤. ê¸ˆìœµë‹¹êµ­ì€ â€œì»¨í‹´ì „ì‹œí”Œëœì— ë”°ë¼ í•„ìš”í•œ ì‹œì¥ ë³€ë™ì„± ì™„í™” ì¡°ì¹˜ë¥¼ ê²€í† Â·ì‹œí–‰í•´ ë‚˜ê°ˆ ê²ƒâ€ì´ë¼ê³  ë§í–ˆë‹¤.<|end_of_text|>\n",
      "\n",
      "Assistant: 1ì¼ 1ì¼ ê¸ˆìœµìœ„ì›íšŒëŠ” ì¦ê¶Œ ìœ ê´€ê¸°ê´€ê³¼ ê¸ˆìœµì‹œì¥í•©ë™ì ê²€íšŒì˜ë¥¼ ì—´ê³  ì½”ìŠ¤í”¼ì§€ìˆ˜ê°€ ì¥ì¤‘ 2300 ì•„ë˜ê¹Œì§€ ë–¨ì–´ì§€ì ì£¼ì‹ì‹œì¥ ë³€ë™ì„±ì„ ì™„í™”í•˜ëŠ” ì¡°ì¹˜ë¥¼ ì‹œí–‰í•˜ê¸°ë¡œ í•˜ì˜€ìœ¼ë©°, ì´ì— ë”°ë¼ ë‹¤ìŒ ì£¼ì‹ì‹œì¥ ê°œì¥ì¼ì¸ ì˜¤ëŠ” 4ì¼ë¶€í„° 9ì›”30ì¼ê¹Œì§€ 3ê°œì›”ê°„ ì¦ê¶Œì‚¬ì˜ ì‹ ìš©ìœµìë‹´ë³´ë¹„ìœ¨ ìœ ì§€ì˜ë¬´ê°€ ë©´ì œëœë‹¤.<|end_of_text|>\n",
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:06<00:00,  1.73s/it]\n",
      "/home/ec2-user/SageMaker/.cs/conda/envs/finetune-image/lib/python3.10/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': dataset_text_field, max_seq_length, packing, dataset_kwargs. Will not be supported from version '1.0.0'.\n",
      "\n",
      "Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "/home/ec2-user/SageMaker/.cs/conda/envs/finetune-image/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:195: UserWarning: You passed a `packing` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n",
      "/home/ec2-user/SageMaker/.cs/conda/envs/finetune-image/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:283: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n",
      "/home/ec2-user/SageMaker/.cs/conda/envs/finetune-image/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:321: UserWarning: You passed a `dataset_text_field` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n",
      "/home/ec2-user/SageMaker/.cs/conda/envs/finetune-image/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:327: UserWarning: You passed a `dataset_kwargs` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n",
      "trainable params: 41,943,040 || all params: 8,072,204,288 || trainable%: 0.5196\n",
      "/home/ec2-user/SageMaker/.cs/conda/envs/finetune-image/lib/python3.10/site-packages/torch/distributed/fsdp/_init_utils.py:440: UserWarning: FSDP is switching to use `NO_SHARD` instead of ShardingStrategy.FULL_SHARD since the world size is 1.\n",
      "  warnings.warn(\n",
      "/home/ec2-user/SageMaker/.cs/conda/envs/finetune-image/lib/python3.10/site-packages/accelerate/accelerator.py:1557: UserWarning: Upcasted low precision parameters in Linear because mixed precision turned on in FSDP. Affects: weight.\n",
      "  warnings.warn(\n",
      "/home/ec2-user/SageMaker/.cs/conda/envs/finetune-image/lib/python3.10/site-packages/accelerate/accelerator.py:1563: UserWarning: FSDP upcast of low precision parameters may affect the precision of model checkpoints.\n",
      "  warnings.warn(\n",
      "  0%|                                                     | 0/1 [00:00<?, ?it/s]/home/ec2-user/SageMaker/.cs/conda/envs/finetune-image/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:20<00:00, 20.56s/it]\n",
      "  0%|                                                     | 0/3 [00:00<?, ?it/s]\u001b[A\n",
      " 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ               | 2/3 [00:00<00:00,  2.15it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 2.02921462059021, 'eval_runtime': 2.8014, 'eval_samples_per_second': 1.071, 'eval_steps_per_second': 1.071, 'epoch': 0.67}\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:23<00:00, 20.56s/it]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:02<00:00,  1.52it/s]\u001b[A\n",
      "                                                                                \u001b[A/home/ec2-user/SageMaker/.cs/conda/envs/finetune-image/lib/python3.10/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:689: FutureWarning: FSDP.state_dict_type() and FSDP.set_state_dict_type() are being deprecated. Please use APIs, get_state_dict() and set_state_dict(), which can support different parallelisms, FSDP1, FSDP2, DDP. API doc: https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_state_dict .Tutorial: https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html .\n",
      "  warnings.warn(\n",
      "/home/ec2-user/SageMaker/.cs/conda/envs/finetune-image/lib/python3.10/site-packages/torch/distributed/fsdp/_state_dict_utils.py:773: UserWarning: When using ``NO_SHARD`` for ``ShardingStrategy``, full_state_dict willbe returned.\n",
      "  warnings.warn(\n",
      "/home/ec2-user/SageMaker/.cs/conda/envs/finetune-image/lib/python3.10/site-packages/torch/distributed/fsdp/_state_dict_utils.py:716: UserWarning: When using ``NO_SHARD`` for ``ShardingStrategy``, full_state_dict willbe returned.\n",
      "  warnings.warn(\n",
      "/home/ec2-user/SageMaker/.cs/conda/envs/finetune-image/lib/python3.10/site-packages/huggingface_hub/file_download.py:1142: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "{'train_runtime': 28.9537, 'train_samples_per_second': 0.104, 'train_steps_per_second': 0.035, 'train_loss': 1.9335482120513916, 'epoch': 0.67}\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:28<00:00, 28.95s/it]\n",
      "/home/ec2-user/SageMaker/.cs/conda/envs/finetune-image/lib/python3.10/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:689: FutureWarning: FSDP.state_dict_type() and FSDP.set_state_dict_type() are being deprecated. Please use APIs, get_state_dict() and set_state_dict(), which can support different parallelisms, FSDP1, FSDP2, DDP. API doc: https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_state_dict .Tutorial: https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html .\n",
      "  warnings.warn(\n",
      "/home/ec2-user/SageMaker/.cs/conda/envs/finetune-image/lib/python3.10/site-packages/torch/distributed/fsdp/_state_dict_utils.py:773: UserWarning: When using ``NO_SHARD`` for ``ShardingStrategy``, full_state_dict willbe returned.\n",
      "  warnings.warn(\n",
      "/home/ec2-user/SageMaker/.cs/conda/envs/finetune-image/lib/python3.10/site-packages/torch/distributed/fsdp/_state_dict_utils.py:716: UserWarning: When using ``NO_SHARD`` for ``ShardingStrategy``, full_state_dict willbe returned.\n",
      "  warnings.warn(\n",
      "/home/ec2-user/SageMaker/.cs/conda/envs/finetune-image/lib/python3.10/site-packages/huggingface_hub/file_download.py:1142: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "!ACCELERATE_USE_FSDP=1 FSDP_CPU_RAM_EFFICIENT_LOADING=1 torchrun --nproc_per_node=1 \\\n",
    "../../scripts/local_run_fsdp_qlora.py \\\n",
    "--config accelerator_config/local_llama_3_8b_fsdp_qlora.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d806dc03-af6c-47c2-ad59-7f77b0d94f96",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m \u001b[0mAutoModelForCausalLM\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mmodel_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m\n",
       "Instantiate one of the model classes of the library (with a causal language modeling head) from a pretrained model.\n",
       "\n",
       "The model class to instantiate is selected based on the `model_type` property of the config object (either\n",
       "passed as an argument or loaded from `pretrained_model_name_or_path` if possible), or when it's missing, by\n",
       "falling back to using pattern matching on `pretrained_model_name_or_path`:\n",
       "\n",
       "    - **bart** -- [`BartForCausalLM`] (BART model)\n",
       "    - **bert** -- [`BertLMHeadModel`] (BERT model)\n",
       "    - **bert-generation** -- [`BertGenerationDecoder`] (Bert Generation model)\n",
       "    - **big_bird** -- [`BigBirdForCausalLM`] (BigBird model)\n",
       "    - **bigbird_pegasus** -- [`BigBirdPegasusForCausalLM`] (BigBird-Pegasus model)\n",
       "    - **biogpt** -- [`BioGptForCausalLM`] (BioGpt model)\n",
       "    - **blenderbot** -- [`BlenderbotForCausalLM`] (Blenderbot model)\n",
       "    - **blenderbot-small** -- [`BlenderbotSmallForCausalLM`] (BlenderbotSmall model)\n",
       "    - **bloom** -- [`BloomForCausalLM`] (BLOOM model)\n",
       "    - **camembert** -- [`CamembertForCausalLM`] (CamemBERT model)\n",
       "    - **code_llama** -- [`LlamaForCausalLM`] (CodeLlama model)\n",
       "    - **codegen** -- [`CodeGenForCausalLM`] (CodeGen model)\n",
       "    - **cohere** -- [`CohereForCausalLM`] (Cohere model)\n",
       "    - **cpmant** -- [`CpmAntForCausalLM`] (CPM-Ant model)\n",
       "    - **ctrl** -- [`CTRLLMHeadModel`] (CTRL model)\n",
       "    - **data2vec-text** -- [`Data2VecTextForCausalLM`] (Data2VecText model)\n",
       "    - **dbrx** -- [`DbrxForCausalLM`] (DBRX model)\n",
       "    - **electra** -- [`ElectraForCausalLM`] (ELECTRA model)\n",
       "    - **ernie** -- [`ErnieForCausalLM`] (ERNIE model)\n",
       "    - **falcon** -- [`FalconForCausalLM`] (Falcon model)\n",
       "    - **fuyu** -- [`FuyuForCausalLM`] (Fuyu model)\n",
       "    - **gemma** -- [`GemmaForCausalLM`] (Gemma model)\n",
       "    - **git** -- [`GitForCausalLM`] (GIT model)\n",
       "    - **gpt-sw3** -- [`GPT2LMHeadModel`] (GPT-Sw3 model)\n",
       "    - **gpt2** -- [`GPT2LMHeadModel`] (OpenAI GPT-2 model)\n",
       "    - **gpt_bigcode** -- [`GPTBigCodeForCausalLM`] (GPTBigCode model)\n",
       "    - **gpt_neo** -- [`GPTNeoForCausalLM`] (GPT Neo model)\n",
       "    - **gpt_neox** -- [`GPTNeoXForCausalLM`] (GPT NeoX model)\n",
       "    - **gpt_neox_japanese** -- [`GPTNeoXJapaneseForCausalLM`] (GPT NeoX Japanese model)\n",
       "    - **gptj** -- [`GPTJForCausalLM`] (GPT-J model)\n",
       "    - **jamba** -- [`JambaForCausalLM`] (Jamba model)\n",
       "    - **llama** -- [`LlamaForCausalLM`] (LLaMA model)\n",
       "    - **mamba** -- [`MambaForCausalLM`] (Mamba model)\n",
       "    - **marian** -- [`MarianForCausalLM`] (Marian model)\n",
       "    - **mbart** -- [`MBartForCausalLM`] (mBART model)\n",
       "    - **mega** -- [`MegaForCausalLM`] (MEGA model)\n",
       "    - **megatron-bert** -- [`MegatronBertForCausalLM`] (Megatron-BERT model)\n",
       "    - **mistral** -- [`MistralForCausalLM`] (Mistral model)\n",
       "    - **mixtral** -- [`MixtralForCausalLM`] (Mixtral model)\n",
       "    - **mpt** -- [`MptForCausalLM`] (MPT model)\n",
       "    - **musicgen** -- [`MusicgenForCausalLM`] (MusicGen model)\n",
       "    - **musicgen_melody** -- [`MusicgenMelodyForCausalLM`] (MusicGen Melody model)\n",
       "    - **mvp** -- [`MvpForCausalLM`] (MVP model)\n",
       "    - **olmo** -- [`OlmoForCausalLM`] (OLMo model)\n",
       "    - **open-llama** -- [`OpenLlamaForCausalLM`] (OpenLlama model)\n",
       "    - **openai-gpt** -- [`OpenAIGPTLMHeadModel`] (OpenAI GPT model)\n",
       "    - **opt** -- [`OPTForCausalLM`] (OPT model)\n",
       "    - **pegasus** -- [`PegasusForCausalLM`] (Pegasus model)\n",
       "    - **persimmon** -- [`PersimmonForCausalLM`] (Persimmon model)\n",
       "    - **phi** -- [`PhiForCausalLM`] (Phi model)\n",
       "    - **plbart** -- [`PLBartForCausalLM`] (PLBart model)\n",
       "    - **prophetnet** -- [`ProphetNetForCausalLM`] (ProphetNet model)\n",
       "    - **qdqbert** -- [`QDQBertLMHeadModel`] (QDQBert model)\n",
       "    - **qwen2** -- [`Qwen2ForCausalLM`] (Qwen2 model)\n",
       "    - **qwen2_moe** -- [`Qwen2MoeForCausalLM`] (Qwen2MoE model)\n",
       "    - **recurrent_gemma** -- [`RecurrentGemmaForCausalLM`] (RecurrentGemma model)\n",
       "    - **reformer** -- [`ReformerModelWithLMHead`] (Reformer model)\n",
       "    - **rembert** -- [`RemBertForCausalLM`] (RemBERT model)\n",
       "    - **roberta** -- [`RobertaForCausalLM`] (RoBERTa model)\n",
       "    - **roberta-prelayernorm** -- [`RobertaPreLayerNormForCausalLM`] (RoBERTa-PreLayerNorm model)\n",
       "    - **roc_bert** -- [`RoCBertForCausalLM`] (RoCBert model)\n",
       "    - **roformer** -- [`RoFormerForCausalLM`] (RoFormer model)\n",
       "    - **rwkv** -- [`RwkvForCausalLM`] (RWKV model)\n",
       "    - **speech_to_text_2** -- [`Speech2Text2ForCausalLM`] (Speech2Text2 model)\n",
       "    - **stablelm** -- [`StableLmForCausalLM`] (StableLm model)\n",
       "    - **starcoder2** -- [`Starcoder2ForCausalLM`] (Starcoder2 model)\n",
       "    - **transfo-xl** -- [`TransfoXLLMHeadModel`] (Transformer-XL model)\n",
       "    - **trocr** -- [`TrOCRForCausalLM`] (TrOCR model)\n",
       "    - **whisper** -- [`WhisperForCausalLM`] (Whisper model)\n",
       "    - **xglm** -- [`XGLMForCausalLM`] (XGLM model)\n",
       "    - **xlm** -- [`XLMWithLMHeadModel`] (XLM model)\n",
       "    - **xlm-prophetnet** -- [`XLMProphetNetForCausalLM`] (XLM-ProphetNet model)\n",
       "    - **xlm-roberta** -- [`XLMRobertaForCausalLM`] (XLM-RoBERTa model)\n",
       "    - **xlm-roberta-xl** -- [`XLMRobertaXLForCausalLM`] (XLM-RoBERTa-XL model)\n",
       "    - **xlnet** -- [`XLNetLMHeadModel`] (XLNet model)\n",
       "    - **xmod** -- [`XmodForCausalLM`] (X-MOD model)\n",
       "\n",
       "The model is set in evaluation mode by default using `model.eval()` (so for instance, dropout modules are\n",
       "deactivated). To train the model, you should first set it back in training mode with `model.train()`\n",
       "\n",
       "Args:\n",
       "    pretrained_model_name_or_path (`str` or `os.PathLike`):\n",
       "        Can be either:\n",
       "\n",
       "            - A string, the *model id* of a pretrained model hosted inside a model repo on huggingface.co.\n",
       "            - A path to a *directory* containing model weights saved using\n",
       "              [`~PreTrainedModel.save_pretrained`], e.g., `./my_model_directory/`.\n",
       "            - A path or url to a *tensorflow index checkpoint file* (e.g, `./tf_model/model.ckpt.index`). In\n",
       "              this case, `from_tf` should be set to `True` and a configuration object should be provided as\n",
       "              `config` argument. This loading path is slower than converting the TensorFlow checkpoint in a\n",
       "              PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.\n",
       "    model_args (additional positional arguments, *optional*):\n",
       "        Will be passed along to the underlying model `__init__()` method.\n",
       "    config ([`PretrainedConfig`], *optional*):\n",
       "        Configuration for the model to use instead of an automatically loaded configuration. Configuration can\n",
       "        be automatically loaded when:\n",
       "\n",
       "            - The model is a model provided by the library (loaded with the *model id* string of a pretrained\n",
       "              model).\n",
       "            - The model was saved using [`~PreTrainedModel.save_pretrained`] and is reloaded by supplying the\n",
       "              save directory.\n",
       "            - The model is loaded by supplying a local directory as `pretrained_model_name_or_path` and a\n",
       "              configuration JSON file named *config.json* is found in the directory.\n",
       "    state_dict (*Dict[str, torch.Tensor]*, *optional*):\n",
       "        A state dictionary to use instead of a state dictionary loaded from saved weights file.\n",
       "\n",
       "        This option can be used if you want to create a model from a pretrained configuration but load your own\n",
       "        weights. In this case though, you should check if using [`~PreTrainedModel.save_pretrained`] and\n",
       "        [`~PreTrainedModel.from_pretrained`] is not a simpler option.\n",
       "    cache_dir (`str` or `os.PathLike`, *optional*):\n",
       "        Path to a directory in which a downloaded pretrained model configuration should be cached if the\n",
       "        standard cache should not be used.\n",
       "    from_tf (`bool`, *optional*, defaults to `False`):\n",
       "        Load the model weights from a TensorFlow checkpoint save file (see docstring of\n",
       "        `pretrained_model_name_or_path` argument).\n",
       "    force_download (`bool`, *optional*, defaults to `False`):\n",
       "        Whether or not to force the (re-)download of the model weights and configuration files, overriding the\n",
       "        cached versions if they exist.\n",
       "    resume_download (`bool`, *optional*, defaults to `False`):\n",
       "        Whether or not to delete incompletely received files. Will attempt to resume the download if such a\n",
       "        file exists.\n",
       "    proxies (`Dict[str, str]`, *optional*):\n",
       "        A dictionary of proxy servers to use by protocol or endpoint, e.g., `{'http': 'foo.bar:3128',\n",
       "        'http://hostname': 'foo.bar:4012'}`. The proxies are used on each request.\n",
       "    output_loading_info(`bool`, *optional*, defaults to `False`):\n",
       "        Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.\n",
       "    local_files_only(`bool`, *optional*, defaults to `False`):\n",
       "        Whether or not to only look at local files (e.g., not try downloading the model).\n",
       "    revision (`str`, *optional*, defaults to `\"main\"`):\n",
       "        The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a\n",
       "        git-based system for storing models and other artifacts on huggingface.co, so `revision` can be any\n",
       "        identifier allowed by git.\n",
       "    trust_remote_code (`bool`, *optional*, defaults to `False`):\n",
       "        Whether or not to allow for custom models defined on the Hub in their own modeling files. This option\n",
       "        should only be set to `True` for repositories you trust and in which you have read the code, as it will\n",
       "        execute code present on the Hub on your local machine.\n",
       "    code_revision (`str`, *optional*, defaults to `\"main\"`):\n",
       "        The specific revision to use for the code on the Hub, if the code leaves in a different repository than\n",
       "        the rest of the model. It can be a branch name, a tag name, or a commit id, since we use a git-based\n",
       "        system for storing models and other artifacts on huggingface.co, so `revision` can be any identifier\n",
       "        allowed by git.\n",
       "    kwargs (additional keyword arguments, *optional*):\n",
       "        Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,\n",
       "        `output_attentions=True`). Behaves differently depending on whether a `config` is provided or\n",
       "        automatically loaded:\n",
       "\n",
       "            - If a configuration is provided with `config`, `**kwargs` will be directly passed to the\n",
       "              underlying model's `__init__` method (we assume all relevant updates to the configuration have\n",
       "              already been done)\n",
       "            - If a configuration is not provided, `kwargs` will be first passed to the configuration class\n",
       "              initialization function ([`~PretrainedConfig.from_pretrained`]). Each key of `kwargs` that\n",
       "              corresponds to a configuration attribute will be used to override said attribute with the\n",
       "              supplied `kwargs` value. Remaining keys that do not correspond to any configuration attribute\n",
       "              will be passed to the underlying model's `__init__` function.\n",
       "\n",
       "Examples:\n",
       "\n",
       "```python\n",
       ">>> from transformers import AutoConfig, AutoModelForCausalLM\n",
       "\n",
       ">>> # Download model and configuration from huggingface.co and cache.\n",
       ">>> model = AutoModelForCausalLM.from_pretrained(\"google-bert/bert-base-cased\")\n",
       "\n",
       ">>> # Update configuration during loading\n",
       ">>> model = AutoModelForCausalLM.from_pretrained(\"google-bert/bert-base-cased\", output_attentions=True)\n",
       ">>> model.config.output_attentions\n",
       "True\n",
       "\n",
       ">>> # Loading from a TF checkpoint file instead of a PyTorch model (slower)\n",
       ">>> config = AutoConfig.from_pretrained(\"./tf_model/bert_tf_model_config.json\")\n",
       ">>> model = AutoModelForCausalLM.from_pretrained(\n",
       "...     \"./tf_model/bert_tf_checkpoint.ckpt.index\", from_tf=True, config=config\n",
       "... )\n",
       "```\n",
       "\u001b[0;31mFile:\u001b[0m      ~/SageMaker/.cs/conda/envs/finetune-image/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py\n",
       "\u001b[0;31mType:\u001b[0m      method"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import BitsAndBytesConfig, AutoModelForCausalLM\n",
    "#BitsAndBytesConfig?\n",
    "AutoModelForCausalLM.from_pretrained?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "441607e4-f93f-438e-8985-99a76233fe47",
   "metadata": {},
   "source": [
    "## 4. ë² ì´ìŠ¤ ëª¨ë¸ê³¼ í›ˆë ¨ëœ ëª¨ë¸ ë¨¸ì§€"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cebdb212",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_id, output_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3a8acc4",
   "metadata": {},
   "source": [
    "### ëª¨ë¸ ë¨¸ì§€ ë° ë¡œì»¬ì— ì €ì¥\n",
    "- ì•½ 2ë¶„ ê±¸ë¦¼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19543636",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from peft import AutoPeftModelForCausalLM\n",
    "import torch\n",
    "\n",
    "# Load PEFT model on CPU\n",
    "\n",
    "model = AutoPeftModelForCausalLM.from_pretrained(\n",
    "    output_dir,\n",
    "    torch_dtype=torch.float16,\n",
    "    low_cpu_mem_usage=True,\n",
    ")  \n",
    "# Merge LoRA and base model and save\n",
    "merged_model = model.merge_and_unload()\n",
    "merged_model.save_pretrained(output_dir,safe_serialization=True, max_shard_size=\"2GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d0e3827",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.device, merged_model.device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aff0ea42",
   "metadata": {},
   "source": [
    "### ë¨¸ì§€ëœ ëª¨ë¸ ë¡œë”©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1a6607d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from peft import AutoPeftModelForCausalLM\n",
    "from transformers import AutoTokenizer \n",
    "\n",
    "\n",
    "# Load Model with PEFT adapter\n",
    "model = AutoPeftModelForCausalLM.from_pretrained(\n",
    "  pretrained_model_name_or_path = output_dir,\n",
    "  torch_dtype=torch.float16,\n",
    "  quantization_config= {\"load_in_4bit\": True},\n",
    "  device_map=\"auto\"\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6908046b",
   "metadata": {},
   "source": [
    "## 5. ì¶”ë¡ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5388b34",
   "metadata": {},
   "source": [
    "### í…ŒìŠ¤íŠ¸ ë°ì´í„° ì…‹ ë¡œë”©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eacd377",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset \n",
    "from random import randint\n",
    "\n",
    "def get_message_from_dataset(sample_dataset_json_file):\n",
    "    # Load our test dataset\n",
    "    full_test_dataset = load_dataset(\"json\", data_files=sample_dataset_json_file, split=\"train\")\n",
    "\n",
    "    # Test on sample \n",
    "    rand_idx = randint(0, len(full_test_dataset)-1)\n",
    "    rand_idx = 75\n",
    "    print(\"rand_idx: \", rand_idx)\n",
    "    messages = full_test_dataset[rand_idx][\"messages\"][:2]\n",
    "    # messages = test_dataset[rand_idx][\"text\"][:2]\n",
    "    print(\"messages: \\n\", messages)\n",
    "\n",
    "    return messages, full_test_dataset, rand_idx\n",
    "\n",
    "messages, full_test_dataset, rand_idx = get_message_from_dataset(sample_dataset_json_file = full_test_data_json)    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba15b47b",
   "metadata": {},
   "source": [
    "### ì¶”ë¡ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddaf71a0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "def generate_response(messages, model, tokenizer, full_test_dataset, rand_idx):\n",
    "    input_ids = tokenizer.apply_chat_template(messages,add_generation_prompt=True,return_tensors=\"pt\").to(model.device)\n",
    "    outputs = model.generate(\n",
    "        input_ids,\n",
    "        max_new_tokens=512,\n",
    "        eos_token_id= tokenizer.eos_token_id,\n",
    "        do_sample=True,\n",
    "        temperature=0.6,\n",
    "        top_p=0.9,\n",
    "    )\n",
    "    response = outputs[0][input_ids.shape[-1]:]\n",
    "\n",
    "    print(f\"**Query:**\\n{full_test_dataset[rand_idx]['messages'][1]['content']}\\n\")\n",
    "    # print(f\"**Query:**\\n{test_dataset[rand_idx]['text'][1]['content']}\\n\")\n",
    "    # print(f\"**Original Answer:**\\n{test_dataset[rand_idx]['text'][2]['content']}\\n\")\n",
    "    print(f\"**Original Answer:**\\n{full_test_dataset[rand_idx]['messages'][2]['content']}\\n\")\n",
    "    print(f\"**Generated Answer:**\\n{tokenizer.decode(response,skip_special_tokens=True)}\")\n",
    "\n",
    "generate_response(messages, model, tokenizer, full_test_dataset, rand_idx)    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "360223a2-02b4-429d-8e47-6ec5ca55247e",
   "metadata": {},
   "source": [
    "### í• ë‹¹ëœ CUDA memoryë¥¼ Release"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14070303-ea9f-40e5-a56e-cca1bf56f7f8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "del model\n",
    "del tokenizer\n",
    "torch.cuda.empty_cache()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_finetune-image",
   "language": "python",
   "name": "conda_finetune-image"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "vscode": {
   "interpreter": {
    "hash": "6daafc7ae2313787fa97137de7504cfa7c5a594d29476828201b4f7d7fb5c4e1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
