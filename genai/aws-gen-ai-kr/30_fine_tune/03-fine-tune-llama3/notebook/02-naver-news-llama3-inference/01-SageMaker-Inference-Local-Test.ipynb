{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 모델 추론 로컬 테스트\n",
    "\n",
    "- 이 노트북은 Hugging Face TGI (Text Generation Interface) 의 도커 이미지를 다운로드 받아서,이전의 파인 튜닝된 모델을 도커 컨테이너로 테스트를 합니다.\n",
    "\n",
    "### 필수 사항\n",
    "- 파인 튜닝된 모델이 있어야 하기에, 아래의 노트북을 먼저 실행을 하셔야 합니다.\n",
    "    - [01-naver-news-fsdp-QLoRA/03-SageMaker-Training.ipynb](../01-naver-news-fsdp-QLoRA/03-SageMaker-Training.ipynb) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 환경 구성\n",
    "01-naver-news-fsdp-QLoRA/03-SageMaker-Training.ipynb 노트북에서 model_s3_path 에 저장한 경로를 가져 옵니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_s3_path:  {'S3DataSource': {'S3Uri': 's3://sagemaker-us-east-1-057716757052/llama3-8b-naver-news-2024-07-27-07-50-0-2024-07-27-07-50-10-938/output/model/', 'S3DataType': 'S3Prefix', 'CompressionType': 'None'}}\n"
     ]
    }
   ],
   "source": [
    "%store -r model_s3_path\n",
    "print(\"model_s3_path: \", model_s3_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 모델 아티펙트 다운로드 \n",
    "이전에 훈련한 파인 튜닝 모델의 모델 아티펙트를 다운로드 합니다.\n",
    "- 모델 다운로드는 아래 경로 입니다. 사용하시는 환경이 다르면 수정 하세요.\n",
    "    - ```model_data_dir = \"/home/ec2-user/SageMaker/models/llama-3-8b-naver-news-s3-download\" ```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_data_dir:  /home/ec2-user/SageMaker/models/llama-3-8b-naver-news-s3-download\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "model_data_dir = \"/home/ec2-user/SageMaker/models/llama-3-8b-naver-news-s3-download\"\n",
    "os.makedirs(model_data_dir, exist_ok=True)\n",
    "print(\"model_data_dir: \", model_data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "artifact_path:  s3://sagemaker-us-east-1-057716757052/llama3-8b-naver-news-2024-07-27-07-50-0-2024-07-27-07-50-10-938/output/model/\n"
     ]
    }
   ],
   "source": [
    "artifact_path = model_s3_path[\"S3DataSource\"][\"S3Uri\"]\n",
    "print(\"artifact_path: \", artifact_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://sagemaker-us-east-1-057716757052/llama3-8b-naver-news-2024-07-27-07-50-0-2024-07-27-07-50-10-938/output/model/\n",
      "/home/ec2-user/SageMaker/models/llama-3-8b-naver-news-s3-download\n",
      "download: s3://sagemaker-us-east-1-057716757052/llama3-8b-naver-news-2024-07-27-07-50-0-2024-07-27-07-50-10-938/output/model/config.json to ../../../../../../../models/llama-3-8b-naver-news-s3-download/config.json\n",
      "download: s3://sagemaker-us-east-1-057716757052/llama3-8b-naver-news-2024-07-27-07-50-0-2024-07-27-07-50-10-938/output/model/generation_config.json to ../../../../../../../models/llama-3-8b-naver-news-s3-download/generation_config.json\n",
      "download: s3://sagemaker-us-east-1-057716757052/llama3-8b-naver-news-2024-07-27-07-50-0-2024-07-27-07-50-10-938/output/model/model-00002-of-00007.safetensors to ../../../../../../../models/llama-3-8b-naver-news-s3-download/model-00002-of-00007.safetensors\n",
      "download: s3://sagemaker-us-east-1-057716757052/llama3-8b-naver-news-2024-07-27-07-50-0-2024-07-27-07-50-10-938/output/model/model-00005-of-00007.safetensors to ../../../../../../../models/llama-3-8b-naver-news-s3-download/model-00005-of-00007.safetensors\n",
      "download: s3://sagemaker-us-east-1-057716757052/llama3-8b-naver-news-2024-07-27-07-50-0-2024-07-27-07-50-10-938/output/model/special_tokens_map.json to ../../../../../../../models/llama-3-8b-naver-news-s3-download/special_tokens_map.json\n",
      "download: s3://sagemaker-us-east-1-057716757052/llama3-8b-naver-news-2024-07-27-07-50-0-2024-07-27-07-50-10-938/output/model/model.safetensors.index.json to ../../../../../../../models/llama-3-8b-naver-news-s3-download/model.safetensors.index.json\n",
      "download: s3://sagemaker-us-east-1-057716757052/llama3-8b-naver-news-2024-07-27-07-50-0-2024-07-27-07-50-10-938/output/model/model-00001-of-00007.safetensors to ../../../../../../../models/llama-3-8b-naver-news-s3-download/model-00001-of-00007.safetensors\n",
      "download: s3://sagemaker-us-east-1-057716757052/llama3-8b-naver-news-2024-07-27-07-50-0-2024-07-27-07-50-10-938/output/model/tokenizer_config.json to ../../../../../../../models/llama-3-8b-naver-news-s3-download/tokenizer_config.json\n",
      "download: s3://sagemaker-us-east-1-057716757052/llama3-8b-naver-news-2024-07-27-07-50-0-2024-07-27-07-50-10-938/output/model/tokenizer.json to ../../../../../../../models/llama-3-8b-naver-news-s3-download/tokenizer.json\n",
      "download: s3://sagemaker-us-east-1-057716757052/llama3-8b-naver-news-2024-07-27-07-50-0-2024-07-27-07-50-10-938/output/model/model-00003-of-00007.safetensors to ../../../../../../../models/llama-3-8b-naver-news-s3-download/model-00003-of-00007.safetensors\n",
      "download: s3://sagemaker-us-east-1-057716757052/llama3-8b-naver-news-2024-07-27-07-50-0-2024-07-27-07-50-10-938/output/model/model-00004-of-00007.safetensors to ../../../../../../../models/llama-3-8b-naver-news-s3-download/model-00004-of-00007.safetensors\n",
      "download: s3://sagemaker-us-east-1-057716757052/llama3-8b-naver-news-2024-07-27-07-50-0-2024-07-27-07-50-10-938/output/model/model-00007-of-00007.safetensors to ../../../../../../../models/llama-3-8b-naver-news-s3-download/model-00007-of-00007.safetensors\n",
      "download: s3://sagemaker-us-east-1-057716757052/llama3-8b-naver-news-2024-07-27-07-50-0-2024-07-27-07-50-10-938/output/model/model-00006-of-00007.safetensors to ../../../../../../../models/llama-3-8b-naver-news-s3-download/model-00006-of-00007.safetensors\n",
      "total 31377252\n",
      "drwxrwxr-x 2 ec2-user ec2-user       4096 Jul 27 14:08 .\n",
      "drwxrwxr-x 7 ec2-user ec2-user       4096 Jul 27 07:44 ..\n",
      "-rw-rw-r-- 1 ec2-user ec2-user        697 Jul 27 08:03 config.json\n",
      "-rw-rw-r-- 1 ec2-user ec2-user        172 Jul 27 08:03 generation_config.json\n",
      "-rw-rw-r-- 1 ec2-user ec2-user 4886466168 Jul 27 08:02 model-00001-of-00007.safetensors\n",
      "-rw-rw-r-- 1 ec2-user ec2-user 4832007448 Jul 27 08:03 model-00002-of-00007.safetensors\n",
      "-rw-rw-r-- 1 ec2-user ec2-user 4999813112 Jul 27 08:03 model-00003-of-00007.safetensors\n",
      "-rw-rw-r-- 1 ec2-user ec2-user 4999813128 Jul 27 08:02 model-00004-of-00007.safetensors\n",
      "-rw-rw-r-- 1 ec2-user ec2-user 4832007496 Jul 27 08:02 model-00005-of-00007.safetensors\n",
      "-rw-rw-r-- 1 ec2-user ec2-user 4999813120 Jul 27 08:03 model-00006-of-00007.safetensors\n",
      "-rw-rw-r-- 1 ec2-user ec2-user 2571158184 Jul 27 08:03 model-00007-of-00007.safetensors\n",
      "-rw-rw-r-- 1 ec2-user ec2-user      23950 Jul 27 08:03 model.safetensors.index.json\n",
      "-rw-rw-r-- 1 ec2-user ec2-user        335 Jul 27 08:03 special_tokens_map.json\n",
      "-rw-rw-r-- 1 ec2-user ec2-user      50988 Jul 27 08:03 tokenizer_config.json\n",
      "-rw-rw-r-- 1 ec2-user ec2-user    9085698 Jul 27 08:03 tokenizer.json\n"
     ]
    }
   ],
   "source": [
    "%%sh -s {artifact_path} {model_data_dir}\n",
    "\n",
    "artifact_path=$1\n",
    "model_data_dir=$2\n",
    "\n",
    "echo $artifact_path\n",
    "echo $model_data_dir\n",
    "\n",
    "# 기존 데이터 삭제\n",
    "rm -rf $model_data_dir/*\n",
    "\n",
    "# 모델을 S3에서 로컬로 다운로드\n",
    "aws s3 cp $artifact_path $model_data_dir --recursive\n",
    "\n",
    "ls -al $model_data_dir\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. HF TGI 도커 컨테이너 실행 예시 보기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### HF TGI Docker Container Shell 보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "aws ecr get-login-password --region us-east-1 | docker login --username AWS --password-stdin 763104351884.dkr.ecr.us-east-1.amazonaws.com\n",
      "\n",
      "# Get TGI image\n",
      "docker pull 763104351884.dkr.ecr.us-east-1.amazonaws.com/huggingface-pytorch-tgi-inference:2.3.0-tgi2.0.2-gpu-py310-cu121-ubuntu22.04 \n",
      "\n",
      "# export env. vars \n",
      "export MODEL_REPO_DIR=/home/ec2-user/SageMaker/models/llama-3-8b-naver-news-s3-download\n",
      "export MODEL_LOG_DIR=/home/ec2-user/SageMaker/tmp/log\n",
      "\n",
      "mkdir -p $MODEL_LOG_DIR\n",
      "\n",
      "docker run -it --runtime=nvidia --gpus all --shm-size 12g \\\n",
      " -v $MODEL_REPO_DIR:/opt/ml/model:ro \\\n",
      " -v $MODEL_LOG_DIR:/opt/djl/logs \\\n",
      " -p 8080:8080 \\\n",
      " -e HF_MODEL_ID=/opt/ml/model \\\n",
      " -e SM_NUM_GPUS=4 \\\n",
      " -e MESSAGES_API_ENABLED=true \\\n",
      " 763104351884.dkr.ecr.us-east-1.amazonaws.com/huggingface-pytorch-tgi-inference:2.3.0-tgi2.0.2-gpu-py310-cu121-ubuntu22.04 \n",
      "\n",
      "# if needed, comment out and run as a test\n",
      "# curl http://localhost:8080/generate \\\n",
      "#     -X POST \\\n",
      "#     -d '{\"inputs\":\"What is the capital of France?\",\"parameters\":{\"max_new_tokens\":50}}' \\\n",
      "#     -H 'Content-Type: application/json'\n",
      "\n",
      "# curl http://localhost:8080/generate_stream \\\n",
      "#     -X POST \\\n",
      "#     -d '{\"inputs\":\"Tell me a short story\",\"parameters\":{\"max_new_tokens\":100}}' \\\n",
      "#     -H 'Content-Type: application/json'"
     ]
    }
   ],
   "source": [
    "! cat /home/ec2-user/SageMaker/aws-ai-ml-workshop-kr/genai/aws-gen-ai-kr/30_fine_tune/03-fine-tune-llama3/notebook/02-naver-news-llama3-inference/local_test/run_tgi_docker_container.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### docker container shell 실행\n",
    "아래는 run_docker_container_shell 를 실행한 화면 입니다.\n",
    "- 첫 번째는 docker image 를 ECR 로 부터 다운로드 합니다.\n",
    "    - ![run_docker_container_shell.png](img/run_docker_container_shell.png)\n",
    "- 두 번째는 tgi docker container 실행 하고, 요청을 받기 위해 준비된 상태 입니다.\n",
    "    - ![run_tgi_container.png](img/run_tgi_container.png)\n",
    "    - ![loading_tgi.png](img/loading_tgi.png)\n",
    "    - ![ tgi_container_ready.png](img/tgi_container_ready.png)\n",
    "- 이렇게 실행된 docker container 를 root 로 접근해서 컨테이너 내부를 보겠습니다.\n",
    "    - ![TGI_container_inside.png](img/TGI_container_inside.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. HF TGI 도커 컨테이너 실행 직접 하기\n",
    "\n",
    "단계 1: Terminal 를 오픈 합니다. <br>\n",
    "--> 왼쪽 상단의 File >> New >> Terminal 차례대로 선택 합니다. <br><br>\n",
    "단계 2: 아래 처럼 해당 경로에 S3 에서 다운로드한 모델 파일들이 있는지 확인 합니다.\n",
    "```\n",
    "ls /home/ec2-user/SageMaker/models/llama-3-8b-naver-news-s3-download\n",
    "```\n",
    "단계 3: 아래 처럼 해당 경로로 이동 합니다.\n",
    "```\n",
    "cd /home/ec2-user/SageMaker/aws-ai-ml-workshop-kr/genai/aws-gen-ai-kr/30_fine_tune/03-fine-tune-llama3/notebook/02-naver-news-llama3-inference/local_test\n",
    "```\n",
    "단계 4: 아래 처럼 shell 을 실행 합니다.\n",
    "```\n",
    "./run_tgi_docker_container.sh \n",
    "```\n",
    "위의 실행 화면과 같이 나오면 정상 입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 도커 실행 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "def docker_inference(endpoint,headers, payload):\n",
    "    response = requests.post(\n",
    "        endpoint, headers=headers, json=payload)\n",
    "    return response\n",
    "\n",
    "\n",
    "endpoint = \"http://127.0.0.1:8080/generate\"\n",
    "headers = {\"content-type\": \"application/json\"}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Completion 스타일 프롬프트 실행 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'generated_text': ' Paris, of course! But what about the capital of the world? That’s a little more difficult to answer. There are many contenders for the title, but there is no clear consensus. Some people might say that New York City is the capital of'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "payload = {\"inputs\":\"What is the capital of France?\",\"parameters\":{\"max_new_tokens\":50}}\n",
    "response = docker_inference(endpoint,headers, payload)\n",
    "result = json.loads(response.text)\n",
    "result\n",
    "#json.dumps(result, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Message Style 프롬프트 실행 테스트\n",
    "- 아래는 현재 에러 발생. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [422]>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "payload = {'messages': [{'role': 'system',\n",
    "   'content': 'You are a helpful assistant and write only in English'},\n",
    "  {'role': 'user', 'content': 'How to make cake?'}],\n",
    " 'model': 'meta-llama-3-fine-tuned',\n",
    " 'parameters': {'max_tokens': 512,\n",
    "  'top_p': 0.9,\n",
    "  'temperature': 0.6,\n",
    "  'stop': ['<|eot_id|>']}}\n",
    "response = docker_inference(endpoint,headers, payload)\n",
    "response\n",
    "# result = json.loads(response.text)\n",
    "# result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Failed to deserialize the JSON body into the target type: missing field `inputs` at line 1 column 281'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 도커 실행 중지\n",
    "아래와 같이 명령어를 제공하여 컨테이너를 중지 합니다.\n",
    "- ![docker_stop.png](img/docker_stop.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "축하 합니다. 로컬에서 TGI Docker Container 테스트가 완료 되었습니다.  ^^"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.14 ('pytorch_p310')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "vscode": {
   "interpreter": {
    "hash": "e2257b1c3513dc4782645ad49f694a4b0012bebbbbc3534a56d350db8e4f89a2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
