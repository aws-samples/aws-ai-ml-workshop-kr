{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "81f236c9",
   "metadata": {},
   "source": [
    "# Faster autoscaling on Amazon SageMaker realtime endpoints with inference components (Application Autoscaling)\n",
    "\n",
    "---\n",
    "\n",
    "This notebook's CI test result for us-west-2 is as follows. CI test results in other regions can be found at the end of the notebook.\n",
    "\n",
    "![This us-west-2 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/us-west-2/inference|generativeai|huggingfacetgi|meta-llama|llama3-8b|faster-autoscaling|realtime-endpoints|FasterAutoscaling-IC-Llama3-8B-AppAutoScaling.ipynb)\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "In this notebook we show how the new faster autoscaling feature helps scale sagemaker inference endpoints by almost 6x faster than earlier.\n",
    "\n",
    "We deploy Meta's `Llama3-8B-Instruct` model to an Amazon SageMaker realtime endpoint using Text Generation Inference (TGI) Deep Learning Container (DLC) and apply <span style='color:green'><b>Application Autoscaling</b></span> scaling policies to the endpoint.\n",
    "\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "    Please select <b>m5.2xlarge</b> or larger instance types when running this on Amazon SageMaker Notebook Instance.<br/>\n",
    "    Select <b>conda_pytorch_p310</b> kernel when running this notebook on Amazon SageMaker Notebook Instance. <br/><br/>\n",
    "    Ensure python version for the kernel is <b>3.10.x</b> (3.11 is not supported). <br/>\n",
    "</div>\n",
    "\n",
    "---\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "\n",
    "\n",
    "<div style=\"border: 1px solid #f00; border-radius: 5px; padding: 10px; background-color: #fee;\">\n",
    "Before using this notebook please ensure you have access to an active access token from HuggingFace and have accepted the license agreement from Meta.\n",
    "\n",
    "- **Step 1:** Create user access token in HuggingFace (HF). Refer [here](https://huggingface.co/docs/hub/security-tokens) on how to create HF tokens.\n",
    "- **Step 2:** Login to [HuggingFace](https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct/tree/main) and navigate to *Meta-Llama-3-8B-Instruct** home page.\n",
    "- **Step 3:** Accept META LLAMA 3 COMMUNITY LICENSE AGREEMENT by following the instructions [here](https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct/tree/main)\n",
    "- **Step 4:** Wait for the approval email from META (Approval may take any where b/w 1-3 hrs)\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a241652-091a-4769-9480-ba64b9e30c9d",
   "metadata": {
    "tags": []
   },
   "source": [
    "Install packages using uv, an extremely fast python package installer\\\n",
    "Read more about uv here <https://astral.sh/blog/uv>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7767c519-29c9-4794-8a4e-67cb43779697",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.10.14\n"
     ]
    }
   ],
   "source": [
    "# ensure python version of the selected kernel is not greater than 3.10\n",
    "!python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9d260cb1-1355-448e-8fd9-3eebb1584ba0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting uv\n",
      "  Downloading uv-0.2.30-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (35 kB)\n",
      "Downloading uv-0.2.30-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.9/13.9 MB\u001b[0m \u001b[31m110.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: uv\n",
      "Successfully installed uv-0.2.30\n",
      "\u001b[2K\u001b[2mResolved \u001b[1m22 packages\u001b[0m \u001b[2min 63ms\u001b[0m\u001b[0m                                         \u001b[0m\n",
      "\u001b[2K\u001b[2mPrepared \u001b[1m14 packages\u001b[0m \u001b[2min 326ms\u001b[0m\u001b[0m                                            \n",
      "\u001b[2mUninstalled \u001b[1m14 packages\u001b[0m \u001b[2min 372ms\u001b[0m\u001b[0m\n",
      "\u001b[2K\u001b[2mInstalled \u001b[1m14 packages\u001b[0m \u001b[2min 49ms\u001b[0m\u001b[0m                               \u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1mexceptiongroup\u001b[0m\u001b[2m==1.2.0 (from file:///home/conda/feedstock_root/build_artifacts/exceptiongroup_1704921103267/work)\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mexceptiongroup\u001b[0m\u001b[2m==1.2.2\u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1mipython\u001b[0m\u001b[2m==8.22.2 (from file:///home/conda/feedstock_root/build_artifacts/ipython_1709559745751/work)\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mipython\u001b[0m\u001b[2m==8.26.0\u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1mipywidgets\u001b[0m\u001b[2m==8.1.2 (from file:///home/conda/feedstock_root/build_artifacts/ipywidgets_1707427226251/work)\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mipywidgets\u001b[0m\u001b[2m==8.1.3\u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1mjedi\u001b[0m\u001b[2m==0.18.2 (from file:///home/conda/feedstock_root/build_artifacts/jedi_1669134318875/work)\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mjedi\u001b[0m\u001b[2m==0.19.1\u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1mjupyterlab-widgets\u001b[0m\u001b[2m==3.0.10 (from file:///home/conda/feedstock_root/build_artifacts/jupyterlab_widgets_1707421892171/work)\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mjupyterlab-widgets\u001b[0m\u001b[2m==3.0.11\u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1mmatplotlib-inline\u001b[0m\u001b[2m==0.1.6 (from file:///home/conda/feedstock_root/build_artifacts/matplotlib-inline_1660814786464/work)\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mmatplotlib-inline\u001b[0m\u001b[2m==0.1.7\u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1mparso\u001b[0m\u001b[2m==0.8.3 (from file:///home/conda/feedstock_root/build_artifacts/parso_1638334955874/work)\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mparso\u001b[0m\u001b[2m==0.8.4\u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1mprompt-toolkit\u001b[0m\u001b[2m==3.0.42 (from file:///home/conda/feedstock_root/build_artifacts/prompt-toolkit_1702399386289/work)\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mprompt-toolkit\u001b[0m\u001b[2m==3.0.47\u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1mpure-eval\u001b[0m\u001b[2m==0.2.2 (from file:///home/conda/feedstock_root/build_artifacts/pure_eval_1642875951954/work)\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mpure-eval\u001b[0m\u001b[2m==0.2.3\u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1mpygments\u001b[0m\u001b[2m==2.17.2 (from file:///home/conda/feedstock_root/build_artifacts/pygments_1700607939962/work)\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mpygments\u001b[0m\u001b[2m==2.18.0\u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1mstack-data\u001b[0m\u001b[2m==0.6.2 (from file:///home/conda/feedstock_root/build_artifacts/stack_data_1669632077133/work)\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mstack-data\u001b[0m\u001b[2m==0.6.3\u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1mtraitlets\u001b[0m\u001b[2m==5.14.2 (from file:///home/conda/feedstock_root/build_artifacts/traitlets_1710254411456/work)\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mtraitlets\u001b[0m\u001b[2m==5.14.3\u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1mtyping-extensions\u001b[0m\u001b[2m==4.10.0 (from file:///home/conda/feedstock_root/build_artifacts/typing_extensions_1708904622550/work)\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mtyping-extensions\u001b[0m\u001b[2m==4.12.2\u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1mwidgetsnbextension\u001b[0m\u001b[2m==4.0.10 (from file:///home/conda/feedstock_root/build_artifacts/widgetsnbextension_1707420319466/work)\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mwidgetsnbextension\u001b[0m\u001b[2m==4.0.11\u001b[0m\n",
      "\u001b[2K\u001b[2mResolved \u001b[1m65 packages\u001b[0m \u001b[2min 341ms\u001b[0m\u001b[0m                                        \u001b[0m\n",
      "\u001b[2K\u001b[2mPrepared \u001b[1m22 packages\u001b[0m \u001b[2min 848ms\u001b[0m\u001b[0m                                            \n",
      "\u001b[2mUninstalled \u001b[1m22 packages\u001b[0m \u001b[2min 538ms\u001b[0m\u001b[0m\n",
      "\u001b[2K\u001b[2mInstalled \u001b[1m22 packages\u001b[0m \u001b[2min 50ms\u001b[0m\u001b[0m                               \u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1mboto3\u001b[0m\u001b[2m==1.34.101\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mboto3\u001b[0m\u001b[2m==1.34.142\u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1mbotocore\u001b[0m\u001b[2m==1.34.101\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mbotocore\u001b[0m\u001b[2m==1.34.142\u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1mdocker\u001b[0m\u001b[2m==6.1.3\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mdocker\u001b[0m\u001b[2m==7.1.0\u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1mexceptiongroup\u001b[0m\u001b[2m==1.2.2\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mexceptiongroup\u001b[0m\u001b[2m==1.2.1\u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1midna\u001b[0m\u001b[2m==3.6 (from file:///home/conda/feedstock_root/build_artifacts/idna_1701026962277/work)\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1midna\u001b[0m\u001b[2m==3.7\u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1mipython\u001b[0m\u001b[2m==8.26.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mipython\u001b[0m\u001b[2m==8.24.0\u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1mjsonschema\u001b[0m\u001b[2m==4.21.1 (from file:///home/conda/feedstock_root/build_artifacts/jsonschema-meta_1705707496704/work)\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mjsonschema\u001b[0m\u001b[2m==4.22.0\u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1mnumpy\u001b[0m\u001b[2m==1.22.4 (from file:///home/conda/feedstock_root/build_artifacts/numpy_1653325310407/work)\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mnumpy\u001b[0m\u001b[2m==1.26.4\u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1mpackaging\u001b[0m\u001b[2m==21.3\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mpackaging\u001b[0m\u001b[2m==24.0\u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1mpandas\u001b[0m\u001b[2m==2.2.1 (from file:///home/conda/feedstock_root/build_artifacts/pandas_1708708607448/work)\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mpandas\u001b[0m\u001b[2m==2.2.2\u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1mplatformdirs\u001b[0m\u001b[2m==4.2.0 (from file:///home/conda/feedstock_root/build_artifacts/platformdirs_1706713388748/work)\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mplatformdirs\u001b[0m\u001b[2m==4.2.2\u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1mprompt-toolkit\u001b[0m\u001b[2m==3.0.47\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mprompt-toolkit\u001b[0m\u001b[2m==3.0.45\u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1mpure-eval\u001b[0m\u001b[2m==0.2.3\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mpure-eval\u001b[0m\u001b[2m==0.2.2\u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1mpython-dateutil\u001b[0m\u001b[2m==2.9.0 (from file:///home/conda/feedstock_root/build_artifacts/python-dateutil_1709299778482/work)\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mpython-dateutil\u001b[0m\u001b[2m==2.9.0.post0\u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1mreferencing\u001b[0m\u001b[2m==0.34.0 (from file:///home/conda/feedstock_root/build_artifacts/referencing_1710763696991/work)\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mreferencing\u001b[0m\u001b[2m==0.35.1\u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1mrequests\u001b[0m\u001b[2m==2.31.0 (from file:///home/conda/feedstock_root/build_artifacts/requests_1684774241324/work)\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mrequests\u001b[0m\u001b[2m==2.32.2\u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1mrpds-py\u001b[0m\u001b[2m==0.18.0 (from file:///home/conda/feedstock_root/build_artifacts/rpds-py_1707922703488/work)\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mrpds-py\u001b[0m\u001b[2m==0.18.1\u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1msagemaker\u001b[0m\u001b[2m==2.219.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1msagemaker\u001b[0m\u001b[2m==2.225.0\u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1mtqdm\u001b[0m\u001b[2m==4.66.2 (from file:///home/conda/feedstock_root/build_artifacts/tqdm_1707598593068/work)\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mtqdm\u001b[0m\u001b[2m==4.66.4\u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1mtyping-extensions\u001b[0m\u001b[2m==4.12.2\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mtyping-extensions\u001b[0m\u001b[2m==4.12.0\u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1muv\u001b[0m\u001b[2m==0.2.30\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1muv\u001b[0m\u001b[2m==0.2.5\u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1mzipp\u001b[0m\u001b[2m==3.17.0 (from file:///home/conda/feedstock_root/build_artifacts/zipp_1695255097490/work)\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mzipp\u001b[0m\u001b[2m==3.19.0\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install uv && uv pip install -U ipywidgets\n",
    "!uv pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b2abe082-5ce0-4a26-bae8-68f9bff4104c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script>Jupyter.notebook.kernel.restart()</script>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# restart kernel\n",
    "from IPython.core.display import HTML\n",
    "\n",
    "HTML(\"<script>Jupyter.notebook.kernel.restart()</script>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "590ec770-d05e-474d-80da-d2f2bab63db2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# load rich extension\n",
    "%load_ext rich"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3a97edd4-8bba-4806-bce5-c559e23da05d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/ec2-user/.config/sagemaker/config.yaml\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "import time\n",
    "from getpass import getpass\n",
    "import boto3\n",
    "import sagemaker\n",
    "from rich import print\n",
    "from sagemaker.deserializers import JSONDeserializer\n",
    "from sagemaker.huggingface import get_huggingface_llm_image_uri\n",
    "from sagemaker.predictor import Predictor\n",
    "from sagemaker.serializers import JSONSerializer\n",
    "from sagemaker.deserializers import JSONDeserializer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c02d12b-2109-4f01-8da8-8972ba493398",
   "metadata": {},
   "source": [
    "## Initiate sagemaker session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "349a795d-df01-494e-b5fa-5f14971a1431",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">TGI DLC: \n",
       "<span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold; font-style: italic\">763104351884.dkr.ecr.us-east-1.amazonaws.com/huggingface-pytorch-tgi-inference:2.1.1-tgi2.0.0-gpu-py310-cu121-ubunt</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold; font-style: italic\">u22.04</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "TGI DLC: \n",
       "\u001b[1;3;32m763104351884.\u001b[0m\u001b[1;3;32mdkr.ecr.us-east-\u001b[0m\u001b[1;3;32m1.\u001b[0m\u001b[1;3;32mamazonaws.com/huggingface-pytorch-tgi-inferen\u001b[0m\u001b[1;3;32mce:2\u001b[0m\u001b[1;3;32m.\u001b[0m\u001b[1;3;32m1.1\u001b[0m\u001b[1;3;32m-tgi2.\u001b[0m\u001b[1;3;32m0.0\u001b[0m\u001b[1;3;32m-gpu-py310-cu121-ubunt\u001b[0m\n",
       "\u001b[1;3;32mu22.\u001b[0m\u001b[1;3;32m04\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Region: <span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">us-east-</span><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">1</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Region: \u001b[1;34mus-east-\u001b[0m\u001b[1;34m1\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Role: <span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">arn:aws:iam::</span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">057716757052</span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">:role/gen_ai_gsmoon</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Role: \u001b[1;31marn:aws:iam::\u001b[0m\u001b[1;31m057716757052\u001b[0m\u001b[1;31m:role/gen_ai_gsmoon\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sess = sagemaker.Session()\n",
    "role = sagemaker.get_execution_role()\n",
    "region = sess.boto_region_name\n",
    "\n",
    "boto_session = boto3.Session(region_name=region)\n",
    "\n",
    "sagemaker_client = sess.sagemaker_client\n",
    "sagemaker_runtime_client = sess.sagemaker_runtime_client\n",
    "cloudwatch_client = boto3.client(\"cloudwatch\", region_name=region)\n",
    "\n",
    "hf_model_id = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "\n",
    "# retrieve the llm image uri\n",
    "# tgi_dlc = f\"763104351884.dkr.ecr.{region}.amazonaws.com/huggingface-pytorch-tgi-inference:2.1-tgi2.0-gpu-py310-cu121-ubuntu22.04\"\n",
    "tgi_dlc = get_huggingface_llm_image_uri(\"huggingface\", version=\"2.0.0\")\n",
    "\n",
    "print(f\"TGI DLC: \\n[b i green]{tgi_dlc}[/b i green]\")\n",
    "print(f\"Region: [b blue]{region}[/b blue]\")\n",
    "print(f\"Role: [b red]{role}[/b red]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3d06b2c-dec7-4b42-af1a-423d39f211d6",
   "metadata": {},
   "source": [
    "## Create Endpoint\n",
    "\n",
    "1. Create `EndpointConfiguration`\n",
    "2. Create Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0cb8a39a-0f45-4e0e-a678-25c158a268c2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">prefix: llama3-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1722146573</span>-5d08\n",
       "</pre>\n"
      ],
      "text/plain": [
       "prefix: llama3-\u001b[1;36m1722146573\u001b[0m-5d08\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Endpoint config name: llama3-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1722146573</span>-5d08-endpoint-config\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Endpoint config name: llama3-\u001b[1;36m1722146573\u001b[0m-5d08-endpoint-config\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Initial instance count: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Initial instance count: \u001b[1;36m1\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Max instance count: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Max instance count: \u001b[1;36m2\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">{</span>\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'EndpointConfigArn'</span>: \n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'arn:aws:sagemaker:us-east-1:057716757052:endpoint-config/llama3-1722146573-5d08-endpoint-config'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'ResponseMetadata'</span>: <span style=\"font-weight: bold\">{</span>\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'RequestId'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'3869917a-91f4-47aa-9a27-9dff94633eb7'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'HTTPStatusCode'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">200</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'HTTPHeaders'</span>: <span style=\"font-weight: bold\">{</span>\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'x-amzn-requestid'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'3869917a-91f4-47aa-9a27-9dff94633eb7'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'content-type'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'application/x-amz-json-1.1'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'content-length'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'119'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'date'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'Sun, 28 Jul 2024 06:02:53 GMT'</span>\n",
       "        <span style=\"font-weight: bold\">}</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'RetryAttempts'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>\n",
       "    <span style=\"font-weight: bold\">}</span>\n",
       "<span style=\"font-weight: bold\">}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m{\u001b[0m\n",
       "    \u001b[32m'EndpointConfigArn'\u001b[0m: \n",
       "\u001b[32m'arn:aws:sagemaker:us-east-1:057716757052:endpoint-config/llama3-1722146573-5d08-endpoint-config'\u001b[0m,\n",
       "    \u001b[32m'ResponseMetadata'\u001b[0m: \u001b[1m{\u001b[0m\n",
       "        \u001b[32m'RequestId'\u001b[0m: \u001b[32m'3869917a-91f4-47aa-9a27-9dff94633eb7'\u001b[0m,\n",
       "        \u001b[32m'HTTPStatusCode'\u001b[0m: \u001b[1;36m200\u001b[0m,\n",
       "        \u001b[32m'HTTPHeaders'\u001b[0m: \u001b[1m{\u001b[0m\n",
       "            \u001b[32m'x-amzn-requestid'\u001b[0m: \u001b[32m'3869917a-91f4-47aa-9a27-9dff94633eb7'\u001b[0m,\n",
       "            \u001b[32m'content-type'\u001b[0m: \u001b[32m'application/x-amz-json-1.1'\u001b[0m,\n",
       "            \u001b[32m'content-length'\u001b[0m: \u001b[32m'119'\u001b[0m,\n",
       "            \u001b[32m'date'\u001b[0m: \u001b[32m'Sun, 28 Jul 2024 06:02:53 GMT'\u001b[0m\n",
       "        \u001b[1m}\u001b[0m,\n",
       "        \u001b[32m'RetryAttempts'\u001b[0m: \u001b[1;36m0\u001b[0m\n",
       "    \u001b[1m}\u001b[0m\n",
       "\u001b[1m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Set an unique endpoint config name\n",
    "prefix = sagemaker.utils.unique_name_from_base(\"llama3\")\n",
    "print(f\"prefix: {prefix}\")\n",
    "\n",
    "endpoint_config_name = f\"{prefix}-endpoint-config\"\n",
    "print(f\"Endpoint config name: {endpoint_config_name}\")\n",
    "\n",
    "# Set varient name and instance type for hosting\n",
    "variant_name = \"AllTraffic\"\n",
    "instance_type = \"ml.g5.2xlarge\"\n",
    "model_data_download_timeout_in_seconds = 3600\n",
    "container_startup_health_check_timeout_in_seconds = 3600\n",
    "\n",
    "initial_instance_count = 1\n",
    "max_instance_count = 2\n",
    "print(f\"Initial instance count: {initial_instance_count}\")\n",
    "print(f\"Max instance count: {max_instance_count}\")\n",
    "\n",
    "epc_response = sagemaker_client.create_endpoint_config(\n",
    "    EndpointConfigName=endpoint_config_name,\n",
    "    ExecutionRoleArn=role,\n",
    "    ProductionVariants=[\n",
    "        {\n",
    "            \"VariantName\": variant_name,\n",
    "            \"InstanceType\": instance_type,\n",
    "            \"InitialInstanceCount\": 1,\n",
    "            \"ModelDataDownloadTimeoutInSeconds\": model_data_download_timeout_in_seconds,\n",
    "            \"ContainerStartupHealthCheckTimeoutInSeconds\": container_startup_health_check_timeout_in_seconds,\n",
    "            \"ManagedInstanceScaling\": {\n",
    "                \"Status\": \"ENABLED\",\n",
    "                \"MinInstanceCount\": initial_instance_count,\n",
    "                \"MaxInstanceCount\": max_instance_count,\n",
    "            },\n",
    "            \"RoutingConfig\": {\"RoutingStrategy\": \"LEAST_OUTSTANDING_REQUESTS\"},\n",
    "        }\n",
    "    ],\n",
    ")\n",
    "print(epc_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "17259935-6a85-46af-89dd-b10e064a1c3b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Creating endpoint: <span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">llama3-</span><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">1722146573</span><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">-5d08-endpoint...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Creating endpoint: \u001b[1;34mllama3-\u001b[0m\u001b[1;34m1722146573\u001b[0m\u001b[1;34m-5d08-endpoint\u001b[0m\u001b[1;34m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----!"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "\u001b[1m{\u001b[0m\n",
       "    \u001b[32m'EndpointName'\u001b[0m: \u001b[32m'llama3-1722146573-5d08-endpoint'\u001b[0m,\n",
       "    \u001b[32m'EndpointArn'\u001b[0m: \u001b[32m'arn:aws:sagemaker:us-east-1:057716757052:endpoint/llama3-1722146573-5d08-endpoint'\u001b[0m,\n",
       "    \u001b[32m'EndpointConfigName'\u001b[0m: \u001b[32m'llama3-1722146573-5d08-endpoint-config'\u001b[0m,\n",
       "    \u001b[32m'ProductionVariants'\u001b[0m: \u001b[1m[\u001b[0m\n",
       "        \u001b[1m{\u001b[0m\n",
       "            \u001b[32m'VariantName'\u001b[0m: \u001b[32m'AllTraffic'\u001b[0m,\n",
       "            \u001b[32m'CurrentInstanceCount'\u001b[0m: \u001b[1;36m1\u001b[0m,\n",
       "            \u001b[32m'DesiredInstanceCount'\u001b[0m: \u001b[1;36m1\u001b[0m,\n",
       "            \u001b[32m'ManagedInstanceScaling'\u001b[0m: \u001b[1m{\u001b[0m\n",
       "                \u001b[32m'Status'\u001b[0m: \u001b[32m'ENABLED'\u001b[0m,\n",
       "                \u001b[32m'MinInstanceCount'\u001b[0m: \u001b[1;36m1\u001b[0m,\n",
       "                \u001b[32m'MaxInstanceCount'\u001b[0m: \u001b[1;36m2\u001b[0m\n",
       "            \u001b[1m}\u001b[0m,\n",
       "            \u001b[32m'RoutingConfig'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'RoutingStrategy'\u001b[0m: \u001b[32m'LEAST_OUTSTANDING_REQUESTS'\u001b[0m\u001b[1m}\u001b[0m\n",
       "        \u001b[1m}\u001b[0m\n",
       "    \u001b[1m]\u001b[0m,\n",
       "    \u001b[32m'EndpointStatus'\u001b[0m: \u001b[32m'InService'\u001b[0m,\n",
       "    \u001b[32m'CreationTime'\u001b[0m: \u001b[1;35mdatetime.datetime\u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m2024\u001b[0m, \u001b[1;36m7\u001b[0m, \u001b[1;36m28\u001b[0m, \u001b[1;36m6\u001b[0m, \u001b[1;36m3\u001b[0m, \u001b[1;36m19\u001b[0m, \u001b[1;36m975000\u001b[0m, \u001b[33mtzinfo\u001b[0m=\u001b[1;35mtzlocal\u001b[0m\u001b[1m(\u001b[0m\u001b[1m)\u001b[0m\u001b[1m)\u001b[0m,\n",
       "    \u001b[32m'LastModifiedTime'\u001b[0m: \u001b[1;35mdatetime.datetime\u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m2024\u001b[0m, \u001b[1;36m7\u001b[0m, \u001b[1;36m28\u001b[0m, \u001b[1;36m6\u001b[0m, \u001b[1;36m6\u001b[0m, \u001b[1;36m10\u001b[0m, \u001b[1;36m61000\u001b[0m, \u001b[33mtzinfo\u001b[0m=\u001b[1;35mtzlocal\u001b[0m\u001b[1m(\u001b[0m\u001b[1m)\u001b[0m\u001b[1m)\u001b[0m,\n",
       "    \u001b[32m'ResponseMetadata'\u001b[0m: \u001b[1m{\u001b[0m\n",
       "        \u001b[32m'RequestId'\u001b[0m: \u001b[32m'0efc7190-5016-4331-a664-366b08e37262'\u001b[0m,\n",
       "        \u001b[32m'HTTPStatusCode'\u001b[0m: \u001b[1;36m200\u001b[0m,\n",
       "        \u001b[32m'HTTPHeaders'\u001b[0m: \u001b[1m{\u001b[0m\n",
       "            \u001b[32m'x-amzn-requestid'\u001b[0m: \u001b[32m'0efc7190-5016-4331-a664-366b08e37262'\u001b[0m,\n",
       "            \u001b[32m'content-type'\u001b[0m: \u001b[32m'application/x-amz-json-1.1'\u001b[0m,\n",
       "            \u001b[32m'content-length'\u001b[0m: \u001b[32m'562'\u001b[0m,\n",
       "            \u001b[32m'date'\u001b[0m: \u001b[32m'Sun, 28 Jul 2024 06:06:20 GMT'\u001b[0m\n",
       "        \u001b[1m}\u001b[0m,\n",
       "        \u001b[32m'RetryAttempts'\u001b[0m: \u001b[1;36m0\u001b[0m\n",
       "    \u001b[1m}\u001b[0m\n",
       "\u001b[1m}\u001b[0m"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set a unique endpoint name\n",
    "endpoint_name = f\"{prefix}-endpoint\"\n",
    "\n",
    "ep_response = sagemaker_client.create_endpoint(\n",
    "    EndpointName=endpoint_name,\n",
    "    EndpointConfigName=endpoint_config_name,\n",
    ")\n",
    "# print(ep_response)\n",
    "print(f\"Creating endpoint: [b blue]{endpoint_name}...\")\n",
    "sess.wait_for_endpoint(endpoint_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "043f7d75-de75-4687-a2e2-ab4aa7168ef6",
   "metadata": {},
   "source": [
    "## Deploy model\n",
    "\n",
    "Create and deploy model using Amazon SageMaker HuggingFace TGI DLC\n",
    "\n",
    "<https://sagemaker.readthedocs.io/en/stable/api/inference/model.html#sagemaker.model.Model.deploy>\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>NOTE:</b> Remember to copy your Hugging Face Access Token from <a href=\"https://hf.co/\">https://hf.co/</a> before running the below cell.<br/><br/>\n",
    "Refer <a href=\"https://huggingface.co/docs/hub/security-tokens\">here</a> to learn about creating HF tokens.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74991ddd-3b2c-4f07-b35b-55f5d8c19ada",
   "metadata": {},
   "source": [
    "## Configure container and environment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8b89da4d-9ce7-4e5b-a02a-3f2c690cd26d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">llm image uri: \n",
       "<span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">763104351884.</span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">dkr.ecr.us-east-</span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">1.</span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">amazonaws.com/huggingface-pytorch-tgi-inference:2.</span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">1.1</span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">-tgi2.</span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">0.0</span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">-gpu-py310-cu121-ubunt</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">u22.</span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">04</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "llm image uri: \n",
       "\u001b[1;32m763104351884.\u001b[0m\u001b[1;32mdkr.ecr.us-east-\u001b[0m\u001b[1;32m1.\u001b[0m\u001b[1;32mamazonaws.com/huggingface-pytorch-tgi-inferen\u001b[0m\u001b[1;32mce:2\u001b[0m\u001b[1;32m.\u001b[0m\u001b[1;32m1.1\u001b[0m\u001b[1;32m-tgi2.\u001b[0m\u001b[1;32m0.0\u001b[0m\u001b[1;32m-gpu-py310-cu121-ubunt\u001b[0m\n",
       "\u001b[1;32mu22.\u001b[0m\u001b[1;32m04\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Creating model: <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">sm-model-llama3...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Creating model: \u001b[1;32msm-model-llama3\u001b[0m\u001b[1;32m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">{</span>\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'ModelArn'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'arn:aws:sagemaker:us-east-1:057716757052:model/sm-model-llama3'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'ResponseMetadata'</span>: <span style=\"font-weight: bold\">{</span>\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'RequestId'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'38bc404b-df57-4db3-9d10-0f37a404fa34'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'HTTPStatusCode'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">200</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'HTTPHeaders'</span>: <span style=\"font-weight: bold\">{</span>\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'x-amzn-requestid'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'38bc404b-df57-4db3-9d10-0f37a404fa34'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'content-type'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'application/x-amz-json-1.1'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'content-length'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'77'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'date'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'Sun, 28 Jul 2024 06:10:30 GMT'</span>\n",
       "        <span style=\"font-weight: bold\">}</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'RetryAttempts'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>\n",
       "    <span style=\"font-weight: bold\">}</span>\n",
       "<span style=\"font-weight: bold\">}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m{\u001b[0m\n",
       "    \u001b[32m'ModelArn'\u001b[0m: \u001b[32m'arn:aws:sagemaker:us-east-1:057716757052:model/sm-model-llama3'\u001b[0m,\n",
       "    \u001b[32m'ResponseMetadata'\u001b[0m: \u001b[1m{\u001b[0m\n",
       "        \u001b[32m'RequestId'\u001b[0m: \u001b[32m'38bc404b-df57-4db3-9d10-0f37a404fa34'\u001b[0m,\n",
       "        \u001b[32m'HTTPStatusCode'\u001b[0m: \u001b[1;36m200\u001b[0m,\n",
       "        \u001b[32m'HTTPHeaders'\u001b[0m: \u001b[1m{\u001b[0m\n",
       "            \u001b[32m'x-amzn-requestid'\u001b[0m: \u001b[32m'38bc404b-df57-4db3-9d10-0f37a404fa34'\u001b[0m,\n",
       "            \u001b[32m'content-type'\u001b[0m: \u001b[32m'application/x-amz-json-1.1'\u001b[0m,\n",
       "            \u001b[32m'content-length'\u001b[0m: \u001b[32m'77'\u001b[0m,\n",
       "            \u001b[32m'date'\u001b[0m: \u001b[32m'Sun, 28 Jul 2024 06:10:30 GMT'\u001b[0m\n",
       "        \u001b[1m}\u001b[0m,\n",
       "        \u001b[32m'RetryAttempts'\u001b[0m: \u001b[1;36m0\u001b[0m\n",
       "    \u001b[1m}\u001b[0m\n",
       "\u001b[1m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# print ecr image uri\n",
    "print(f\"llm image uri: [b green]{tgi_dlc}\")\n",
    "\n",
    "HF_TOKEN = os.getenv(\"HUGGING_FACE_HUB_TOKEN\") or getpass(\"Enter HUGGINGFACE Access Token: \")\n",
    "\n",
    "\n",
    "llama3model = {\n",
    "    \"Image\": tgi_dlc,\n",
    "    \"Environment\": {\n",
    "        \"HF_MODEL_ID\": \"meta-llama/Meta-Llama-3-8B-Instruct\",  # model_id from hf.co/models\n",
    "        \"SM_NUM_GPUS\": \"1\",  # Number of GPU used per replica\n",
    "        \"MAX_INPUT_LENGTH\": \"2048\",  # Max length of input text\n",
    "        \"MAX_TOTAL_TOKENS\": \"4096\",  # Max length of the generation (including input text)\n",
    "        \"MAX_BATCH_TOTAL_TOKENS\": \"8192\",  # Limits the number of tokens that can be processed in parallel during the generation\n",
    "        \"MESSAGES_API_ENABLED\": \"true\",  # Enable the messages API\n",
    "        \"HUGGING_FACE_HUB_TOKEN\": HF_TOKEN,\n",
    "    },\n",
    "}\n",
    "\n",
    "# create Model\n",
    "deployment_name = \"sm\"\n",
    "model_name = f\"{deployment_name}-model-llama3\"\n",
    "\n",
    "print(f\"Creating model: [b green]{model_name}...\")\n",
    "model_response = sagemaker_client.create_model(\n",
    "    ModelName=model_name,\n",
    "    ExecutionRoleArn=role,\n",
    "    Containers=[llama3model],\n",
    ")\n",
    "\n",
    "print(model_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7a93cea6-703e-4eb9-8b4d-d92b60597a7a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">InferenceComponent <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">llama3-</span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">1722146573</span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">-5d08-IC-llama3b...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "InferenceComponent \u001b[1;35mllama3-\u001b[0m\u001b[1;35m1722146573\u001b[0m\u001b[1;35m-5d08-IC-llama3b\u001b[0m\u001b[1;35m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Creating\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Creating\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Creating\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Creating\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Creating\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Creating\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Creating\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Creating\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Creating\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Creating\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Creating\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Creating\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Creating\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Creating\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Creating\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Creating\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Creating\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Creating\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Creating\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Creating\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Creating\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Creating\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Creating\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Creating\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">InService\n",
       "</pre>\n"
      ],
      "text/plain": [
       "InService\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Deploy model to Amazon SageMaker Inference Component\n",
    "inference_component_name_llama3b = f\"{prefix}-IC-llama3b\"\n",
    "variant_name = \"AllTraffic\"\n",
    "\n",
    "ic_response = sagemaker_client.create_inference_component(\n",
    "    InferenceComponentName=inference_component_name_llama3b,\n",
    "    EndpointName=endpoint_name,\n",
    "    VariantName=variant_name,\n",
    "    Specification={\n",
    "        \"ModelName\": f\"{deployment_name}-model-llama3\",\n",
    "        \"ComputeResourceRequirements\": {\n",
    "            \"NumberOfAcceleratorDevicesRequired\": 1,\n",
    "            \"NumberOfCpuCoresRequired\": 1,\n",
    "            \"MinMemoryRequiredInMb\": 1024,\n",
    "        },\n",
    "    },\n",
    "    RuntimeConfig={\"CopyCount\": 1},\n",
    ")\n",
    "\n",
    "# print(ic_response)\n",
    "\n",
    "# Wait for IC to come InService\n",
    "print(f\"InferenceComponent [b magenta]{inference_component_name_llama3b}...\")\n",
    "while True:\n",
    "    desc = sagemaker_client.describe_inference_component(\n",
    "        InferenceComponentName=inference_component_name_llama3b\n",
    "    )\n",
    "    status = desc[\"InferenceComponentStatus\"]\n",
    "    print(status)\n",
    "    sys.stdout.flush()\n",
    "    if status in [\"InService\", \"Failed\"]:\n",
    "        break\n",
    "    time.sleep(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83e1af5c-e713-4cf8-bc23-1c96f1e61327",
   "metadata": {},
   "source": [
    "## Inference\n",
    "\n",
    "Invoke and test endpoint using messages API. Refer to HF [Messages API](https://huggingface.co/docs/text-generation-inference/messages_api) for more info."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "039223e9-0981-4692-a302-3153b817a1fc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001b[1m(\u001b[0m\u001b[32m'llama3-1722146573-5d08-endpoint'\u001b[0m, \u001b[32m'llama3-1722146573-5d08-IC-llama3b'\u001b[0m\u001b[1m)\u001b[0m"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "endpoint_name, inference_component_name_llama3b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "59fa5f76-5498-47a1-a443-5483c3077172",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create predictor object\n",
    "predictor = Predictor(\n",
    "    endpoint_name=endpoint_name,\n",
    "    sagemaker_session=sess,\n",
    "    component_name=inference_component_name_llama3b,\n",
    "    serializer=JSONSerializer(),\n",
    "    deserializer=JSONDeserializer(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b6d9ecc2-fffe-4ff1-b78b-1222fe6d32de",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Deep learning is a subfield of machine learning that is inspired by the structure and function of the human brain. \n",
       "It involves the use of artificial neural networks, which are composed of multiple layers of interconnected nodes or\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">\"neurons,\"</span> to analyze and interpret data.\n",
       "\n",
       "In traditional machine learning, algorithms are designed to learn from data by identifying patterns and \n",
       "relationships between inputs and outputs. However, these algorithms are typically limited to learning from shallow \n",
       "representations of data, such as features or attributes.\n",
       "\n",
       "Deep learning, on the other hand, allows machines to learn from complex, hierarchical representations of data, such\n",
       "as images, speech, and text. This is achieved through the use of multiple layers of neural networks, each of which \n",
       "is trained to recognize and extract more abstract features from the data.\n",
       "\n",
       "The key characteristics of deep learning are:\n",
       "\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>. Hierarchical representations: Deep learning models learn to represent data in a hierarchical manner, with each \n",
       "layer building on the previous one to create a more abstract and complex representation of the data.\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>. Multiple layers: Deep learning models typically consist of multiple layers of neural networks, each of which is \n",
       "trained to perform a specific task, such as feature extraction or classification.\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>. Large amounts of data: Deep learning models require large amounts of data to train, as they need to learn from \n",
       "the patterns and relationships present in the data.\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span>. Computational power: Deep learning models require significant computational power to train, as they involve \n",
       "complex mathematical calculations and iterative optimization processes.\n",
       "\n",
       "Some of the key applications of deep learning include:\n",
       "\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>. Computer vision: Deep learning has been used to develop systems that can recognize and interpret images and \n",
       "videos, such as facial recognition, object detection, and image classification.\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>. Natural language processing: Deep learning has been used to develop systems that can understand and generate \n",
       "human language, such as speech recognition, language translation, and text summarization.\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>. Speech recognition: Deep learning has been used to develop systems that can recognize and transcribe spoken \n",
       "language, such as voice assistants and speech-to-text systems.\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span>. Robotics: Deep learning has been used to develop systems that can control and interact with robots, such as \n",
       "autonomous vehicles and robotic arms.\n",
       "\n",
       "Some of the key benefits of deep learning include:\n",
       "\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>. Improved accuracy: Deep learning models can achieve high levels of accuracy on complex tasks, such as image \n",
       "recognition and speech recognition.\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>. Flexibility: Deep learning models can be trained on a wide range of data types and can be adapted to new tasks \n",
       "and domains.\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>. Scalability: Deep learning models can be trained on large datasets and can be\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Deep learning is a subfield of machine learning that is inspired by the structure and function of the human brain. \n",
       "It involves the use of artificial neural networks, which are composed of multiple layers of interconnected nodes or\n",
       "\u001b[32m\"neurons,\"\u001b[0m to analyze and interpret data.\n",
       "\n",
       "In traditional machine learning, algorithms are designed to learn from data by identifying patterns and \n",
       "relationships between inputs and outputs. However, these algorithms are typically limited to learning from shallow \n",
       "representations of data, such as features or attributes.\n",
       "\n",
       "Deep learning, on the other hand, allows machines to learn from complex, hierarchical representations of data, such\n",
       "as images, speech, and text. This is achieved through the use of multiple layers of neural networks, each of which \n",
       "is trained to recognize and extract more abstract features from the data.\n",
       "\n",
       "The key characteristics of deep learning are:\n",
       "\n",
       "\u001b[1;36m1\u001b[0m. Hierarchical representations: Deep learning models learn to represent data in a hierarchical manner, with each \n",
       "layer building on the previous one to create a more abstract and complex representation of the data.\n",
       "\u001b[1;36m2\u001b[0m. Multiple layers: Deep learning models typically consist of multiple layers of neural networks, each of which is \n",
       "trained to perform a specific task, such as feature extraction or classification.\n",
       "\u001b[1;36m3\u001b[0m. Large amounts of data: Deep learning models require large amounts of data to train, as they need to learn from \n",
       "the patterns and relationships present in the data.\n",
       "\u001b[1;36m4\u001b[0m. Computational power: Deep learning models require significant computational power to train, as they involve \n",
       "complex mathematical calculations and iterative optimization processes.\n",
       "\n",
       "Some of the key applications of deep learning include:\n",
       "\n",
       "\u001b[1;36m1\u001b[0m. Computer vision: Deep learning has been used to develop systems that can recognize and interpret images and \n",
       "videos, such as facial recognition, object detection, and image classification.\n",
       "\u001b[1;36m2\u001b[0m. Natural language processing: Deep learning has been used to develop systems that can understand and generate \n",
       "human language, such as speech recognition, language translation, and text summarization.\n",
       "\u001b[1;36m3\u001b[0m. Speech recognition: Deep learning has been used to develop systems that can recognize and transcribe spoken \n",
       "language, such as voice assistants and speech-to-text systems.\n",
       "\u001b[1;36m4\u001b[0m. Robotics: Deep learning has been used to develop systems that can control and interact with robots, such as \n",
       "autonomous vehicles and robotic arms.\n",
       "\n",
       "Some of the key benefits of deep learning include:\n",
       "\n",
       "\u001b[1;36m1\u001b[0m. Improved accuracy: Deep learning models can achieve high levels of accuracy on complex tasks, such as image \n",
       "recognition and speech recognition.\n",
       "\u001b[1;36m2\u001b[0m. Flexibility: Deep learning models can be trained on a wide range of data types and can be adapted to new tasks \n",
       "and domains.\n",
       "\u001b[1;36m3\u001b[0m. Scalability: Deep learning models can be trained on large datasets and can be\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Prompt to generate\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"What is deep learning?\"},\n",
    "]\n",
    "\n",
    "# Generation arguments\n",
    "parameters = {\n",
    "    \"model\": hf_model_id,  # model id is required\n",
    "    \"top_p\": 0.6,\n",
    "    \"temperature\": 0.9,\n",
    "    \"max_tokens\": 512,\n",
    "    \"stop\": [\"<|eot_id|>\"],\n",
    "}\n",
    "\n",
    "chat = predictor.predict({\"messages\": messages, **parameters})\n",
    "\n",
    "# Unpack and print response\n",
    "print(chat[\"choices\"][0][\"message\"][\"content\"].strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29f7a5ab-0264-4b12-8243-b4aa649335b7",
   "metadata": {},
   "source": [
    "## Apply Autoscaling policies to the endpoint\n",
    "\n",
    "Apply Application Autoscaling Policy to endpoint\n",
    "\n",
    "1. Register Scalable Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1bbf762f-beec-42ed-9ff8-5b06f76269ab",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Resource ID: <span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">inference-component/llama3-</span><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">1722146573</span><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">-5d08-IC-llama3b</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Resource ID: \u001b[1;34minference-component/llama3-\u001b[0m\u001b[1;34m1722146573\u001b[0m\u001b[1;34m-5d08-IC-llama3b\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Scalable_target_arn:\n",
       "<span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">arn:aws:application-autoscaling:us-east-1:057716757052:scalable-target/056mffd3d71426ed4b47997cf648b07f9600</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Scalable_target_arn:\n",
       "\u001b[1;32marn:aws:application-autoscaling:us-east-\u001b[0m\u001b[1;32m1:0577\u001b[0m\u001b[1;32m16757052:scalable-target/056mffd3d71426ed4b47997cf648b07f9600\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "as_min_capacity = 1\n",
    "as_max_capacity = 2\n",
    "\n",
    "resource_id = f\"inference-component/{inference_component_name_llama3b}\"\n",
    "\n",
    "autoscaling_client = boto3.client(\"application-autoscaling\", region_name=region)\n",
    "\n",
    "# Register scalable target\n",
    "scalable_target = autoscaling_client.register_scalable_target(\n",
    "    ServiceNamespace=\"sagemaker\",\n",
    "    ResourceId=resource_id,\n",
    "    ScalableDimension=\"sagemaker:inference-component:DesiredCopyCount\",\n",
    "    MinCapacity=as_min_capacity,\n",
    "    MaxCapacity=as_max_capacity,  # Replace with your desired maximum instances\n",
    ")\n",
    "\n",
    "scalable_target_arn = scalable_target[\"ScalableTargetARN\"]\n",
    "print(f\"Resource ID: [b blue]{resource_id}\")\n",
    "print(f\"Scalable_target_arn:\\n[b green]{scalable_target_arn}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0af2e234-d1c7-4575-b943-5291c70c326d",
   "metadata": {},
   "source": [
    "## Use the latest high-resolution Metrics to trigger auto-scaling\n",
    "\n",
    "- New feature introduces a new <span style='color:green'><b>PredefinedMetricType</b></span> for scaling policy configuration i.e. <span style='color:green'><b>SageMakerVariantConcurrentRequestsPerModelHighResolution</b></span> to trigger scaling actions.\n",
    "- Creating a scaling policy with this metric type will create cloudwatch alarms that track a new metric called <span style='color:green'><b>ConcurrentRequestsPerModel</b></span>.\n",
    "- These high-resolution metrics are published at sub-minute intervals (10s intervals to CW + any additional jitter + delays)\n",
    "- We should observe significant improvement in scale out times with this new metric\n",
    "\n",
    "\n",
    "### Steps to create Application autoscaling policy\n",
    "\n",
    "- Create scaling policy\n",
    "  - Set `PolicyType` to `TargetTrackingScaling`\n",
    "  - Set `TargetValue` to `5.0`. i.e., Scaling triggers when endpoint receives 5 `ConcurrentRequestsPerModel`\n",
    "  - Set `PredefinedMetricType` to `SageMakerVariantConcurrentRequestsPerModelHighResolution`\n",
    "  - Set `ScaleInCoolDown` and `ScaleOutCoolDown` values to `300` seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "44cc8c60-37cd-4852-a03d-e08149ccad17",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Policy ARN: \n",
       "<span style=\"color: #000080; text-decoration-color: #000080; font-style: italic\">arn:aws:autoscaling:us-east-</span><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold; font-style: italic\">1:0577</span><span style=\"color: #000080; text-decoration-color: #000080; font-style: italic\">16757052:scalingPolicy:</span><span style=\"color: #000080; text-decoration-color: #000080; font-style: italic\">ffd3d714-26ed-4b47-997c-f648b07f9600</span><span style=\"color: #000080; text-decoration-color: #000080; font-style: italic\">:resource/sagemaker/in</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080; font-style: italic\">ference-component/llama3-</span><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold; font-style: italic\">1722146573</span><span style=\"color: #000080; text-decoration-color: #000080; font-style: italic\">-5d08-IC-llama3b:policyName/SageMakerICScalingPolicy</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Policy ARN: \n",
       "\u001b[3;34marn:aws:autoscaling:us-east-\u001b[0m\u001b[1;3;34m1:0577\u001b[0m\u001b[3;34m16757052:scalingPolicy:\u001b[0m\u001b[3;34mffd3d714-26ed-4b47-997c-f648b07f9600\u001b[0m\u001b[3;34m:resource/sagemaker/in\u001b[0m\n",
       "\u001b[3;34mference-component/llama3-\u001b[0m\u001b[1;3;34m1722146573\u001b[0m\u001b[3;34m-5d08-IC-llama3b:policyName/SageMakerICScalingPolicy\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Alarm Name:</span> \n",
       "<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">TargetTracking-inference-component/llama3-</span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">1722146573</span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">-5d08-IC-llama3b-AlarmHigh-5a1667aa-0a72-4ffd-9dee-9ba29af885ef</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mAlarm Name:\u001b[0m \n",
       "\u001b[1;35mTargetTracking-inference-component/llama3-\u001b[0m\u001b[1;35m1722146573\u001b[0m\u001b[1;35m-5d08-IC-llama3b-AlarmHigh-\u001b[0m\u001b[1;35m5a1667aa-0a72-4ffd-9dee-9ba29af885ef\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">=============================================\n",
       "</pre>\n"
      ],
      "text/plain": [
       "=============================================\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Alarm Name:</span> \n",
       "<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">TargetTracking-inference-component/llama3-</span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">1722146573</span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">-5d08-IC-llama3b-AlarmLow-23ed4150-b5aa-4940-9b66-a5812adf9673</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mAlarm Name:\u001b[0m \n",
       "\u001b[1;35mTargetTracking-inference-component/llama3-\u001b[0m\u001b[1;35m1722146573\u001b[0m\u001b[1;35m-5d08-IC-llama3b-AlarmLow-\u001b[0m\u001b[1;35m23ed4150-b5aa-4940-9b66-a5812adf9673\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">=============================================\n",
       "</pre>\n"
      ],
      "text/plain": [
       "=============================================\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create Target Tracking Scaling Policy\n",
    "target_tracking_policy_response = autoscaling_client.put_scaling_policy(\n",
    "    PolicyName=\"SageMakerICScalingPolicy\",\n",
    "    ServiceNamespace=\"sagemaker\",\n",
    "    ResourceId=resource_id,\n",
    "    ScalableDimension=\"sagemaker:inference-component:DesiredCopyCount\",\n",
    "    PolicyType=\"TargetTrackingScaling\",\n",
    "    TargetTrackingScalingPolicyConfiguration={\n",
    "        \"TargetValue\": 5.0,  # Scaling triggers when endpoint receives 5 ConcurrentRequestsPerModel\n",
    "        \"PredefinedMetricSpecification\": {\n",
    "            \"PredefinedMetricType\": \"SageMakerInferenceComponentConcurrentRequestsPerCopyHighResolution\"\n",
    "        },\n",
    "        \"ScaleInCooldown\": 300,  # Cooldown period after scale-in activity\n",
    "        \"ScaleOutCooldown\": 300,  # Cooldown period after scale-out activity\n",
    "    },\n",
    ")\n",
    "\n",
    "# print(target_tracking_policy_response)\n",
    "print(f\"Policy ARN: [i blue]{target_tracking_policy_response['PolicyARN']}\")\n",
    "\n",
    "# print Cloudwatch Alarms\n",
    "alarms = target_tracking_policy_response[\"Alarms\"]\n",
    "\n",
    "for alarm in alarms:\n",
    "    print(f\"[b]Alarm Name:[/b] [b magenta]{alarm['AlarmName']}\")\n",
    "    # print(f\"[b]Alarm ARN:[/b] [i green]{alarm['AlarmARN']}[/i green]\")\n",
    "    print(\"===\" * 15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02a2d5b0-dc4b-40e3-8ada-ceddecfdac1a",
   "metadata": {},
   "source": [
    "## Cleanup\n",
    "\n",
    "- Deregister scalable target. This automatically deletes associated cloudwatch alarms.\n",
    "- Delete model\n",
    "- Delete endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7aacabc3-2b60-41c6-b903-9de5e31fc8e7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Scalable target for <span style=\"font-weight: bold\">inference-component/llama3-</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1722146573</span><span style=\"font-weight: bold\">-5d08-IC-llama3b</span> not found!.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Scalable target for \u001b[1minference-component/llama3-\u001b[0m\u001b[1;36m1722146573\u001b[0m\u001b[1m-5d08-IC-llama3b\u001b[0m not found!.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">------------------------------\n",
       "</pre>\n"
      ],
      "text/plain": [
       "------------------------------\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Deleting inference components: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">llama3-</span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">1722146573</span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">-5d08-IC-llama3b ✅</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Deleting inference components: \u001b[1;35mllama3-\u001b[0m\u001b[1;35m1722146573\u001b[0m\u001b[1;35m-5d08-IC-llama3b ✅\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Deleting model: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">sm-model-llama3 ✅</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Deleting model: \u001b[1;35msm-model-llama3 ✅\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Deleting endpoint: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">llama3-</span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">1722146573</span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">-5d08-endpoint ✅</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Deleting endpoint: \u001b[1;35mllama3-\u001b[0m\u001b[1;35m1722146573\u001b[0m\u001b[1;35m-5d08-endpoint ✅\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">------------------------------\n",
       "</pre>\n"
      ],
      "text/plain": [
       "------------------------------\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Done\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Done\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "try:\n",
    "    # Deregister the scalable target for AAS\n",
    "    autoscaling_client.deregister_scalable_target(\n",
    "        ServiceNamespace=\"sagemaker\",\n",
    "        ResourceId=resource_id,\n",
    "        ScalableDimension=\"sagemaker:variant:DesiredInstanceCount\",\n",
    "    )\n",
    "    print(f\"Scalable target for [b]{resource_id}[/b] deregistered. ✅\")\n",
    "except autoscaling_client.exceptions.ObjectNotFoundException:\n",
    "    print(f\"Scalable target for [b]{resource_id}[/b] not found!.\")\n",
    "\n",
    "print(\"---\" * 10)\n",
    "\n",
    "try:\n",
    "    print(f\"Deleting inference components: [b magenta]{inference_component_name_llama3b} ✅\")\n",
    "    # Delete inference component\n",
    "    sagemaker_client.delete_inference_component(\n",
    "        InferenceComponentName=inference_component_name_llama3b\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(f\"{e}\")\n",
    "\n",
    "\n",
    "try:\n",
    "    print(f\"Deleting model: [b magenta]{deployment_name}-model-llama3 ✅\")\n",
    "    predictor.delete_model()\n",
    "except Exception as e:\n",
    "    print(f\"{e}\")\n",
    "\n",
    "\n",
    "try:\n",
    "    print(f\"Deleting endpoint: [b magenta]{predictor.endpoint_name} ✅\")\n",
    "    predictor.delete_endpoint()\n",
    "except Exception as e:\n",
    "    print(f\"{e}\")\n",
    "\n",
    "print(\"---\" * 10)\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fdee8f2",
   "metadata": {},
   "source": [
    "## Notebook CI Test Results\n",
    "\n",
    "This notebook was tested in multiple regions. The test results are as follows, except for us-west-2 which is shown at the top of the notebook.\n",
    "\n",
    "\n",
    "![This us-east-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/us-east-1/inference|generativeai|huggingfacetgi|meta-llama|llama3-8b|faster-autoscaling|realtime-endpoints|FasterAutoscaling-IC-Llama3-8B-AppAutoScaling.ipynb)\n",
    "\n",
    "![This us-east-2 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/us-east-2/inference|generativeai|huggingfacetgi|meta-llama|llama3-8b|faster-autoscaling|realtime-endpoints|FasterAutoscaling-IC-Llama3-8B-AppAutoScaling.ipynb)\n",
    "\n",
    "![This us-west-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/us-west-1/inference|generativeai|huggingfacetgi|meta-llama|llama3-8b|faster-autoscaling|realtime-endpoints|FasterAutoscaling-IC-Llama3-8B-AppAutoScaling.ipynb)\n",
    "\n",
    "![This ca-central-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/ca-central-1/inference|generativeai|huggingfacetgi|meta-llama|llama3-8b|faster-autoscaling|realtime-endpoints|FasterAutoscaling-IC-Llama3-8B-AppAutoScaling.ipynb)\n",
    "\n",
    "![This sa-east-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/sa-east-1/inference|generativeai|huggingfacetgi|meta-llama|llama3-8b|faster-autoscaling|realtime-endpoints|FasterAutoscaling-IC-Llama3-8B-AppAutoScaling.ipynb)\n",
    "\n",
    "![This eu-west-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/eu-west-1/inference|generativeai|huggingfacetgi|meta-llama|llama3-8b|faster-autoscaling|realtime-endpoints|FasterAutoscaling-IC-Llama3-8B-AppAutoScaling.ipynb)\n",
    "\n",
    "![This eu-west-2 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/eu-west-2/inference|generativeai|huggingfacetgi|meta-llama|llama3-8b|faster-autoscaling|realtime-endpoints|FasterAutoscaling-IC-Llama3-8B-AppAutoScaling.ipynb)\n",
    "\n",
    "![This eu-west-3 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/eu-west-3/inference|generativeai|huggingfacetgi|meta-llama|llama3-8b|faster-autoscaling|realtime-endpoints|FasterAutoscaling-IC-Llama3-8B-AppAutoScaling.ipynb)\n",
    "\n",
    "![This eu-central-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/eu-central-1/inference|generativeai|huggingfacetgi|meta-llama|llama3-8b|faster-autoscaling|realtime-endpoints|FasterAutoscaling-IC-Llama3-8B-AppAutoScaling.ipynb)\n",
    "\n",
    "![This eu-north-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/eu-north-1/inference|generativeai|huggingfacetgi|meta-llama|llama3-8b|faster-autoscaling|realtime-endpoints|FasterAutoscaling-IC-Llama3-8B-AppAutoScaling.ipynb)\n",
    "\n",
    "![This ap-southeast-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/ap-southeast-1/inference|generativeai|huggingfacetgi|meta-llama|llama3-8b|faster-autoscaling|realtime-endpoints|FasterAutoscaling-IC-Llama3-8B-AppAutoScaling.ipynb)\n",
    "\n",
    "![This ap-southeast-2 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/ap-southeast-2/inference|generativeai|huggingfacetgi|meta-llama|llama3-8b|faster-autoscaling|realtime-endpoints|FasterAutoscaling-IC-Llama3-8B-AppAutoScaling.ipynb)\n",
    "\n",
    "![This ap-northeast-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/ap-northeast-1/inference|generativeai|huggingfacetgi|meta-llama|llama3-8b|faster-autoscaling|realtime-endpoints|FasterAutoscaling-IC-Llama3-8B-AppAutoScaling.ipynb)\n",
    "\n",
    "![This ap-northeast-2 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/ap-northeast-2/inference|generativeai|huggingfacetgi|meta-llama|llama3-8b|faster-autoscaling|realtime-endpoints|FasterAutoscaling-IC-Llama3-8B-AppAutoScaling.ipynb)\n",
    "\n",
    "![This ap-south-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/ap-south-1/inference|generativeai|huggingfacetgi|meta-llama|llama3-8b|faster-autoscaling|realtime-endpoints|FasterAutoscaling-IC-Llama3-8B-AppAutoScaling.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
