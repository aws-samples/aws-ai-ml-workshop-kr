{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SageMaker Inference Component 모델 배포 및 추론\n",
    "\n",
    "아래 모델 배포는 아래의 인스턴스에서 테스트 완료 되었습니다.\n",
    "- ml.g5.xlarge (SageMaker On-Demand Endpoint 기준으로 us-east-1 에서 $1.4084) 입니다. 참조: [Pricing Link](https://aws.amazon.com/sagemaker/pricing/)\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 환경 구성"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 상위 폴더의 Python 경로 추가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python path: /home/ec2-user/SageMaker/aws-ai-ml-workshop-kr/genai/aws-gen-ai-kr/30_fine_tune/03-fine-tune-llama3 is added\n",
      "sys.path:  ['/home/ec2-user/SageMaker/aws-ai-ml-workshop-kr/genai/aws-gen-ai-kr/30_fine_tune/03-fine-tune-llama3/notebook/02-naver-news-llama3-inference', '/home/ec2-user/SageMaker/.cs/conda/envs/llama3_puy310/lib/python310.zip', '/home/ec2-user/SageMaker/.cs/conda/envs/llama3_puy310/lib/python3.10', '/home/ec2-user/SageMaker/.cs/conda/envs/llama3_puy310/lib/python3.10/lib-dynload', '', '/home/ec2-user/SageMaker/.cs/conda/envs/llama3_puy310/lib/python3.10/site-packages', '/home/ec2-user/SageMaker/aws-ai-ml-workshop-kr/genai/aws-gen-ai-kr/30_fine_tune/03-fine-tune-llama3']\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys, os\n",
    "\n",
    "def add_python_path(module_path):\n",
    "    if os.path.abspath(module_path) not in sys.path:\n",
    "        sys.path.append(os.path.abspath(module_path))\n",
    "        print(f\"python path: {os.path.abspath(module_path)} is added\")\n",
    "    else:\n",
    "        print(f\"python path: {os.path.abspath(module_path)} already exists\")\n",
    "    print(\"sys.path: \", sys.path)\n",
    "\n",
    "module_path = \"../..\"\n",
    "add_python_path(module_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 이전 노트북에서 훈련된 모델 경로 가져오기\n",
    "- ../01-naver-news-fsdp-QLoRA/03-SageMaker-Training.ipynb 가 실행 되었으면, model_s3_path 로 세팅\n",
    "- ../01-naver-news-fsdp-QLoRA/03-1-Option-SageMaker-Training.ipynb 가 실행 되었으면, optimized_model_s3_path 로 세팅"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimized_model_s3_path: \n",
      " {'S3DataSource': {'S3Uri': 's3://sagemaker-us-east-1-057716757052/llama3-8b-naver-news-2024-07-29-07-13-1-2024-07-29-07-13-12-264/output/model/', 'S3DataType': 'S3Prefix', 'CompressionType': 'None'}}\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    %store -r optimized_model_s3_path\n",
    "    optimized_model_s3_path\n",
    "    print(\"optimized_model_s3_path: \\n\", optimized_model_s3_path)\n",
    "    model_s3_path = optimized_model_s3_path\n",
    "except:\n",
    "    try:\n",
    "        %store -r model_s3_path\n",
    "        model_s3_path\n",
    "        print(\"model_s3_path: \", model_s3_path)\n",
    "    except:\n",
    "        print(\"optimized_model_s3_path and model_s3_path not found\")\n",
    "        model_s3_path = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 추론 이미지 가져오기\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/ec2-user/SageMaker/.xdg/config/sagemaker/config.yaml\n",
      "sagemaker role arn: arn:aws:iam::057716757052:role/gen_ai_gsmoon\n",
      "sagemaker session region: us-east-1\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "sess = sagemaker.Session()\n",
    "role = sagemaker.get_execution_role()\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "sagemaker_client = sess.sagemaker_client\n",
    "sagemaker_runtime_client = sess.sagemaker_runtime_client\n",
    "\n",
    "\n",
    "print(f\"sagemaker role arn: {role}\")\n",
    "print(f\"sagemaker session region: {sess.boto_region_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "llm image uri: 763104351884.dkr.ecr.us-east-1.amazonaws.com/huggingface-pytorch-tgi-inference:2.3.0-tgi2.0.2-gpu-py310-cu121-ubuntu22.04\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.huggingface import get_huggingface_llm_image_uri\n",
    "\n",
    "# retrieve the llm image uri\n",
    "llm_image = get_huggingface_llm_image_uri(\n",
    "  \"huggingface\",\n",
    "  session=sess,\n",
    "  version=\"2.0.2\",\n",
    ")\n",
    "\n",
    "# print ecr image uri\n",
    "print(f\"llm image uri: {llm_image}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. SageMaker Endpoint 생성\n",
    "- EndpointConfiguration 생성\n",
    "- Endpoint 생성"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 모델을 배포할 인스턴스 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ml.g5.xlarge and # of GPU 1 is set\n"
     ]
    }
   ],
   "source": [
    "# instance_type = \"ml.g5.12xlarge\"\n",
    "# instance_type = \"ml.g5.4xlarge\"\n",
    "instance_type = \"ml.g5.xlarge\"\n",
    "\n",
    "\n",
    "if instance_type == \"ml.p4d.24xlarge\":\n",
    "    num_GPUSs = 8\n",
    "elif instance_type == \"ml.g5.12xlarge\":\n",
    "    num_GPUSs = 4\n",
    "elif instance_type == \"ml.g5.4xlarge\":\n",
    "    num_GPUSs = 1    \n",
    "else:\n",
    "    num_GPUSs = 1\n",
    "    \n",
    "print(f\"{instance_type} and # of GPU {num_GPUSs} is set\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Endpoint config name 및 설정 값 기술"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The current time is 2024-07-29-12-25-04\n",
      "Endpoint config name: llama3-endpoint-config-2024-07-29-12-25-04\n",
      "Initial instance count: 1\n",
      "Max instance count: 2\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "currentDateAndTime = datetime.now()\n",
    "currentTime = currentDateAndTime.strftime(\"%Y-%m-%d-%H-%M-%S\")\n",
    "print(\"The current time is\", currentTime)\n",
    "\n",
    "# Set an unique endpoint config name\n",
    "endpoint_config_name = f\"llama3-endpoint-config-{currentTime}\" \n",
    "print(f\"Endpoint config name: {endpoint_config_name}\")\n",
    "\n",
    "\n",
    "# Set varient name and instance type for hosting\n",
    "variant_name = \"AllTraffic\"\n",
    "model_data_download_timeout_in_seconds = 600\n",
    "container_startup_health_check_timeout_in_seconds = 600\n",
    "\n",
    "initial_instance_count = 1\n",
    "max_instance_count = 2\n",
    "print(f\"Initial instance count: {initial_instance_count}\")\n",
    "print(f\"Max instance count: {max_instance_count}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SageMaker Endpoint Configuration 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "epc_response = sagemaker_client.create_endpoint_config(\n",
    "    EndpointConfigName=endpoint_config_name,\n",
    "    ExecutionRoleArn=role,\n",
    "    ProductionVariants=[\n",
    "        {\n",
    "            \"VariantName\": variant_name,\n",
    "            \"InstanceType\": instance_type,\n",
    "            \"InitialInstanceCount\": 1,\n",
    "            \"ModelDataDownloadTimeoutInSeconds\": model_data_download_timeout_in_seconds,\n",
    "            \"ContainerStartupHealthCheckTimeoutInSeconds\": container_startup_health_check_timeout_in_seconds,\n",
    "            \"ManagedInstanceScaling\": {\n",
    "                \"Status\": \"ENABLED\",\n",
    "                \"MinInstanceCount\": initial_instance_count,\n",
    "                \"MaxInstanceCount\": max_instance_count,\n",
    "            },\n",
    "            \"RoutingConfig\": {\"RoutingStrategy\": \"LEAST_OUTSTANDING_REQUESTS\"},\n",
    "        }\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"EndpointConfigArn\": \"arn:aws:sagemaker:us-east-1:057716757052:endpoint-config/llama3-endpoint-config-2024-07-29-12-25-04\",\n",
      "    \"ResponseMetadata\": {\n",
      "        \"RequestId\": \"9b7c30c6-b5be-4f6f-b64d-ecd078e64ae7\",\n",
      "        \"HTTPStatusCode\": 200,\n",
      "        \"HTTPHeaders\": {\n",
      "            \"x-amzn-requestid\": \"9b7c30c6-b5be-4f6f-b64d-ecd078e64ae7\",\n",
      "            \"content-type\": \"application/x-amz-json-1.1\",\n",
      "            \"content-length\": \"123\",\n",
      "            \"date\": \"Mon, 29 Jul 2024 12:25:05 GMT\"\n",
      "        },\n",
      "        \"RetryAttempts\": 0\n",
      "    }\n",
      "}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/SageMaker/.cs/conda/envs/llama3_puy310/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from scripts.inference_util import print_json\n",
    "# print(epc_response)\n",
    "print_json(epc_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Endpoint 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "endpoint_name: llama3-endpoint-2024-07-29-12-25-04\n",
      "Creating endpoint: llama3-endpoint-2024-07-29-12-25-04\n",
      "-----!CPU times: user 18.3 ms, sys: 5.41 ms, total: 23.7 ms\n",
      "Wall time: 3min\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'EndpointName': 'llama3-endpoint-2024-07-29-12-25-04',\n",
       " 'EndpointArn': 'arn:aws:sagemaker:us-east-1:057716757052:endpoint/llama3-endpoint-2024-07-29-12-25-04',\n",
       " 'EndpointConfigName': 'llama3-endpoint-config-2024-07-29-12-25-04',\n",
       " 'ProductionVariants': [{'VariantName': 'AllTraffic',\n",
       "   'CurrentInstanceCount': 1,\n",
       "   'DesiredInstanceCount': 1,\n",
       "   'ManagedInstanceScaling': {'Status': 'ENABLED',\n",
       "    'MinInstanceCount': 1,\n",
       "    'MaxInstanceCount': 2},\n",
       "   'RoutingConfig': {'RoutingStrategy': 'LEAST_OUTSTANDING_REQUESTS'}}],\n",
       " 'EndpointStatus': 'InService',\n",
       " 'CreationTime': datetime.datetime(2024, 7, 29, 12, 25, 6, 162000, tzinfo=tzlocal()),\n",
       " 'LastModifiedTime': datetime.datetime(2024, 7, 29, 12, 27, 43, 138000, tzinfo=tzlocal()),\n",
       " 'ResponseMetadata': {'RequestId': 'bf989fe2-0589-4944-b6e6-e1913c59cb41',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': 'bf989fe2-0589-4944-b6e6-e1913c59cb41',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '574',\n",
       "   'date': 'Mon, 29 Jul 2024 12:28:06 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# Set a unique endpoint name\n",
    "endpoint_name = f\"llama3-endpoint-{currentTime}\"\n",
    "print(f\"endpoint_name: {endpoint_name}\")\n",
    "\n",
    "ep_response = sagemaker_client.create_endpoint(\n",
    "    EndpointName=endpoint_name,\n",
    "    EndpointConfigName=endpoint_config_name,\n",
    ")\n",
    "# print(ep_response)\n",
    "print(f\"Creating endpoint: {endpoint_name}\")\n",
    "sess.wait_for_endpoint(endpoint_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. SageMaker Model 생성"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SageMaker Model 정의\n",
    "- 추론 이미지 기술 \n",
    "- 모델 아티펙트 경로 기술 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import HfFolder\n",
    "from sagemaker.huggingface import HuggingFaceModel\n",
    "\n",
    "# sagemaker config\n",
    "\n",
    "health_check_timeout = 600 # 20 minutes\n",
    "model_name = f\"llama3-model-{currentTime}\"\n",
    "\n",
    "import time\n",
    "\n",
    "# Define Model and Endpoint configuration parameter\n",
    "config = {\n",
    "  'HF_MODEL_ID': \"/opt/ml/model\",       # Path to the model in the container\n",
    "  'SM_NUM_GPUS': f\"{num_GPUSs}\",        # Number of GPU used per replica\n",
    "  'MAX_INPUT_LENGTH': \"2048\",           # Max length of input text\n",
    "  'MAX_TOTAL_TOKENS': \"4096\",           # Max length of the generation (including input text)\n",
    "  #'MAX_BATCH_PREFILL_TOKENS': \"16182\",  # Limits the number of tokens that can be processed in parallel during the generation\n",
    "  'MAX_BATCH_PREFILL_TOKENS': \"4096\",  # Limits the number of tokens that can be processed in parallel during the generation\n",
    "  'MESSAGES_API_ENABLED': \"true\",       # Enable the OpenAI Messages API\n",
    "}\n",
    "\n",
    "# create HuggingFaceModel with the image uri\n",
    "llm_model = HuggingFaceModel(\n",
    "  role=role,\n",
    "  name=model_name,\n",
    "  model_data=model_s3_path, # path to s3 bucket with model, we are not using a compressed model\n",
    "  image_uri=llm_image,\n",
    "  env=config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "llm_model.create()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### inference component 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference_component_name_llama3b:  llama3b-IC-2024-07-29-12-25-04\n"
     ]
    }
   ],
   "source": [
    "# Deploy model to Amazon SageMaker Inference Component\n",
    "inference_component_name_llama3b = f\"llama3b-IC-{currentTime}\"\n",
    "print(\"inference_component_name_llama3b: \", inference_component_name_llama3b)\n",
    "variant_name = \"AllTraffic\"\n",
    "\n",
    "ic_response = sagemaker_client.create_inference_component(\n",
    "    InferenceComponentName=inference_component_name_llama3b,\n",
    "    EndpointName=endpoint_name,\n",
    "    VariantName=variant_name,\n",
    "    Specification={\n",
    "        \"ModelName\": model_name,\n",
    "        \"ComputeResourceRequirements\": {\n",
    "            \"NumberOfAcceleratorDevicesRequired\": num_GPUSs,\n",
    "            \"NumberOfCpuCoresRequired\": 1,\n",
    "            \"MinMemoryRequiredInMb\": 1024,\n",
    "        },\n",
    "    },\n",
    "    RuntimeConfig={\"CopyCount\": 1},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "InferenceComponent: llama3b-IC-2024-07-29-12-25-04\n",
      "Creating\n",
      "Creating\n",
      "Creating\n",
      "Creating\n",
      "Creating\n",
      "Creating\n",
      "Creating\n",
      "Creating\n",
      "Creating\n",
      "Creating\n",
      "Creating\n",
      "Creating\n",
      "Creating\n",
      "Creating\n",
      "Creating\n",
      "Creating\n",
      "Creating\n",
      "InService\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "# Wait for IC to come InService\n",
    "print(f\"InferenceComponent: {inference_component_name_llama3b}\")\n",
    "while True:\n",
    "    desc = sagemaker_client.describe_inference_component(\n",
    "        InferenceComponentName=inference_component_name_llama3b\n",
    "    )\n",
    "    status = desc[\"InferenceComponentStatus\"]\n",
    "    print(status)\n",
    "    sys.stdout.flush()\n",
    "    if status in [\"InService\", \"Failed\"]:\n",
    "        break\n",
    "    time.sleep(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 추론: 한국어 요약 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from scripts.inference_util import (\n",
    "    print_json,\n",
    "    create_messages_parameters,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%store -r full_test_data_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "from scripts.inference_util import (\n",
    "    get_message_from_dataset,\n",
    "    extract_system_user_prompt,\n",
    "    # run_inference,\n",
    "    generate_response_IC\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed time: 3.418 second\n",
      "**Query:**\n",
      "{'messages': [{'role': 'system', 'content': 'You are an AI assistant specialized in news articles.Your role is to provide accurate summaries and insights in Korean. Please analyze the given text and provide concise, informative summaries that highlight the key goals and findings.'}, {'role': 'user', 'content': 'Please summarize the goals for journalist in this text:\\n\\n산업현장에서 세종텔레콤의 스마트 안전 플랫폼 솔루션 을 활용하고 있다. 세종텔레콤 제공 세종텔레콤은 태영건설에 중대재해처벌법 대응에 최적화된 스마트 안전 플랫폼 솔루션 납품 계약을 체결했다고 4일 밝혔다. 우선 태영건설 전국 산업현장에 스마트 안전 솔루션 500대 납품 계약을 체결했고 추가 구축을 협의 중이다. 지난 1월 본격 시행된 중대재해처벌법은 산업현장에서 인명사고 발생 시 경영진이나 법인에게 책임을 물을 수 있도록 규정한 법이다. 해당 법은 안전 보건 관련 관리상의 조치 구축을 의무화하고 있으나 기업의 한정적 자원과 부족한 인력 문제로 어려움을 겪고 있다. 세종텔레콤의 스마트 안전 플랫폼 솔루션은 출입관리부터 CCTV 가스탐지 각종 센서 등을 하나로 통합해 현장을 종합 관리할 수 있다. LBS 위치기반 IoT 사물인터넷 등 스마트 기술을 융합했다. 안전 관리 담당자는 각 현장마다 설치된 카메라 및 CCTV 개소별 센서와 통신 인프라를 통해 현장 정보를 실시간으로 확인하고 비상 상황 시에는 전체 현장 또는 해당 구역 상황실 시스템이나 모바일로 근로자에게 안전 조치사항을 지시할 수 있다. 이와 함께 타워크레인에 설치한 360도 카메라를 통해 현장의 불안전 요소를 발견하면 관계자에게 알림을 보낼 수 있다. 지하 작업에서는 이동형 스마트 영상 장비로 안전 사각지대를 살필 수 있고 밀폐된 작업 공간에서는 가스 센서와 신호등형 전광판을 설치해 실시간으로 스마트 상황판에 가스 농도를 전송한다. 유해가스가 허용 농도를 초과하면 현장에서 환기 시스템이 자동으로 작동한다. 현장 내 추락 사고가 발생할 수 있는 개구부에 부착한 센서는 개구부가 비정상적으로 개폐됐을 때 경고음을 보내 위험상황을 알린다. 강효상 세종텔레콤 통신사업본부장은 중대재해처벌법 시행에 따른 건설 현장의 안전사고 예방을 위한 최적의 솔루션이 요구되고 있는 시점에서 스마트 안전 솔루션 예방 플랫폼 구축의 의의가 크다 고 말했다.'}], 'model': 'meta-llama-3-fine-tuned', 'parameters': {'max_tokens': 512, 'top_p': 0.9, 'temperature': 0.6, 'stop': ['<|eot_id|>']}}\n",
      "**Original Answer:**\n",
      "세종텔레콤은 태영건설에 출입관리부터 CCTV 가스탐지 각종 센서 등을 하나로 통합해 현장을 종합 관리할 수 있는 스마트 안전 플랫폼 솔루션 납품 계약을 체결했다고 4일 밝혔으며 태영건설 전국 산업현장에 스마트 안전 솔루션 500대 납품 계약을 체결했고 추가 구축을 협의 중이다.\n",
      "\n",
      "**Generated Answer:**\n",
      "4일 세종텔레콤은 산업현장에서 인명사고 발생 시 경영진이나 법인에게 책임을 물을 수 있도록 규정한 중대재해처벌법 대응에 최적화된 스마트 안전 솔루션 500대 납품 계약을 체결했다고 4일 밝히면서 앞으로 추가 구축을 평가하는 중이다.\n"
     ]
    }
   ],
   "source": [
    "messages, full_test_dataset, rand_idx = get_message_from_dataset(\n",
    "                                        sample_dataset_json_file = full_test_data_json, verbose=False)    \n",
    "generate_response_IC(messages, endpoint_name, full_test_dataset, rand_idx, inference_component_name_llama3b)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 리소스 삭제\n",
    "- 인퍼런스 컴포넌트 삭제\n",
    "- 세이지 메이커 모델 삭제\n",
    "- 엔드포인트 삭제\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.predictor import Predictor\n",
    "\n",
    "predictor = Predictor(\n",
    "    endpoint_name=endpoint_name,\n",
    "    sagemaker_session=sess,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleting inference components: [b magenta]llama3b-IC-2024-07-29-12-25-04 ✅\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    print(f\"Deleting inference components: [b magenta]{inference_component_name_llama3b} ✅\")\n",
    "    # Delete inference component\n",
    "    sagemaker_client.delete_inference_component(\n",
    "        InferenceComponentName=inference_component_name_llama3b\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(f\"{e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleting model: llama3-model-2024-07-29-12-25-04\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    print(f\"Deleting model: {model_name}\")\n",
    "    predictor.delete_model()\n",
    "except Exception as e:\n",
    "    print(f\"{e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleting endpoint: [b magenta]llama3-endpoint-2024-07-29-12-25-04 ✅\n",
      "------------------------------\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "\n",
    "try:\n",
    "    print(f\"Deleting endpoint: [b magenta]{predictor.endpoint_name} ✅\")\n",
    "    predictor.delete_endpoint()\n",
    "except Exception as e:\n",
    "    print(f\"{e}\")\n",
    "\n",
    "print(\"---\" * 10)\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.14 ('llama3_puy310')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "vscode": {
   "interpreter": {
    "hash": "6daafc7ae2313787fa97137de7504cfa7c5a594d29476828201b4f7d7fb5c4e1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
