{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe4872e4-c90b-434b-bfe5-f88292fba385",
   "metadata": {},
   "source": [
    "# ë¡œì»¬ì—ì„œ í›ˆë ¨ í•˜ê¸°\n",
    "- ì´ ë…¸íŠ¸ë¶ì€ ë¡œì»¬ (í˜„ì¬ ë¨¸ì‹ ) ì—ì„œ Hugging Face Accelerator + PyTorch ë¡œ íŒŒì¸ íŠœë‹ í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5da7aa7-1a2d-4db1-b011-59217c32a83a",
   "metadata": {},
   "source": [
    "## 1. í™˜ê²½ ì…‹ì—…"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c268a663-63a3-4d9e-8bd3-8025dec88254",
   "metadata": {},
   "source": [
    "### Hugging Face Token ì…ë ¥\n",
    "- [ì¤‘ìš”] HF Key ê°€ ë…¸ì¶œì´ ì•ˆë˜ë„ë¡ ì¡°ì‹¬í•˜ì„¸ìš”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3ee8f554-0ac4-499a-bc35-7fbae4b96b98",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/SageMaker/.cs/conda/envs/finetune_image/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch: 2.4.1+cu121\n",
      "transformers: 4.45.1\n"
     ]
    }
   ],
   "source": [
    "import torch, transformers\n",
    "print (f'torch: {torch.__version__}')\n",
    "print (f'transformers: {transformers.__version__}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "542f1cbe-5b7e-4fae-bf2d-4a6ac2f3dfe1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: -c: line 0: syntax error near unexpected token `newline'\n",
      "/bin/bash: -c: line 0: `huggingface-cli login --token <Type Your HF Key>'\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def set_hf_key_env_vars(hf_key_name, key_val):\n",
    "    os.environ[hf_key_name] = key_val\n",
    "\n",
    "def get_hf_key_env_vars(hf_key_name):\n",
    "    HF_key_value = os.environ.get(hf_key_name)\n",
    "\n",
    "    return HF_key_value\n",
    "\n",
    "\n",
    "is_sagemaker_notebook = True\n",
    "if is_sagemaker_notebook:\n",
    "    hf_key_name = \"HF_KEY\"\n",
    "    key_val = \"<Type Your HF Key>\"\n",
    "    set_hf_key_env_vars(hf_key_name, key_val)\n",
    "    HF_TOKEN = get_hf_key_env_vars(hf_key_name)\n",
    "else: # VS Code\n",
    "    from dotenv import load_dotenv\n",
    "    HF_TOKEN = os.getenv('HF_TOKEN')\n",
    "\n",
    "\n",
    "# Log in to HF\n",
    "!huggingface-cli login --token {HF_TOKEN}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebcc7a0b-4829-4b2c-88b6-fd423732c53a",
   "metadata": {},
   "source": [
    "### ì €ì¥ëœ ë³€ìˆ˜ ë¡œë”©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "24bce431",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_folder:  ../data/naver-news-summarization-ko\n",
      "train_data_json:  ../data/naver-news-summarization-ko/train/train_dataset.json\n",
      "validation_data_json:  ../data/naver-news-summarization-ko/validation/validation_dataset.json\n",
      "test_data_json:  ../data/naver-news-summarization-ko/test/test_dataset.json\n",
      "full_train_data_json:  ../data/naver-news-summarization-ko/full_train/train_dataset.json\n",
      "full_validation_data_json:  ../data/naver-news-summarization-ko/full_validation/validation_dataset.json\n",
      "full_test_data_json:  ../data/naver-news-summarization-ko/full_test/test_dataset.json\n"
     ]
    }
   ],
   "source": [
    "%store -r data_folder\n",
    "%store -r train_data_json \n",
    "%store -r validation_data_json \n",
    "%store -r test_data_json \n",
    "%store -r full_train_data_json \n",
    "%store -r full_validation_data_json \n",
    "%store -r full_test_data_json\n",
    "\n",
    "\n",
    "print(\"data_folder: \", data_folder)\n",
    "print(\"train_data_json: \", train_data_json)\n",
    "print(\"validation_data_json: \", validation_data_json)\n",
    "print(\"test_data_json: \", test_data_json)\n",
    "print(\"full_train_data_json: \", full_train_data_json)\n",
    "print(\"full_validation_data_json: \", full_validation_data_json)\n",
    "print(\"full_test_data_json: \", full_test_data_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a396bc4f-0b0a-4ffb-8c59-18ed6d0a968d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from datasets import Dataset, load_dataset\n",
    "from datasets import load_dataset\n",
    "from transformers import set_seed\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a78f53c-d21d-4fca-b69d-6a85d966353c",
   "metadata": {},
   "source": [
    "## 2. ë² ì´ìŠ¤ ëª¨ë¸ ì¤€ë¹„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d514e3df",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#model_id = \"meta-llama/Meta-Llama-3-8B\"\n",
    "model_id = \"MLP-KTLim/llama-3-Korean-Bllossom-8B\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5e8a20c2-ef30-4214-89e2-c58fe7e250b5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prefix = \"llama-3-8b-qlora-naver-news\"\n",
    "output_dir = f\"/home/ec2-user/SageMaker/models/{prefix}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2403f6b8",
   "metadata": {},
   "source": [
    "### Config YAML íŒŒì¼ ìƒì„±"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "72d30120",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting accelerator_config/local_llama_3_8b_qlora.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile accelerator_config/local_llama_3_8b_qlora.yaml\n",
    "# script parameters\n",
    "#model_id:  \"meta-llama/Meta-Llama-3-8B\" # Hugging Face model id\n",
    "model_id: \"MLP-KTLim/llama-3-Korean-Bllossom-8B\"\n",
    "###########################\n",
    "# small samples for Debug\n",
    "###########################\n",
    "train_dataset_path: \"../data/naver-news-summarization-ko/train\"                      # path to dataset\n",
    "validation_dataset_path: \"../data/naver-news-summarization-ko/validation\"                      # path to dataset\n",
    "#test_dataset_path: \"../data/naver-news-summarization-ko/test\"                      # path to dataset\n",
    "per_device_train_batch_size: 1         # batch size per device during training\n",
    "per_device_eval_batch_size: 1          # batch size for evaluation\n",
    "gradient_accumulation_steps: 1         # number of steps before performing a backward/update pass\n",
    "###########################\n",
    "# large samples for evaluation\n",
    "###########################\n",
    "# train_dataset_path: \"../data/naver-news-summarization-ko/full_train\"                      # path to dataset\n",
    "# validation_dataset_path: \"../data/naver-news-summarization-ko/full_validation\"                      # path to dataset\n",
    "# test_dataset_path: \"../data/naver-news-summarization-ko/full_test\"                      # path to dataset\n",
    "# per_device_train_batch_size: 16         # batch size per device during training\n",
    "# per_device_eval_batch_size: 1          # batch size for evaluation\n",
    "# gradient_accumulation_steps: 2         # number of steps before performing a backward/update pass\n",
    "###########################\n",
    "max_seq_length:  2048              # max sequence length for model and packing of the dataset\n",
    "\n",
    "\n",
    "# training parameters\n",
    "output_dir: \"/home/ec2-user/SageMaker/models/llama-3-8b-qlora-naver-news\" # Temporary output directory for model checkpoints\n",
    "report_to: \"tensorboard\"               # report metrics to tensorboard\n",
    "logging_dir: \"/home/ec2-user/SageMaker/logs/llama-3-8b-qlora-naver-news\" # log folder for tensorboard\n",
    "learning_rate: 0.0002                  # learning rate 2e-4\n",
    "lr_scheduler_type: \"constant\"          # learning rate scheduler\n",
    "num_train_epochs: 1                    # number of training epochs\n",
    "optim: adamw_torch                     # use torch adamw optimizer\n",
    "logging_steps: 10                      # log every 10 steps\n",
    "save_strategy: epoch                   # save checkpoint every epoch\n",
    "evaluation_strategy: epoch             # evaluate every epoch\n",
    "max_grad_norm: 0.3                     # max gradient norm\n",
    "warmup_ratio: 0.03                     # warmup ratio\n",
    "bf16: true                             # use bfloat16 precision\n",
    "tf32: true                             # use tf32 precision\n",
    "gradient_checkpointing: true           # use gradient checkpointing to save memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c851f75d",
   "metadata": {},
   "source": [
    "## 3. í›ˆë ¨ Script ì‹¤í–‰\n",
    "\n",
    "ì•„ë˜ëŠ” Hugging Face ì˜ Accelerator ê¸°ë°˜ í•™ìŠµ ëª…ë ¹ì–´ ì…ë‹ˆë‹¤.\n",
    "- í˜„ì¬ ë¨¸ì‹ ì— 4ê°œì˜ GPU ê°€ ìˆëŠ” ê²½ìš° ì…ë‹ˆë‹¤. GPU ê°€ 1ê°œ ì´ë©´ nproc_per_node=1 ë¡œ ìˆ˜ì •í•´ì„œ ì‹¤í–‰ í•˜ì„¸ìš”. \n",
    "\n",
    "```\n",
    "!torchrun --nproc_per_node=4 \\\n",
    "../../scripts/local_run_qlora.py \\\n",
    "--config config_folder_name/local_llama_3_8b_qlora.yaml\n",
    "```\n",
    "- ì°¸ê³ \n",
    "    - Launching your ğŸ¤— Accelerate scripts, [Link](https://huggingface.co/docs/accelerate/en/basic_tutorials/launch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "650a43f8-666d-4176-94b5-885f2bcb58cd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "config_folder_name = \"accelerator_config\"\n",
    "os.makedirs(config_folder_name, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cb9be15-e2bc-4b03-a80b-8849e77d9e19",
   "metadata": {},
   "source": [
    "### Hugging Face  Accelerator ì— ì œê³µí•  config.yaml ì…ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccd6de05-6af0-45d0-8ecf-68e796771b96",
   "metadata": {},
   "source": [
    "### IMPORTANT!! Set use reentrant to False when we don't use FSDP\n",
    "```\n",
    "if training_args.gradient_checkpointing:\n",
    "    training_args.gradient_checkpointing_kwargs = {\"use_reentrant\":False**}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9f81f6fe-4e96-4a6a-a354-d89d2f248351",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/SageMaker/.cs/conda/envs/finetune_image/lib/python3.10/site-packages/transformers/training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ğŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "## script_args: \n",
      " ScriptArguments(train_dataset_path='../data/naver-news-summarization-ko/train', validation_dataset_path='../data/naver-news-summarization-ko/validation', model_id='MLP-KTLim/llama-3-Korean-Bllossom-8B', max_seq_length=2048)\n",
      "## training_args: \n",
      " TrainingArguments(\n",
      "_n_gpu=1,\n",
      "accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "auto_find_batch_size=False,\n",
      "batch_eval_metrics=False,\n",
      "bf16=True,\n",
      "bf16_full_eval=False,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_persistent_workers=False,\n",
      "dataloader_pin_memory=True,\n",
      "dataloader_prefetch_factor=None,\n",
      "ddp_backend=None,\n",
      "ddp_broadcast_buffers=None,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "ddp_timeout=1800,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "dispatch_batches=None,\n",
      "do_eval=True,\n",
      "do_predict=False,\n",
      "do_train=False,\n",
      "eval_accumulation_steps=None,\n",
      "eval_delay=0,\n",
      "eval_do_concat_batches=True,\n",
      "eval_on_start=False,\n",
      "eval_steps=None,\n",
      "eval_strategy=epoch,\n",
      "eval_use_gather_object=False,\n",
      "evaluation_strategy=epoch,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "fsdp=[],\n",
      "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\n",
      "fsdp_min_num_params=0,\n",
      "fsdp_transformer_layer_cls_to_wrap=None,\n",
      "full_determinism=False,\n",
      "gradient_accumulation_steps=1,\n",
      "gradient_checkpointing=True,\n",
      "gradient_checkpointing_kwargs=None,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_always_push=False,\n",
      "hub_model_id=None,\n",
      "hub_private_repo=False,\n",
      "hub_strategy=every_save,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "include_inputs_for_metrics=False,\n",
      "include_num_input_tokens_seen=False,\n",
      "include_tokens_per_second=False,\n",
      "jit_mode_eval=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=0.0002,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=0,\n",
      "log_level=passive,\n",
      "log_level_replica=warning,\n",
      "log_on_each_node=True,\n",
      "logging_dir=/home/ec2-user/SageMaker/logs/llama-3-8b-qlora-naver-news,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=10,\n",
      "logging_strategy=steps,\n",
      "lr_scheduler_kwargs={},\n",
      "lr_scheduler_type=constant,\n",
      "max_grad_norm=0.3,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "neftune_noise_alpha=None,\n",
      "no_cuda=False,\n",
      "num_train_epochs=1,\n",
      "optim=adamw_torch,\n",
      "optim_args=None,\n",
      "optim_target_modules=None,\n",
      "output_dir=/home/ec2-user/SageMaker/models/llama-3-8b-qlora-naver-news,\n",
      "overwrite_output_dir=False,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=1,\n",
      "per_device_train_batch_size=1,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "ray_scope=last,\n",
      "remove_unused_columns=True,\n",
      "report_to=['tensorboard'],\n",
      "restore_callback_states_from_checkpoint=False,\n",
      "resume_from_checkpoint=None,\n",
      "run_name=/home/ec2-user/SageMaker/models/llama-3-8b-qlora-naver-news,\n",
      "save_on_each_node=False,\n",
      "save_only_model=False,\n",
      "save_safetensors=True,\n",
      "save_steps=500,\n",
      "save_strategy=epoch,\n",
      "save_total_limit=None,\n",
      "seed=42,\n",
      "skip_memory_metrics=True,\n",
      "split_batches=None,\n",
      "tf32=True,\n",
      "torch_compile=False,\n",
      "torch_compile_backend=None,\n",
      "torch_compile_mode=None,\n",
      "torch_empty_cache_steps=None,\n",
      "torchdynamo=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_cpu=False,\n",
      "use_ipex=False,\n",
      "use_legacy_prediction_loop=False,\n",
      "use_liger_kernel=False,\n",
      "use_mps_device=False,\n",
      "warmup_ratio=0.03,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      ")\n",
      "Generating train split: 10 examples [00:00, 337.87 examples/s]\n",
      "Generating train split: 10 examples [00:00, 7952.79 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:00<00:00, 557.74 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:00<00:00, 2761.41 examples/s]\n",
      "\n",
      "You are an AI assistant specialized in news articles.\n",
      "Your role is to provide accurate summaries and insights in Korean.\n",
      "Please analyze the given text and provide concise, informative summaries that highlight the key goals and findings.\n",
      "\n",
      "\n",
      "Human: Please summarize the goals for journalist in this text:\n",
      "\n",
      "êµ­ë‚´ ìµœëŒ€ê·œëª¨ ë‚˜ë…¸ ì „ì‹œíšŒì´ì ì„¸ê³„ 3ëŒ€ ë‚˜ë…¸í–‰ì‚¬ ë‚˜ë…¸ì½”ë¦¬ì•„ 2022 ê°€ 6ì¼ë¶€í„° 3ì¼ê°„ ê²½ê¸°ë„ í‚¨í…ìŠ¤ ì œ1ì „ì‹œì¥ 4Â·5í™€ ì—ì„œ ê°œìµœëœë‹¤. ì˜¬í•´ 20íšŒë¥¼ ë§ëŠ” ë‚˜ë…¸ì½”ë¦¬ì•„ëŠ” ì‚°ì—…í†µìƒìì›ë¶€ ê³¼í•™ê¸°ìˆ ì •ë³´í†µì‹ ë¶€ê°€ ê³µë™ ì£¼ìµœí•œë‹¤. ë‚˜ë…¸ìœµí•©ì‚°ì—…ì—°êµ¬ì¡°í•©ê³¼ ë‚˜ë…¸ê¸°ìˆ ì—°êµ¬í˜‘ì˜íšŒê°€ ì£¼ê´€í•œë‹¤. ë‚˜ë…¸ê¸°ìˆ ê³¼ ì‚°ì—…ì˜ í˜„ì¬ ë¯¸ë˜ íŠ¸ëœë“œë¥¼ ì¡°ë§í•˜ëŠ” ê¸°ì¡° ê°•ì—°ì„ ì‹œì‘ìœ¼ë¡œ ë‚˜ë…¸ ìœµí•© ì „ì‹œí™”ì™€ êµ­ì œ ì‹¬í¬ì§€ì—„ í–‰ì‚¬ê°€ ë‹¤ì–‘í•˜ê²Œ ê°œìµœëœë‹¤. ê°•ë¯¼ì„ LGì´ë…¸í… ë¶€ì‚¬ì¥ì´ ììœ¨ì£¼í–‰ì‚°ì—… ë™í–¥ì— ë”°ë¥¸ ë‚˜ë…¸ê¸°ìˆ ê³¼ ì¸ê³µì§€ëŠ¥ AI ì˜ í™œìš© ì„ ì£¼ì œë¡œ ê¸°ì¡°ê°•ì—°ì„ í•œë‹¤. ì•Œë² ë¥´í˜ë¥´ í”„ë‘ìŠ¤ íŒŒë¦¬ ìŠˆë“œëŒ€ êµìˆ˜ë„ ê¸°ì¡° ê°•ì—°ì„ ë§¡ëŠ”ë‹¤. ì „ì‹œê·œëª¨ëŠ” ì½”ë¡œë‚˜19 ì´ì „ ìˆ˜ì¤€ìœ¼ë¡œ íšŒë³µëë‹¤. ì‚¼ì„±ì „ì LG ë“± ì£¼ìš” ê¸°ì—…ê³¼ ë‚˜ë…¸ ê¸°ìˆ  ê¸°ì—… ë“± ì´ 360ê°œì‚¬ê°€ ì°¸ì—¬í•œë‹¤. ë‚˜ë…¸ 20ì£¼ë…„ íŠ¹ë³„ ê¸°ë…ê´€ë„ ë§ˆë ¨ëœë‹¤. 20ì£¼ë…„ íŠ¹ë³„ ê¸°ë…ê´€ì—ëŠ” ì°¨ì„¸ëŒ€ ë°˜ë„ì²´ ë¯¸ë˜ì°¨ 6ì„¸ëŒ€ 6G ì´ë™í†µì‹  íƒ„ì†Œì¤‘ë¦½ ë””ì§€í„¸ ë°”ì´ì˜¤ ë“± 6ê°œ ë¶„ì•¼ í˜ì‹  ê¸°ìˆ ì´ ì†Œê°œëœë‹¤. ì‚°ì—…í™” ì„¸ì…˜ì—ì„œëŠ” ì§€ì† ê°€ëŠ¥ ì„±ì¥ì„ ìœ„í•œ ESG ë‚˜ë…¸ìœµí•©ê¸°ìˆ ì„ ì£¼ì œë¡œ ì´ˆì²­ ê°•ì—°ì´ ì—´ë¦°ë‹¤. ë‚˜ë…¸ì œí’ˆê±°ë˜ìƒë‹´íšŒ ì „ì‹œíšŒí…Œí¬ë‹ˆì»¬íˆ¬ì–´ ìµœì‹ ê¸°ìˆ ë°œí‘œíšŒ ë“± ë‹¤ì–‘í•œ ë‚˜ë…¸ ê´€ë ¨ ë¶€ëŒ€ í–‰ì‚¬ë„ ì¤€ë¹„ëœë‹¤.<|eot_id|>\n",
      "\n",
      "Assistant: êµ­ë‚´ 3ëŒ€ ë‚˜ë…¸í–‰ì‚¬ ì¤‘ í•˜ë‚˜ì¸ ë‚˜ë…¸ì½”ë¦¬ì•„ 2022 ê°€ 6ì¼ë¶€í„° 3ì¼ê°„ ê²½ê¸°ë„ í‚¨í…ìŠ¤ ì œ1ì „ì‹œì¥ 4Â·5í™€ì—ì„œ ê°œìµœë˜ëŠ”ë°, ì‚¼ì„±ì „ì ë“± ì£¼ìš” ê¸°ì—…ê³¼ ë‚˜ë…¸ ê¸°ìˆ  ê¸°ì—… ë“± ì´ 360ê°œì‚¬ê°€ ì°¸ì—¬í•  ì˜ˆì •ì´ë©° ë‚˜ë…¸ê¸°ìˆ ê³¼ ì‚°ì—…ì˜ í˜„ì¬ ë¯¸ë˜ íŠ¸ëœë“œë¥¼ ì¡°ë§í•˜ëŠ” ê¸°ì¡° ê°•ì—°ì„ ì‹œì‘ìœ¼ë¡œ ë‚˜ë…¸ ìœµí•© ì „ì‹œí™”ì™€ êµ­ì œ ì‹¬í¬ì§€ì—„ í–‰ì‚¬ê°€ ë‹¤ì–‘í•˜ê²Œ ê°œìµœëœë‹¤.<|eot_id|>\n",
      "\n",
      "You are an AI assistant specialized in news articles.\n",
      "Your role is to provide accurate summaries and insights in Korean.\n",
      "Please analyze the given text and provide concise, informative summaries that highlight the key goals and findings.\n",
      "\n",
      "\n",
      "Human: Please summarize the goals for journalist in this text:\n",
      "\n",
      "4ì¼ë¶€í„° ì„ë‹¬ê°„ ì¦ê¶Œì‚¬ ì‹ ìš©ìœµìë‹´ë³´ë¹„ìœ¨ ìœ ì§€ì˜ë¬´ ë©´ì œ ê¸ˆìœµë‹¹êµ­ì´ ì½”ìŠ¤í”¼ì§€ìˆ˜ê°€ ì¥ì¤‘ 2300 ì•„ë˜ê¹Œì§€ ë–¨ì–´ì§€ì ì£¼ì‹ì‹œì¥ ë³€ë™ì„±ì„ ì™„í™”í•˜ëŠ” ì¡°ì¹˜ë¥¼ ì‹œí–‰í•˜ê¸°ë¡œ í–ˆë‹¤. ê¸ˆìœµìœ„ì›íšŒëŠ” 1ì¼ ì£¼ì‹ì‹œì¥ ë§ˆê° í›„ ê¹€ì†Œì˜ ë¶€ìœ„ì›ì¥ ì£¼ì¬ë¡œ ì¦ê¶Œ ìœ ê´€ê¸°ê´€ê³¼ ê¸ˆìœµì‹œì¥í•©ë™ì ê²€íšŒì˜ë¥¼ ì—´ê³  ë³€ë™ì„± ì™„í™”ì¡°ì¹˜ë¥¼ ì‹œí–‰í•˜ê¸°ë¡œ í–ˆë‹¤ê³  ë°í˜”ë‹¤. ì´ì— ë”°ë¼ ë‹¤ìŒ ì£¼ì‹ì‹œì¥ ê°œì¥ì¼ì¸ ì˜¤ëŠ” 4ì¼ë¶€í„° 9ì›”30ì¼ê¹Œì§€ 3ê°œì›”ê°„ ì¦ê¶Œì‚¬ì˜ ì‹ ìš©ìœµìë‹´ë³´ë¹„ìœ¨ ìœ ì§€ì˜ë¬´ê°€ ë©´ì œëœë‹¤. ì‹ ìš©ìœµìë‹´ë³´ë¹„ìœ¨ ìœ ì§€ì˜ë¬´ë€ ì¦ê¶Œì‚¬ê°€ ì¼ëª… â€˜ë¹šíˆ¬â€™ ë¹šë‚´ì„œ íˆ¬ì ìê¸ˆì¸ ì‹ ìš©ìœµìë¥¼ ì‹œí–‰í•  ë•Œ ë‹´ë³´ë¥¼ 140% ì´ìƒ í™•ë³´í•˜ê³  ë‚´ê·œì—ì„œ ì •í•œ ë‹´ë³´ë¹„ìœ¨ì„ ìœ ì§€í•  ê²ƒì„ ìš”êµ¬í•˜ëŠ” ê·œì œë‹¤. ìœ ì§€ì˜ë¬´ ë©´ì œëŠ” ì¦ê¶Œì‚¬ì˜ ì‹ ìš©ìœµì ë‹´ë³´ì£¼ì‹ì— ëŒ€í•œ ê³¼ë„í•œ ë°˜ëŒ€ë§¤ë§¤ë¥¼ ì–µì œí•˜ê¸° ìœ„í•œ ì¡°ì¹˜ë‹¤. ê¸ˆìœµë‹¹êµ­ì€ ì½”ë¡œë‚˜19 ì‚¬íƒœê°€ ë³¸ê²©í™”í•œ 2020ë…„ 3ì›”ì—ë„ ì´ ê°™ì€ ì¡°ì¹˜ë¥¼ 6ê°œì›”ê°„ ì‹œí–‰í•œ ë°” ìˆë‹¤. 7ì¼ë¶€í„° 10ì›”6ì¼ê¹Œì§€ëŠ” ìƒì¥ê¸°ì—…ì˜ í•˜ë£¨ ìê¸°ì£¼ì‹ ë§¤ìˆ˜ ì£¼ë¬¸ ìˆ˜ëŸ‰ í•œë„ ì œí•œë„ ì™„í™”ëœë‹¤. ì‹ íƒì·¨ë“ ì£¼ì‹ë„ ë°œí–‰ì£¼ì‹ì´ìˆ˜ì˜ 1% ì´ë‚´ì—ì„œ ì‹ íƒì¬ì‚° ì´ì•¡ ë²”ìœ„ ë‚´ë¡œ í•œì‹œì ìœ¼ë¡œ í™•ëŒ€ëœë‹¤. ì´ì™€ í•¨ê»˜ ê¸ˆìœµìœ„ëŠ” ê¸ˆìœµê°ë…ì›ê³¼ í•œêµ­ê±°ë˜ì†Œ í•©ë™ìœ¼ë¡œ ê³µë§¤ë„ íŠ¹ë³„ì ê²€ì„ ì‹¤ì‹œí•´ ê³µë§¤ë„ í˜„í™©ê³¼ ì‹œì¥êµë€ ê°€ëŠ¥ì„± ë“±ì„ ì‚´í´ë³´ê¸°ë¡œ í–ˆë‹¤. í˜„ì¬ ê³µë§¤ë„ëŠ” ì§€ë‚œí•´ 5ì›” ì´í›„ ì œí•œì ìœ¼ë¡œ ì‹¤ì‹œë˜ê³  ìˆë‹¤. ê¸ˆìœµìœ„ì™€ ê¸ˆê°ì›ì€ ë§¤ì£¼ ê¸ˆìš”ì¼ ê¸ˆìœµì‹œì¥í•©ë™ì ê²€íšŒì˜ë¥¼ ì—´ì–´ ì¦ì‹œ ë“± ê¸ˆìœµì‹œì¥ìƒí™©ì„ ì ê²€í•  ì˜ˆì •ì´ë‹¤. ê¸ˆìœµë‹¹êµ­ì€ â€œì»¨í‹´ì „ì‹œí”Œëœì— ë”°ë¼ í•„ìš”í•œ ì‹œì¥ ë³€ë™ì„± ì™„í™” ì¡°ì¹˜ë¥¼ ê²€í† Â·ì‹œí–‰í•´ ë‚˜ê°ˆ ê²ƒâ€ì´ë¼ê³  ë§í–ˆë‹¤.<|eot_id|>\n",
      "\n",
      "Assistant: 1ì¼ 1ì¼ ê¸ˆìœµìœ„ì›íšŒëŠ” ì¦ê¶Œ ìœ ê´€ê¸°ê´€ê³¼ ê¸ˆìœµì‹œì¥í•©ë™ì ê²€íšŒì˜ë¥¼ ì—´ê³  ì½”ìŠ¤í”¼ì§€ìˆ˜ê°€ ì¥ì¤‘ 2300 ì•„ë˜ê¹Œì§€ ë–¨ì–´ì§€ì ì£¼ì‹ì‹œì¥ ë³€ë™ì„±ì„ ì™„í™”í•˜ëŠ” ì¡°ì¹˜ë¥¼ ì‹œí–‰í•˜ê¸°ë¡œ í•˜ì˜€ìœ¼ë©°, ì´ì— ë”°ë¼ ë‹¤ìŒ ì£¼ì‹ì‹œì¥ ê°œì¥ì¼ì¸ ì˜¤ëŠ” 4ì¼ë¶€í„° 9ì›”30ì¼ê¹Œì§€ 3ê°œì›”ê°„ ì¦ê¶Œì‚¬ì˜ ì‹ ìš©ìœµìë‹´ë³´ë¹„ìœ¨ ìœ ì§€ì˜ë¬´ê°€ ë©´ì œëœë‹¤.<|eot_id|>\n",
      "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n",
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:03<00:00,  1.10it/s]\n",
      "/home/ec2-user/SageMaker/.cs/conda/envs/finetune_image/lib/python3.10/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': dataset_text_field, max_seq_length, packing, dataset_kwargs. Will not be supported from version '1.0.0'.\n",
      "\n",
      "Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "/home/ec2-user/SageMaker/.cs/conda/envs/finetune_image/lib/python3.10/site-packages/transformers/training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ğŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/home/ec2-user/SageMaker/.cs/conda/envs/finetune_image/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:195: UserWarning: You passed a `packing` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n",
      "/home/ec2-user/SageMaker/.cs/conda/envs/finetune_image/lib/python3.10/site-packages/transformers/training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ğŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/home/ec2-user/SageMaker/.cs/conda/envs/finetune_image/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:283: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n",
      "/home/ec2-user/SageMaker/.cs/conda/envs/finetune_image/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:321: UserWarning: You passed a `dataset_text_field` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n",
      "/home/ec2-user/SageMaker/.cs/conda/envs/finetune_image/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:327: UserWarning: You passed a `dataset_kwargs` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n",
      "Generating train split: 3 examples [00:00, 202.73 examples/s]\n",
      "Generating train split: 3 examples [00:00, 402.42 examples/s]\n",
      "if trainer.is_fsdp_enabled False\n",
      "trainable params: 41,943,040 || all params: 8,072,204,288 || trainable%: 0.5196\n",
      "  0%|                                                     | 0/3 [00:00<?, ?it/s]The input hidden states seems to be silently casted in float32, this might be related to the fact you have upcasted embedding or layer norm layers in float32. We will cast back the input in torch.bfloat16.\n",
      "[rank0]:[W1004 06:39:54.531469935 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n",
      "/home/ec2-user/SageMaker/.cs/conda/envs/finetune_image/lib/python3.10/site-packages/torch/utils/checkpoint.py:1399: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with device_autocast_ctx, torch.cpu.amp.autocast(**cpu_autocast_kwargs), recompute_context:  # type: ignore[attr-defined]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:11<00:00,  3.67s/it]\n",
      "  0%|                                                     | 0/3 [00:00<?, ?it/s]\u001b[A\n",
      " 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ               | 2/3 [00:01<00:00,  1.85it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 3.555945634841919, 'eval_runtime': 3.2566, 'eval_samples_per_second': 0.921, 'eval_steps_per_second': 0.921, 'epoch': 1.0}\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:15<00:00,  3.67s/it]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:02<00:00,  1.31it/s]\u001b[A\n",
      "{'train_runtime': 17.8404, 'train_samples_per_second': 0.168, 'train_steps_per_second': 0.168, 'train_loss': 5.9997812906901045, 'epoch': 1.0}\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:17<00:00,  5.94s/it]\n"
     ]
    }
   ],
   "source": [
    "!CUDA_VISIBLE_DEVICES=0 torchrun --nproc_per_node=1 \\\n",
    "../../scripts/run_qlora.py \\\n",
    "--config accelerator_config/local_llama_3_8b_qlora.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "441607e4-f93f-438e-8985-99a76233fe47",
   "metadata": {},
   "source": [
    "## 4. ë² ì´ìŠ¤ ëª¨ë¸ê³¼ í›ˆë ¨ëœ ëª¨ë¸ ë¨¸ì§€"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cebdb212",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_id: MLP-KTLim/llama-3-Korean-Bllossom-8B\n",
      "output_dir: /home/ec2-user/SageMaker/models/llama-3-8b-qlora-naver-news\n"
     ]
    }
   ],
   "source": [
    "print (f'model_id: {model_id}')\n",
    "print (f'output_dir: {output_dir}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3a8acc4",
   "metadata": {},
   "source": [
    "### ëª¨ë¸ ë¨¸ì§€ ë° ë¡œì»¬ì— ì €ì¥\n",
    "- ì•½ 2ë¶„ ê±¸ë¦¼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "19543636",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:02<00:00,  1.43it/s]\n"
     ]
    }
   ],
   "source": [
    "from peft import AutoPeftModelForCausalLM\n",
    "import torch\n",
    "\n",
    "# Load PEFT model on CPU\n",
    "\n",
    "model = AutoPeftModelForCausalLM.from_pretrained(\n",
    "    output_dir,\n",
    "    torch_dtype=torch.float16, ## why bfloat16ì´ ì•„ë‹ˆì§€?\n",
    "    low_cpu_mem_usage=True,\n",
    ")  \n",
    "# Merge LoRA and base model and save\n",
    "merged_model = model.merge_and_unload()\n",
    "merged_model.save_pretrained(output_dir,safe_serialization=True, max_shard_size=\"2GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9d0e3827",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(device(type='cpu'), device(type='cpu'))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.device, merged_model.device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aff0ea42",
   "metadata": {},
   "source": [
    "### ë¨¸ì§€ëœ ëª¨ë¸ ë¡œë”©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a1a6607d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Can't find 'adapter_config.json' at 's3://sagemaker-us-west-2-419974056037/llama3-1-8b-naver-news-2024-10-01-03-15-2024-10-01-03-15-23-273/output/model/'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHFValidationError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m~/SageMaker/.cs/conda/envs/finetune_image/lib/python3.10/site-packages/peft/config.py:144\u001b[0m, in \u001b[0;36mPeftConfigMixin.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, subfolder, **kwargs)\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 144\u001b[0m     config_file \u001b[38;5;241m=\u001b[39m \u001b[43mhf_hub_download\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    145\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mCONFIG_NAME\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhf_hub_download_kwargs\u001b[49m\n\u001b[1;32m    146\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    147\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "File \u001b[0;32m~/SageMaker/.cs/conda/envs/finetune_image/lib/python3.10/site-packages/huggingface_hub/utils/_deprecation.py:101\u001b[0m, in \u001b[0;36m_deprecate_arguments.<locals>._inner_deprecate_positional_args.<locals>.inner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    100\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m)\n\u001b[0;32m--> 101\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/SageMaker/.cs/conda/envs/finetune_image/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py:106\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m arg_name \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrepo_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfrom_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mto_id\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m--> 106\u001b[0m     \u001b[43mvalidate_repo_id\u001b[49m\u001b[43m(\u001b[49m\u001b[43marg_value\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m arg_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m arg_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/SageMaker/.cs/conda/envs/finetune_image/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py:154\u001b[0m, in \u001b[0;36mvalidate_repo_id\u001b[0;34m(repo_id)\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m repo_id\u001b[38;5;241m.\u001b[39mcount(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m--> 154\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HFValidationError(\n\u001b[1;32m    155\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRepo id must be in the form \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrepo_name\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m or \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnamespace/repo_name\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    156\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrepo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. Use `repo_type` argument if needed.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    157\u001b[0m     )\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m REPO_ID_REGEX\u001b[38;5;241m.\u001b[39mmatch(repo_id):\n",
      "\u001b[0;31mHFValidationError\u001b[0m: Repo id must be in the form 'repo_name' or 'namespace/repo_name': 's3://sagemaker-us-west-2-419974056037/llama3-1-8b-naver-news-2024-10-01-03-15-2024-10-01-03-15-23-273/output/model/'. Use `repo_type` argument if needed.",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 7\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoTokenizer \n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Load Model with PEFT adapter\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoPeftModelForCausalLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m  \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43ms3://sagemaker-us-west-2-419974056037/llama3-1-8b-naver-news-2024-10-01-03-15-2024-10-01-03-15-23-273/output/model/\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;66;43;03m#output_dir,\u001b[39;49;00m\n\u001b[1;32m      9\u001b[0m \u001b[43m  \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat16\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m  \u001b[49m\u001b[43mquantization_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mload_in_4bit\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m  \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mauto\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m     12\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(output_dir)\n",
      "File \u001b[0;32m~/SageMaker/.cs/conda/envs/finetune_image/lib/python3.10/site-packages/peft/auto.py:73\u001b[0m, in \u001b[0;36m_BaseAutoPeftModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, adapter_name, is_trainable, config, revision, **kwargs)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfrom_pretrained\u001b[39m(\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;28mcls\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m     67\u001b[0m ):\n\u001b[1;32m     68\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;124;03m    A wrapper around all the preprocessing steps a user needs to perform in order to load a PEFT model. The kwargs\u001b[39;00m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;124;03m    are passed along to `PeftConfig` that automatically takes care of filtering the kwargs of the Hub methods and\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;124;03m    the config object init.\u001b[39;00m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 73\u001b[0m     peft_config \u001b[38;5;241m=\u001b[39m \u001b[43mPeftConfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     74\u001b[0m     base_model_path \u001b[38;5;241m=\u001b[39m peft_config\u001b[38;5;241m.\u001b[39mbase_model_name_or_path\n\u001b[1;32m     75\u001b[0m     base_model_revision \u001b[38;5;241m=\u001b[39m peft_config\u001b[38;5;241m.\u001b[39mrevision\n",
      "File \u001b[0;32m~/SageMaker/.cs/conda/envs/finetune_image/lib/python3.10/site-packages/peft/config.py:148\u001b[0m, in \u001b[0;36mPeftConfigMixin.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, subfolder, **kwargs)\u001b[0m\n\u001b[1;32m    144\u001b[0m         config_file \u001b[38;5;241m=\u001b[39m hf_hub_download(\n\u001b[1;32m    145\u001b[0m             pretrained_model_name_or_path, CONFIG_NAME, subfolder\u001b[38;5;241m=\u001b[39msubfolder, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mhf_hub_download_kwargs\n\u001b[1;32m    146\u001b[0m         )\n\u001b[1;32m    147\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m--> 148\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCan\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt find \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mCONFIG_NAME\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m at \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpretrained_model_name_or_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mexc\u001b[39;00m\n\u001b[1;32m    150\u001b[0m loaded_attributes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mfrom_json_file(config_file)\n\u001b[1;32m    151\u001b[0m kwargs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mclass_kwargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mloaded_attributes}\n",
      "\u001b[0;31mValueError\u001b[0m: Can't find 'adapter_config.json' at 's3://sagemaker-us-west-2-419974056037/llama3-1-8b-naver-news-2024-10-01-03-15-2024-10-01-03-15-23-273/output/model/'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from peft import AutoPeftModelForCausalLM\n",
    "from transformers import AutoTokenizer \n",
    "\n",
    "\n",
    "# Load Model with PEFT adapter\n",
    "model = AutoPeftModelForCausalLM.from_pretrained(\n",
    "  pretrained_model_name_or_path = \"s3://sagemaker-us-west-2-419974056037/llama3-1-8b-naver-news-2024-10-01-03-15-2024-10-01-03-15-23-273/output/model/\",#output_dir,\n",
    "  torch_dtype=torch.float16,\n",
    "  quantization_config= {\"load_in_4bit\": True},\n",
    "  device_map=\"auto\"\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6908046b",
   "metadata": {},
   "source": [
    "## 5. ì¶”ë¡ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5388b34",
   "metadata": {},
   "source": [
    "### í…ŒìŠ¤íŠ¸ ë°ì´í„° ì…‹ ë¡œë”©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eacd377",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset \n",
    "from random import randint\n",
    "\n",
    "def get_message_from_dataset(sample_dataset_json_file):\n",
    "    # Load our test dataset\n",
    "    full_test_dataset = load_dataset(\"json\", data_files=sample_dataset_json_file, split=\"train\")\n",
    "\n",
    "    # Test on sample \n",
    "    rand_idx = randint(0, len(full_test_dataset)-1)\n",
    "    rand_idx = 75\n",
    "    print(\"rand_idx: \", rand_idx)\n",
    "    messages = full_test_dataset[rand_idx][\"messages\"][:2]\n",
    "    # messages = test_dataset[rand_idx][\"text\"][:2]\n",
    "    print(\"messages: \\n\", messages)\n",
    "\n",
    "    return messages, full_test_dataset, rand_idx\n",
    "\n",
    "messages, full_test_dataset, rand_idx = get_message_from_dataset(sample_dataset_json_file = full_test_data_json)    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba15b47b",
   "metadata": {},
   "source": [
    "### ì¶”ë¡ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddaf71a0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_response(messages, model, tokenizer, full_test_dataset, rand_idx):\n",
    "    input_ids = tokenizer.apply_chat_template(messages,add_generation_prompt=True,return_tensors=\"pt\").to(model.device)\n",
    "    outputs = model.generate(\n",
    "        input_ids,\n",
    "        max_new_tokens=512,\n",
    "        eos_token_id= tokenizer.eos_token_id,\n",
    "        do_sample=True,\n",
    "        temperature=0.6,\n",
    "        top_p=0.9,\n",
    "    )\n",
    "    response = outputs[0][input_ids.shape[-1]:]\n",
    "\n",
    "    print(f\"**Query:**\\n{full_test_dataset[rand_idx]['messages'][1]['content']}\\n\")\n",
    "    # print(f\"**Query:**\\n{test_dataset[rand_idx]['text'][1]['content']}\\n\")\n",
    "    # print(f\"**Original Answer:**\\n{test_dataset[rand_idx]['text'][2]['content']}\\n\")\n",
    "    print(f\"**Original Answer:**\\n{full_test_dataset[rand_idx]['messages'][2]['content']}\\n\")\n",
    "    print(f\"**Generated Answer:**\\n{tokenizer.decode(response,skip_special_tokens=True)}\")\n",
    "\n",
    "generate_response(messages, model, tokenizer, full_test_dataset, rand_idx)    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "360223a2-02b4-429d-8e47-6ec5ca55247e",
   "metadata": {},
   "source": [
    "### í• ë‹¹ëœ CUDA memoryë¥¼ Release"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14070303-ea9f-40e5-a56e-cca1bf56f7f8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "del model\n",
    "del tokenizer\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a71f29e0-ce19-4dfd-8b6c-a210ff88376e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_finetune_image",
   "language": "python",
   "name": "conda_finetune_image"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "vscode": {
   "interpreter": {
    "hash": "6daafc7ae2313787fa97137de7504cfa7c5a594d29476828201b4f7d7fb5c4e1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
