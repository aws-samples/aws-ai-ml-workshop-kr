{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe4872e4-c90b-434b-bfe5-f88292fba385",
   "metadata": {},
   "source": [
    "# Î°úÏª¨ÏóêÏÑú ÌõàÎ†® ÌïòÍ∏∞\n",
    "- Ïù¥ ÎÖ∏Ìä∏Î∂ÅÏùÄ Î°úÏª¨ (ÌòÑÏû¨ Î®∏Ïã†) ÏóêÏÑú Hugging Face Accelerator + PyTorch FSDP Î°ú ÌååÏù∏ ÌäúÎãù Ìï©ÎãàÎã§."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5da7aa7-1a2d-4db1-b011-59217c32a83a",
   "metadata": {},
   "source": [
    "## 1. ÌôòÍ≤Ω ÏÖãÏóÖ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c268a663-63a3-4d9e-8bd3-8025dec88254",
   "metadata": {},
   "source": [
    "### Hugging Face Token ÏûÖÎ†•\n",
    "- [Ï§ëÏöî] HF Key Í∞Ä ÎÖ∏Ï∂úÏù¥ ÏïàÎêòÎèÑÎ°ù Ï°∞Ïã¨ÌïòÏÑ∏Ïöî."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3ee8f554-0ac4-499a-bc35-7fbae4b96b98",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "datasets                      3.0.0\n",
      "sagemaker                     2.232.1\n",
      "sagemaker-core                1.0.4\n",
      "sagemaker_nbi_agent           1.0\n",
      "sagemaker_pyspark             1.4.5\n",
      "torch                         2.4.1\n",
      "transformers                  4.45.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip list | grep -E \"torch|datasets|transformers|sagemaker\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "542f1cbe-5b7e-4fae-bf2d-4a6ac2f3dfe1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to /home/ec2-user/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def set_hf_key_env_vars(hf_key_name, key_val):\n",
    "    os.environ[hf_key_name] = key_val\n",
    "\n",
    "def get_hf_key_env_vars(hf_key_name):\n",
    "    HF_key_value = os.environ.get(hf_key_name)\n",
    "\n",
    "    return HF_key_value\n",
    "\n",
    "\n",
    "is_sagemaker_notebook = True\n",
    "if is_sagemaker_notebook:\n",
    "    hf_key_name = \"HF_KEY\"\n",
    "    key_val = \"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\" #\"<Type Your HF Key>\"\n",
    "    set_hf_key_env_vars(hf_key_name, key_val)\n",
    "    HF_TOKEN = get_hf_key_env_vars(hf_key_name)\n",
    "else: # VS Code\n",
    "    from dotenv import load_dotenv\n",
    "    HF_TOKEN = os.getenv('HF_TOKEN')\n",
    "\n",
    "\n",
    "# Log in to HF\n",
    "!huggingface-cli login --token {HF_TOKEN}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebcc7a0b-4829-4b2c-88b6-fd423732c53a",
   "metadata": {},
   "source": [
    "### Ï†ÄÏû•Îêú Î≥ÄÏàò Î°úÎî©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "24bce431",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_folder:  ../data/naver-news-summarization-ko\n",
      "train_data_json:  ../data/naver-news-summarization-ko/train/train_dataset.json\n",
      "validation_data_json:  ../data/naver-news-summarization-ko/validation/validation_dataset.json\n",
      "test_data_json:  ../data/naver-news-summarization-ko/test/test_dataset.json\n",
      "full_train_data_json:  ../data/naver-news-summarization-ko/full_train/train_dataset.json\n",
      "full_validation_data_json:  ../data/naver-news-summarization-ko/full_validation/validation_dataset.json\n",
      "full_test_data_json:  ../data/naver-news-summarization-ko/full_test/test_dataset.json\n"
     ]
    }
   ],
   "source": [
    "%store -r data_folder\n",
    "%store -r train_data_json \n",
    "%store -r validation_data_json \n",
    "%store -r test_data_json \n",
    "%store -r full_train_data_json \n",
    "%store -r full_validation_data_json \n",
    "%store -r full_test_data_json\n",
    "\n",
    "\n",
    "print(\"data_folder: \", data_folder)\n",
    "print(\"train_data_json: \", train_data_json)\n",
    "print(\"validation_data_json: \", validation_data_json)\n",
    "print(\"test_data_json: \", test_data_json)\n",
    "print(\"full_train_data_json: \", full_train_data_json)\n",
    "print(\"full_validation_data_json: \", full_validation_data_json)\n",
    "print(\"full_test_data_json: \", full_test_data_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a396bc4f-0b0a-4ffb-8c59-18ed6d0a968d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from datasets import Dataset, load_dataset\n",
    "from datasets import load_dataset\n",
    "from transformers import set_seed\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a78f53c-d21d-4fca-b69d-6a85d966353c",
   "metadata": {},
   "source": [
    "## 2. Î≤†Ïù¥Ïä§ Î™®Îç∏ Ï§ÄÎπÑ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d514e3df",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#model_id = \"meta-llama/Meta-Llama-3-8B\"\n",
    "model_id = \"MLP-KTLim/llama-3-Korean-Bllossom-8B\"\n",
    "#model_id = \"meta-llama/Llama-3.1-8B-Instruct\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5e8a20c2-ef30-4214-89e2-c58fe7e250b5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prefix = \"llama-3-8b-qlora-naver-news\"\n",
    "output_dir = f\"/home/ec2-user/SageMaker/models/{prefix}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2403f6b8",
   "metadata": {},
   "source": [
    "### Config YAML ÌååÏùº ÏÉùÏÑ±"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "72d30120",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting accelerator_config/local_llama_3_8b_qlora.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile accelerator_config/local_llama_3_8b_qlora.yaml\n",
    "# script parameters\n",
    "#model_id:  \"meta-llama/Meta-Llama-3-8B\" # Hugging Face model id\n",
    "model_id: \"MLP-KTLim/llama-3-Korean-Bllossom-8B\"\n",
    "#model_id: \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
    "model_id: \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "###########################\n",
    "# small samples for Debug\n",
    "###########################\n",
    "train_dataset_path: \"../data/naver-news-summarization-ko/train\"                      # path to dataset\n",
    "validation_dataset_path: \"../data/naver-news-summarization-ko/validation\"                      # path to dataset\n",
    "#test_dataset_path: \"../data/naver-news-summarization-ko/test\"                      # path to dataset\n",
    "per_device_train_batch_size: 1         # batch size per device during training\n",
    "per_device_eval_batch_size: 1          # batch size for evaluation\n",
    "gradient_accumulation_steps: 1         # number of steps before performing a backward/update pass\n",
    "###########################\n",
    "# large samples for evaluation\n",
    "###########################\n",
    "# train_dataset_path: \"../data/naver-news-summarization-ko/full_train\"                      # path to dataset\n",
    "# validation_dataset_path: \"../data/naver-news-summarization-ko/full_validation\"                      # path to dataset\n",
    "# test_dataset_path: \"../data/naver-news-summarization-ko/full_test\"                      # path to dataset\n",
    "# per_device_train_batch_size: 16         # batch size per device during training\n",
    "# per_device_eval_batch_size: 1          # batch size for evaluation\n",
    "# gradient_accumulation_steps: 2         # number of steps before performing a backward/update pass\n",
    "###########################\n",
    "max_seq_length:  2048              # max sequence length for model and packing of the dataset\n",
    "\n",
    "\n",
    "# training parameters\n",
    "output_dir: \"/home/ec2-user/SageMaker/models/llama-3-8b-fsdp-qlora-naver-news\" # Temporary output directory for model checkpoints\n",
    "report_to: \"tensorboard\"               # report metrics to tensorboard\n",
    "logging_dir: \"/home/ec2-user/SageMaker/logs/llama-3-8b-fsdp-qlora-naver-news\" # log folder for tensorboard\n",
    "learning_rate: 0.0002                  # learning rate 2e-4\n",
    "lr_scheduler_type: \"constant\"          # learning rate scheduler\n",
    "num_train_epochs: 1                    # number of training epochs\n",
    "optim: adamw_torch                     # use torch adamw optimizer\n",
    "logging_steps: 10                      # log every 10 steps\n",
    "save_strategy: epoch                   # save checkpoint every epoch\n",
    "evaluation_strategy: epoch             # evaluate every epoch\n",
    "max_grad_norm: 0.3                     # max gradient norm\n",
    "warmup_ratio: 0.03                     # warmup ratio\n",
    "bf16: true                             # use bfloat16 precision\n",
    "tf32: true                             # use tf32 precision\n",
    "gradient_checkpointing: true          # use gradient checkpointing to save memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c851f75d",
   "metadata": {},
   "source": [
    "## 3. ÌõàÎ†® Script Ïã§Ìñâ\n",
    "\n",
    "ÏïÑÎûòÎäî Hugging Face Ïùò Accelerator Í∏∞Î∞ò ÌïôÏäµ Î™ÖÎ†πÏñ¥ ÏûÖÎãàÎã§.\n",
    "- ÌòÑÏû¨ Î®∏Ïã†Ïóê 4Í∞úÏùò GPU Í∞Ä ÏûàÎäî Í≤ΩÏö∞ ÏûÖÎãàÎã§. GPU Í∞Ä 1Í∞ú Ïù¥Î©¥ nproc_per_node=1 Î°ú ÏàòÏ†ïÌï¥ÏÑú Ïã§Ìñâ ÌïòÏÑ∏Ïöî. \n",
    "\n",
    "```\n",
    "!torchrun --nproc_per_node=4 \\\n",
    "../../scripts/local_run_qlora.py \\\n",
    "--config config_folder_name/local_llama_3_8b_qlora.yaml\n",
    "```\n",
    "- Ï∞∏Í≥†\n",
    "    - Launching your ü§ó Accelerate scripts, [Link](https://huggingface.co/docs/accelerate/en/basic_tutorials/launch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "650a43f8-666d-4176-94b5-885f2bcb58cd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "config_folder_name = \"accelerator_config\"\n",
    "os.makedirs(config_folder_name, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cb9be15-e2bc-4b03-a80b-8849e77d9e19",
   "metadata": {},
   "source": [
    "### Hugging Face  Accelerator Ïóê Ï†úÍ≥µÌï† config.yaml ÏûÖÎãàÎã§."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccd6de05-6af0-45d0-8ecf-68e796771b96",
   "metadata": {},
   "source": [
    "### IMPORTANT!! Set use reentrant to False when we don't use FSDP\n",
    "```\n",
    "if training_args.gradient_checkpointing:\n",
    "    training_args.gradient_checkpointing_kwargs = {\"use_reentrant\":False**}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9f81f6fe-4e96-4a6a-a354-d89d2f248351",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W0928 02:15:57.183000 140697813051200 torch/distributed/run.py:779] \n",
      "W0928 02:15:57.183000 140697813051200 torch/distributed/run.py:779] *****************************************\n",
      "W0928 02:15:57.183000 140697813051200 torch/distributed/run.py:779] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "W0928 02:15:57.183000 140697813051200 torch/distributed/run.py:779] *****************************************\n",
      "/home/ec2-user/anaconda3/lib/python3.10/site-packages/transformers/training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/home/ec2-user/anaconda3/lib/python3.10/site-packages/transformers/training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/home/ec2-user/anaconda3/lib/python3.10/site-packages/transformers/training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/home/ec2-user/anaconda3/lib/python3.10/site-packages/transformers/training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "## script_args: \n",
      " ScriptArguments(train_dataset_path='../data/naver-news-summarization-ko/train', validation_dataset_path='../data/naver-news-summarization-ko/validation', model_id='meta-llama/Llama-3.2-1B-Instruct', max_seq_length=2048)\n",
      "## training_args: \n",
      " TrainingArguments(\n",
      "_n_gpu=1,\n",
      "accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "auto_find_batch_size=False,\n",
      "batch_eval_metrics=False,\n",
      "bf16=True,\n",
      "bf16_full_eval=False,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_persistent_workers=False,\n",
      "dataloader_pin_memory=True,\n",
      "dataloader_prefetch_factor=None,\n",
      "ddp_backend=None,\n",
      "ddp_broadcast_buffers=None,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "ddp_timeout=1800,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "dispatch_batches=None,\n",
      "do_eval=True,\n",
      "do_predict=False,\n",
      "do_train=False,\n",
      "eval_accumulation_steps=None,\n",
      "eval_delay=0,\n",
      "eval_do_concat_batches=True,\n",
      "eval_on_start=False,\n",
      "eval_steps=None,\n",
      "eval_strategy=epoch,\n",
      "eval_use_gather_object=False,\n",
      "evaluation_strategy=epoch,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "fsdp=[],\n",
      "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\n",
      "fsdp_min_num_params=0,\n",
      "fsdp_transformer_layer_cls_to_wrap=None,\n",
      "full_determinism=False,\n",
      "gradient_accumulation_steps=1,\n",
      "gradient_checkpointing=True,\n",
      "gradient_checkpointing_kwargs=None,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_always_push=False,\n",
      "hub_model_id=None,\n",
      "hub_private_repo=False,\n",
      "hub_strategy=every_save,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "include_inputs_for_metrics=False,\n",
      "include_num_input_tokens_seen=False,\n",
      "include_tokens_per_second=False,\n",
      "jit_mode_eval=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=0.0002,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=0,\n",
      "log_level=passive,\n",
      "log_level_replica=warning,\n",
      "log_on_each_node=True,\n",
      "logging_dir=/home/ec2-user/SageMaker/logs/llama-3-8b-fsdp-qlora-naver-news,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=10,\n",
      "logging_strategy=steps,\n",
      "lr_scheduler_kwargs={},\n",
      "lr_scheduler_type=constant,\n",
      "max_grad_norm=0.3,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "neftune_noise_alpha=None,\n",
      "no_cuda=False,\n",
      "num_train_epochs=1,\n",
      "optim=adamw_torch,\n",
      "optim_args=None,\n",
      "optim_target_modules=None,\n",
      "output_dir=/home/ec2-user/SageMaker/models/llama-3-8b-fsdp-qlora-naver-news,\n",
      "overwrite_output_dir=False,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=1,\n",
      "per_device_train_batch_size=1,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "ray_scope=last,\n",
      "remove_unused_columns=True,\n",
      "report_to=['tensorboard'],\n",
      "restore_callback_states_from_checkpoint=False,\n",
      "resume_from_checkpoint=None,\n",
      "run_name=/home/ec2-user/SageMaker/models/llama-3-8b-fsdp-qlora-naver-news,\n",
      "save_on_each_node=False,\n",
      "save_only_model=False,\n",
      "save_safetensors=True,\n",
      "save_steps=500,\n",
      "save_strategy=epoch,\n",
      "save_total_limit=None,\n",
      "seed=42,\n",
      "skip_memory_metrics=True,\n",
      "split_batches=None,\n",
      "tf32=True,\n",
      "torch_compile=False,\n",
      "torch_compile_backend=None,\n",
      "torch_compile_mode=None,\n",
      "torch_empty_cache_steps=None,\n",
      "torchdynamo=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_cpu=False,\n",
      "use_ipex=False,\n",
      "use_legacy_prediction_loop=False,\n",
      "use_liger_kernel=False,\n",
      "use_mps_device=False,\n",
      "warmup_ratio=0.03,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      ")\n",
      "[rank0]: Traceback (most recent call last):\n",
      "[rank0]:   File \"/home/ec2-user/anaconda3/lib/python3.10/site-packages/huggingface_hub/utils/_http.py\", line 406, in hf_raise_for_status\n",
      "[rank0]:     response.raise_for_status()\n",
      "[rank0]:   File \"/home/ec2-user/anaconda3/lib/python3.10/site-packages/requests/models.py\", line 1024, in raise_for_status\n",
      "[rank0]:     raise HTTPError(http_error_msg, response=self)\n",
      "[rank0]: requests.exceptions.HTTPError: 403 Client Error: Forbidden for url: https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct/resolve/main/config.json\n",
      "\n",
      "[rank0]: The above exception was the direct cause of the following exception:\n",
      "\n",
      "[rank0]: Traceback (most recent call last):\n",
      "[rank0]:   File \"/home/ec2-user/anaconda3/lib/python3.10/site-packages/transformers/utils/hub.py\", line 403, in cached_file\n",
      "[rank0]:     resolved_file = hf_hub_download(\n",
      "[rank0]:   File \"/home/ec2-user/anaconda3/lib/python3.10/site-packages/huggingface_hub/utils/_deprecation.py\", line 101, in inner_f\n",
      "[rank0]:     return f(*args, **kwargs)\n",
      "[rank0]:   File \"/home/ec2-user/anaconda3/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py\", line 114, in _inner_fn\n",
      "[rank0]:     return fn(*args, **kwargs)\n",
      "[rank0]:   File \"/home/ec2-user/anaconda3/lib/python3.10/site-packages/huggingface_hub/file_download.py\", line 1232, in hf_hub_download\n",
      "[rank0]:     return _hf_hub_download_to_cache_dir(\n",
      "[rank0]:   File \"/home/ec2-user/anaconda3/lib/python3.10/site-packages/huggingface_hub/file_download.py\", line 1339, in _hf_hub_download_to_cache_dir\n",
      "[rank0]:     _raise_on_head_call_error(head_call_error, force_download, local_files_only)\n",
      "[rank0]:   File \"/home/ec2-user/anaconda3/lib/python3.10/site-packages/huggingface_hub/file_download.py\", line 1854, in _raise_on_head_call_error\n",
      "[rank0]:     raise head_call_error\n",
      "[rank0]:   File \"/home/ec2-user/anaconda3/lib/python3.10/site-packages/huggingface_hub/file_download.py\", line 1746, in _get_metadata_or_catch_error\n",
      "[rank0]:     metadata = get_hf_file_metadata(\n",
      "[rank0]:   File \"/home/ec2-user/anaconda3/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py\", line 114, in _inner_fn\n",
      "[rank0]:     return fn(*args, **kwargs)\n",
      "[rank0]:   File \"/home/ec2-user/anaconda3/lib/python3.10/site-packages/huggingface_hub/file_download.py\", line 1666, in get_hf_file_metadata\n",
      "[rank0]:     r = _request_wrapper(\n",
      "[rank0]:   File \"/home/ec2-user/anaconda3/lib/python3.10/site-packages/huggingface_hub/file_download.py\", line 364, in _request_wrapper\n",
      "[rank0]:     response = _request_wrapper(\n",
      "[rank0]:   File \"/home/ec2-user/anaconda3/lib/python3.10/site-packages/huggingface_hub/file_download.py\", line 388, in _request_wrapper\n",
      "[rank0]:     hf_raise_for_status(response)\n",
      "[rank0]:   File \"/home/ec2-user/anaconda3/lib/python3.10/site-packages/huggingface_hub/utils/_http.py\", line 423, in hf_raise_for_status\n",
      "[rank0]:     raise _format(GatedRepoError, message, response) from e\n",
      "[rank0]: huggingface_hub.errors.GatedRepoError: 403 Client Error. (Request ID: Root=1-66f766e1-0b0787204233ad86051c9e1a;eba4be2a-a796-4f46-84b1-9eeab2f85f51)\n",
      "\n",
      "[rank0]: Cannot access gated repo for url https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct/resolve/main/config.json.\n",
      "[rank0]: Access to model meta-llama/Llama-3.2-1B-Instruct is restricted and you are not in the authorized list. Visit https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct to ask for access.\n",
      "\n",
      "[rank0]: The above exception was the direct cause of the following exception:\n",
      "\n",
      "[rank0]: Traceback (most recent call last):\n",
      "[rank0]:   File \"/home/ec2-user/SageMaker/aws-ai-ml-workshop-kr/genai/aws-gen-ai-kr/30_fine_tune/03-fine-tune-llama3/llama3/notebook/11-naver-news-QLoRA/../../scripts/local_run_qlora.py\", line 194, in <module>\n",
      "[rank0]:     training_function(script_args, training_args)\n",
      "[rank0]:   File \"/home/ec2-user/SageMaker/aws-ai-ml-workshop-kr/genai/aws-gen-ai-kr/30_fine_tune/03-fine-tune-llama3/llama3/notebook/11-naver-news-QLoRA/../../scripts/local_run_qlora.py\", line 84, in training_function\n",
      "[rank0]:     tokenizer = AutoTokenizer.from_pretrained(script_args.model_id, use_fast=True)\n",
      "[rank0]:   File \"/home/ec2-user/anaconda3/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py\", line 864, in from_pretrained\n",
      "[rank0]:     config = AutoConfig.from_pretrained(\n",
      "[rank0]:   File \"/home/ec2-user/anaconda3/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py\", line 1006, in from_pretrained\n",
      "[rank0]:     config_dict, unused_kwargs = PretrainedConfig.get_config_dict(pretrained_model_name_or_path, **kwargs)\n",
      "[rank0]:   File \"/home/ec2-user/anaconda3/lib/python3.10/site-packages/transformers/configuration_utils.py\", line 567, in get_config_dict\n",
      "[rank0]:     config_dict, kwargs = cls._get_config_dict(pretrained_model_name_or_path, **kwargs)\n",
      "[rank0]:   File \"/home/ec2-user/anaconda3/lib/python3.10/site-packages/transformers/configuration_utils.py\", line 626, in _get_config_dict\n",
      "[rank0]:     resolved_config_file = cached_file(\n",
      "[rank0]:   File \"/home/ec2-user/anaconda3/lib/python3.10/site-packages/transformers/utils/hub.py\", line 421, in cached_file\n",
      "[rank0]:     raise EnvironmentError(\n",
      "[rank0]: OSError: You are trying to access a gated repo.\n",
      "[rank0]: Make sure to have access to it at https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct.\n",
      "[rank0]: 403 Client Error. (Request ID: Root=1-66f766e1-0b0787204233ad86051c9e1a;eba4be2a-a796-4f46-84b1-9eeab2f85f51)\n",
      "\n",
      "[rank0]: Cannot access gated repo for url https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct/resolve/main/config.json.\n",
      "[rank0]: Access to model meta-llama/Llama-3.2-1B-Instruct is restricted and you are not in the authorized list. Visit https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct to ask for access.\n",
      "[rank3]: Traceback (most recent call last):\n",
      "[rank3]:   File \"/home/ec2-user/anaconda3/lib/python3.10/site-packages/huggingface_hub/utils/_http.py\", line 406, in hf_raise_for_status\n",
      "[rank3]:     response.raise_for_status()\n",
      "[rank3]:   File \"/home/ec2-user/anaconda3/lib/python3.10/site-packages/requests/models.py\", line 1024, in raise_for_status\n",
      "[rank3]:     raise HTTPError(http_error_msg, response=self)\n",
      "[rank3]: requests.exceptions.HTTPError: 403 Client Error: Forbidden for url: https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct/resolve/main/config.json\n",
      "\n",
      "[rank3]: The above exception was the direct cause of the following exception:\n",
      "\n",
      "[rank3]: Traceback (most recent call last):\n",
      "[rank3]:   File \"/home/ec2-user/anaconda3/lib/python3.10/site-packages/transformers/utils/hub.py\", line 403, in cached_file\n",
      "[rank3]:     resolved_file = hf_hub_download(\n",
      "[rank3]:   File \"/home/ec2-user/anaconda3/lib/python3.10/site-packages/huggingface_hub/utils/_deprecation.py\", line 101, in inner_f\n",
      "[rank3]:     return f(*args, **kwargs)\n",
      "[rank3]:   File \"/home/ec2-user/anaconda3/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py\", line 114, in _inner_fn\n",
      "[rank3]:     return fn(*args, **kwargs)\n",
      "[rank3]:   File \"/home/ec2-user/anaconda3/lib/python3.10/site-packages/huggingface_hub/file_download.py\", line 1232, in hf_hub_download\n",
      "[rank3]:     return _hf_hub_download_to_cache_dir(\n",
      "[rank3]:   File \"/home/ec2-user/anaconda3/lib/python3.10/site-packages/huggingface_hub/file_download.py\", line 1339, in _hf_hub_download_to_cache_dir\n",
      "[rank3]:     _raise_on_head_call_error(head_call_error, force_download, local_files_only)\n",
      "[rank3]:   File \"/home/ec2-user/anaconda3/lib/python3.10/site-packages/huggingface_hub/file_download.py\", line 1854, in _raise_on_head_call_error\n",
      "[rank3]:     raise head_call_error\n",
      "[rank3]:   File \"/home/ec2-user/anaconda3/lib/python3.10/site-packages/huggingface_hub/file_download.py\", line 1746, in _get_metadata_or_catch_error\n",
      "[rank3]:     metadata = get_hf_file_metadata(\n",
      "[rank3]:   File \"/home/ec2-user/anaconda3/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py\", line 114, in _inner_fn\n",
      "[rank3]:     return fn(*args, **kwargs)\n",
      "[rank3]:   File \"/home/ec2-user/anaconda3/lib/python3.10/site-packages/huggingface_hub/file_download.py\", line 1666, in get_hf_file_metadata\n",
      "[rank3]:     r = _request_wrapper(\n",
      "[rank3]:   File \"/home/ec2-user/anaconda3/lib/python3.10/site-packages/huggingface_hub/file_download.py\", line 364, in _request_wrapper\n",
      "[rank3]:     response = _request_wrapper(\n",
      "[rank3]:   File \"/home/ec2-user/anaconda3/lib/python3.10/site-packages/huggingface_hub/file_download.py\", line 388, in _request_wrapper\n",
      "[rank3]:     hf_raise_for_status(response)\n",
      "[rank3]:   File \"/home/ec2-user/anaconda3/lib/python3.10/site-packages/huggingface_hub/utils/_http.py\", line 423, in hf_raise_for_status\n",
      "[rank3]:     raise _format(GatedRepoError, message, response) from e\n",
      "[rank3]: huggingface_hub.errors.GatedRepoError: 403 Client Error. (Request ID: Root=1-66f766e1-017b80fc70f31d232330d6d2;1179743a-a942-4f43-acd8-f1dbac7421fa)\n",
      "\n",
      "[rank3]: Cannot access gated repo for url https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct/resolve/main/config.json.\n",
      "[rank3]: Access to model meta-llama/Llama-3.2-1B-Instruct is restricted and you are not in the authorized list. Visit https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct to ask for access.\n",
      "\n",
      "[rank3]: The above exception was the direct cause of the following exception:\n",
      "\n",
      "[rank3]: Traceback (most recent call last):\n",
      "[rank3]:   File \"/home/ec2-user/SageMaker/aws-ai-ml-workshop-kr/genai/aws-gen-ai-kr/30_fine_tune/03-fine-tune-llama3/llama3/notebook/11-naver-news-QLoRA/../../scripts/local_run_qlora.py\", line 194, in <module>\n",
      "[rank3]:     training_function(script_args, training_args)\n",
      "[rank3]:   File \"/home/ec2-user/SageMaker/aws-ai-ml-workshop-kr/genai/aws-gen-ai-kr/30_fine_tune/03-fine-tune-llama3/llama3/notebook/11-naver-news-QLoRA/../../scripts/local_run_qlora.py\", line 84, in training_function\n",
      "[rank3]:     tokenizer = AutoTokenizer.from_pretrained(script_args.model_id, use_fast=True)\n",
      "[rank3]:   File \"/home/ec2-user/anaconda3/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py\", line 864, in from_pretrained\n",
      "[rank3]:     config = AutoConfig.from_pretrained(\n",
      "[rank3]:   File \"/home/ec2-user/anaconda3/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py\", line 1006, in from_pretrained\n",
      "[rank3]:     config_dict, unused_kwargs = PretrainedConfig.get_config_dict(pretrained_model_name_or_path, **kwargs)\n",
      "[rank3]:   File \"/home/ec2-user/anaconda3/lib/python3.10/site-packages/transformers/configuration_utils.py\", line 567, in get_config_dict\n",
      "[rank3]:     config_dict, kwargs = cls._get_config_dict(pretrained_model_name_or_path, **kwargs)\n",
      "[rank3]:   File \"/home/ec2-user/anaconda3/lib/python3.10/site-packages/transformers/configuration_utils.py\", line 626, in _get_config_dict\n",
      "[rank3]:     resolved_config_file = cached_file(\n",
      "[rank3]:   File \"/home/ec2-user/anaconda3/lib/python3.10/site-packages/transformers/utils/hub.py\", line 421, in cached_file\n",
      "[rank3]:     raise EnvironmentError(\n",
      "[rank3]: OSError: You are trying to access a gated repo.\n",
      "[rank3]: Make sure to have access to it at https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct.\n",
      "[rank3]: 403 Client Error. (Request ID: Root=1-66f766e1-017b80fc70f31d232330d6d2;1179743a-a942-4f43-acd8-f1dbac7421fa)\n",
      "\n",
      "[rank3]: Cannot access gated repo for url https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct/resolve/main/config.json.\n",
      "[rank3]: Access to model meta-llama/Llama-3.2-1B-Instruct is restricted and you are not in the authorized list. Visit https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct to ask for access.\n",
      "[rank2]: Traceback (most recent call last):\n",
      "[rank2]:   File \"/home/ec2-user/anaconda3/lib/python3.10/site-packages/huggingface_hub/utils/_http.py\", line 406, in hf_raise_for_status\n",
      "[rank2]:     response.raise_for_status()\n",
      "[rank2]:   File \"/home/ec2-user/anaconda3/lib/python3.10/site-packages/requests/models.py\", line 1024, in raise_for_status\n",
      "[rank2]:     raise HTTPError(http_error_msg, response=self)\n",
      "[rank2]: requests.exceptions.HTTPError: 403 Client Error: Forbidden for url: https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct/resolve/main/config.json\n",
      "\n",
      "[rank2]: The above exception was the direct cause of the following exception:\n",
      "\n",
      "[rank2]: Traceback (most recent call last):\n",
      "[rank2]:   File \"/home/ec2-user/anaconda3/lib/python3.10/site-packages/transformers/utils/hub.py\", line 403, in cached_file\n",
      "[rank2]:     resolved_file = hf_hub_download(\n",
      "[rank2]:   File \"/home/ec2-user/anaconda3/lib/python3.10/site-packages/huggingface_hub/utils/_deprecation.py\", line 101, in inner_f\n",
      "[rank2]:     return f(*args, **kwargs)\n",
      "[rank2]:   File \"/home/ec2-user/anaconda3/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py\", line 114, in _inner_fn\n",
      "[rank2]:     return fn(*args, **kwargs)\n",
      "[rank2]:   File \"/home/ec2-user/anaconda3/lib/python3.10/site-packages/huggingface_hub/file_download.py\", line 1232, in hf_hub_download\n",
      "[rank2]:     return _hf_hub_download_to_cache_dir(\n",
      "[rank2]:   File \"/home/ec2-user/anaconda3/lib/python3.10/site-packages/huggingface_hub/file_download.py\", line 1339, in _hf_hub_download_to_cache_dir\n",
      "[rank2]:     _raise_on_head_call_error(head_call_error, force_download, local_files_only)\n",
      "[rank2]:   File \"/home/ec2-user/anaconda3/lib/python3.10/site-packages/huggingface_hub/file_download.py\", line 1854, in _raise_on_head_call_error\n",
      "[rank2]:     raise head_call_error\n",
      "[rank2]:   File \"/home/ec2-user/anaconda3/lib/python3.10/site-packages/huggingface_hub/file_download.py\", line 1746, in _get_metadata_or_catch_error\n",
      "[rank2]:     metadata = get_hf_file_metadata(\n",
      "[rank2]:   File \"/home/ec2-user/anaconda3/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py\", line 114, in _inner_fn\n",
      "[rank2]:     return fn(*args, **kwargs)\n",
      "[rank2]:   File \"/home/ec2-user/anaconda3/lib/python3.10/site-packages/huggingface_hub/file_download.py\", line 1666, in get_hf_file_metadata\n",
      "[rank2]:     r = _request_wrapper(\n",
      "[rank2]:   File \"/home/ec2-user/anaconda3/lib/python3.10/site-packages/huggingface_hub/file_download.py\", line 364, in _request_wrapper\n",
      "[rank2]:     response = _request_wrapper(\n",
      "[rank2]:   File \"/home/ec2-user/anaconda3/lib/python3.10/site-packages/huggingface_hub/file_download.py\", line 388, in _request_wrapper\n",
      "[rank2]:     hf_raise_for_status(response)\n",
      "[rank2]:   File \"/home/ec2-user/anaconda3/lib/python3.10/site-packages/huggingface_hub/utils/_http.py\", line 423, in hf_raise_for_status\n",
      "[rank2]:     raise _format(GatedRepoError, message, response) from e\n",
      "[rank2]: huggingface_hub.errors.GatedRepoError: 403 Client Error. (Request ID: Root=1-66f766e2-2a6ad4f60016f91d11130497;f6e8ed6b-c5b0-4ee3-a71f-6f9e60f708d8)\n",
      "\n",
      "[rank2]: Cannot access gated repo for url https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct/resolve/main/config.json.\n",
      "[rank2]: Access to model meta-llama/Llama-3.2-1B-Instruct is restricted and you are not in the authorized list. Visit https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct to ask for access.\n",
      "\n",
      "[rank2]: The above exception was the direct cause of the following exception:\n",
      "\n",
      "[rank2]: Traceback (most recent call last):\n",
      "[rank2]:   File \"/home/ec2-user/SageMaker/aws-ai-ml-workshop-kr/genai/aws-gen-ai-kr/30_fine_tune/03-fine-tune-llama3/llama3/notebook/11-naver-news-QLoRA/../../scripts/local_run_qlora.py\", line 194, in <module>\n",
      "[rank2]:     training_function(script_args, training_args)\n",
      "[rank2]:   File \"/home/ec2-user/SageMaker/aws-ai-ml-workshop-kr/genai/aws-gen-ai-kr/30_fine_tune/03-fine-tune-llama3/llama3/notebook/11-naver-news-QLoRA/../../scripts/local_run_qlora.py\", line 84, in training_function\n",
      "[rank2]:     tokenizer = AutoTokenizer.from_pretrained(script_args.model_id, use_fast=True)\n",
      "[rank2]:   File \"/home/ec2-user/anaconda3/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py\", line 864, in from_pretrained\n",
      "[rank2]:     config = AutoConfig.from_pretrained(\n",
      "[rank2]:   File \"/home/ec2-user/anaconda3/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py\", line 1006, in from_pretrained\n",
      "[rank2]:     config_dict, unused_kwargs = PretrainedConfig.get_config_dict(pretrained_model_name_or_path, **kwargs)\n",
      "[rank2]:   File \"/home/ec2-user/anaconda3/lib/python3.10/site-packages/transformers/configuration_utils.py\", line 567, in get_config_dict\n",
      "[rank2]:     config_dict, kwargs = cls._get_config_dict(pretrained_model_name_or_path, **kwargs)\n",
      "[rank2]:   File \"/home/ec2-user/anaconda3/lib/python3.10/site-packages/transformers/configuration_utils.py\", line 626, in _get_config_dict\n",
      "[rank2]:     resolved_config_file = cached_file(\n",
      "[rank2]:   File \"/home/ec2-user/anaconda3/lib/python3.10/site-packages/transformers/utils/hub.py\", line 421, in cached_file\n",
      "[rank2]:     raise EnvironmentError(\n",
      "[rank2]: OSError: You are trying to access a gated repo.\n",
      "[rank2]: Make sure to have access to it at https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct.\n",
      "[rank2]: 403 Client Error. (Request ID: Root=1-66f766e2-2a6ad4f60016f91d11130497;f6e8ed6b-c5b0-4ee3-a71f-6f9e60f708d8)\n",
      "\n",
      "[rank2]: Cannot access gated repo for url https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct/resolve/main/config.json.\n",
      "[rank2]: Access to model meta-llama/Llama-3.2-1B-Instruct is restricted and you are not in the authorized list. Visit https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct to ask for access.\n",
      "[rank1]: Traceback (most recent call last):\n",
      "[rank1]:   File \"/home/ec2-user/anaconda3/lib/python3.10/site-packages/huggingface_hub/utils/_http.py\", line 406, in hf_raise_for_status\n",
      "[rank1]:     response.raise_for_status()\n",
      "[rank1]:   File \"/home/ec2-user/anaconda3/lib/python3.10/site-packages/requests/models.py\", line 1024, in raise_for_status\n",
      "[rank1]:     raise HTTPError(http_error_msg, response=self)\n",
      "[rank1]: requests.exceptions.HTTPError: 403 Client Error: Forbidden for url: https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct/resolve/main/config.json\n",
      "\n",
      "[rank1]: The above exception was the direct cause of the following exception:\n",
      "\n",
      "[rank1]: Traceback (most recent call last):\n",
      "[rank1]:   File \"/home/ec2-user/anaconda3/lib/python3.10/site-packages/transformers/utils/hub.py\", line 403, in cached_file\n",
      "[rank1]:     resolved_file = hf_hub_download(\n",
      "[rank1]:   File \"/home/ec2-user/anaconda3/lib/python3.10/site-packages/huggingface_hub/utils/_deprecation.py\", line 101, in inner_f\n",
      "[rank1]:     return f(*args, **kwargs)\n",
      "[rank1]:   File \"/home/ec2-user/anaconda3/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py\", line 114, in _inner_fn\n",
      "[rank1]:     return fn(*args, **kwargs)\n",
      "[rank1]:   File \"/home/ec2-user/anaconda3/lib/python3.10/site-packages/huggingface_hub/file_download.py\", line 1232, in hf_hub_download\n",
      "[rank1]:     return _hf_hub_download_to_cache_dir(\n",
      "[rank1]:   File \"/home/ec2-user/anaconda3/lib/python3.10/site-packages/huggingface_hub/file_download.py\", line 1339, in _hf_hub_download_to_cache_dir\n",
      "[rank1]:     _raise_on_head_call_error(head_call_error, force_download, local_files_only)\n",
      "[rank1]:   File \"/home/ec2-user/anaconda3/lib/python3.10/site-packages/huggingface_hub/file_download.py\", line 1854, in _raise_on_head_call_error\n",
      "[rank1]:     raise head_call_error\n",
      "[rank1]:   File \"/home/ec2-user/anaconda3/lib/python3.10/site-packages/huggingface_hub/file_download.py\", line 1746, in _get_metadata_or_catch_error\n",
      "[rank1]:     metadata = get_hf_file_metadata(\n",
      "[rank1]:   File \"/home/ec2-user/anaconda3/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py\", line 114, in _inner_fn\n",
      "[rank1]:     return fn(*args, **kwargs)\n",
      "[rank1]:   File \"/home/ec2-user/anaconda3/lib/python3.10/site-packages/huggingface_hub/file_download.py\", line 1666, in get_hf_file_metadata\n",
      "[rank1]:     r = _request_wrapper(\n",
      "[rank1]:   File \"/home/ec2-user/anaconda3/lib/python3.10/site-packages/huggingface_hub/file_download.py\", line 364, in _request_wrapper\n",
      "[rank1]:     response = _request_wrapper(\n",
      "[rank1]:   File \"/home/ec2-user/anaconda3/lib/python3.10/site-packages/huggingface_hub/file_download.py\", line 388, in _request_wrapper\n",
      "[rank1]:     hf_raise_for_status(response)\n",
      "[rank1]:   File \"/home/ec2-user/anaconda3/lib/python3.10/site-packages/huggingface_hub/utils/_http.py\", line 423, in hf_raise_for_status\n",
      "[rank1]:     raise _format(GatedRepoError, message, response) from e\n",
      "[rank1]: huggingface_hub.errors.GatedRepoError: 403 Client Error. (Request ID: Root=1-66f766e2-0d23b0f065c7576b40544ffd;0578c8ca-97e0-4219-91e3-596713e2bfb4)\n",
      "\n",
      "[rank1]: Cannot access gated repo for url https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct/resolve/main/config.json.\n",
      "[rank1]: Access to model meta-llama/Llama-3.2-1B-Instruct is restricted and you are not in the authorized list. Visit https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct to ask for access.\n",
      "\n",
      "[rank1]: The above exception was the direct cause of the following exception:\n",
      "\n",
      "[rank1]: Traceback (most recent call last):\n",
      "[rank1]:   File \"/home/ec2-user/SageMaker/aws-ai-ml-workshop-kr/genai/aws-gen-ai-kr/30_fine_tune/03-fine-tune-llama3/llama3/notebook/11-naver-news-QLoRA/../../scripts/local_run_qlora.py\", line 194, in <module>\n",
      "[rank1]:     training_function(script_args, training_args)\n",
      "[rank1]:   File \"/home/ec2-user/SageMaker/aws-ai-ml-workshop-kr/genai/aws-gen-ai-kr/30_fine_tune/03-fine-tune-llama3/llama3/notebook/11-naver-news-QLoRA/../../scripts/local_run_qlora.py\", line 84, in training_function\n",
      "[rank1]:     tokenizer = AutoTokenizer.from_pretrained(script_args.model_id, use_fast=True)\n",
      "[rank1]:   File \"/home/ec2-user/anaconda3/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py\", line 864, in from_pretrained\n",
      "[rank1]:     config = AutoConfig.from_pretrained(\n",
      "[rank1]:   File \"/home/ec2-user/anaconda3/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py\", line 1006, in from_pretrained\n",
      "[rank1]:     config_dict, unused_kwargs = PretrainedConfig.get_config_dict(pretrained_model_name_or_path, **kwargs)\n",
      "[rank1]:   File \"/home/ec2-user/anaconda3/lib/python3.10/site-packages/transformers/configuration_utils.py\", line 567, in get_config_dict\n",
      "[rank1]:     config_dict, kwargs = cls._get_config_dict(pretrained_model_name_or_path, **kwargs)\n",
      "[rank1]:   File \"/home/ec2-user/anaconda3/lib/python3.10/site-packages/transformers/configuration_utils.py\", line 626, in _get_config_dict\n",
      "[rank1]:     resolved_config_file = cached_file(\n",
      "[rank1]:   File \"/home/ec2-user/anaconda3/lib/python3.10/site-packages/transformers/utils/hub.py\", line 421, in cached_file\n",
      "[rank1]:     raise EnvironmentError(\n",
      "[rank1]: OSError: You are trying to access a gated repo.\n",
      "[rank1]: Make sure to have access to it at https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct.\n",
      "[rank1]: 403 Client Error. (Request ID: Root=1-66f766e2-0d23b0f065c7576b40544ffd;0578c8ca-97e0-4219-91e3-596713e2bfb4)\n",
      "\n",
      "[rank1]: Cannot access gated repo for url https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct/resolve/main/config.json.\n",
      "[rank1]: Access to model meta-llama/Llama-3.2-1B-Instruct is restricted and you are not in the authorized list. Visit https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct to ask for access.\n",
      "W0928 02:16:02.503000 140697813051200 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 10479 closing signal SIGTERM\n",
      "W0928 02:16:02.503000 140697813051200 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 10480 closing signal SIGTERM\n",
      "W0928 02:16:02.503000 140697813051200 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 10482 closing signal SIGTERM\n",
      "E0928 02:16:02.983000 140697813051200 torch/distributed/elastic/multiprocessing/api.py:833] failed (exitcode: 1) local_rank: 0 (pid: 10478) of binary: /home/ec2-user/anaconda3/bin/python3.10\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ec2-user/anaconda3/bin/torchrun\", line 8, in <module>\n",
      "    sys.exit(main())\n",
      "  File \"/home/ec2-user/anaconda3/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py\", line 348, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/ec2-user/anaconda3/lib/python3.10/site-packages/torch/distributed/run.py\", line 901, in main\n",
      "    run(args)\n",
      "  File \"/home/ec2-user/anaconda3/lib/python3.10/site-packages/torch/distributed/run.py\", line 892, in run\n",
      "    elastic_launch(\n",
      "  File \"/home/ec2-user/anaconda3/lib/python3.10/site-packages/torch/distributed/launcher/api.py\", line 133, in __call__\n",
      "    return launch_agent(self._config, self._entrypoint, list(args))\n",
      "  File \"/home/ec2-user/anaconda3/lib/python3.10/site-packages/torch/distributed/launcher/api.py\", line 264, in launch_agent\n",
      "    raise ChildFailedError(\n",
      "torch.distributed.elastic.multiprocessing.errors.ChildFailedError: \n",
      "============================================================\n",
      "../../scripts/local_run_qlora.py FAILED\n",
      "------------------------------------------------------------\n",
      "Failures:\n",
      "  <NO_OTHER_FAILURES>\n",
      "------------------------------------------------------------\n",
      "Root Cause (first observed failure):\n",
      "[0]:\n",
      "  time      : 2024-09-28_02:16:02\n",
      "  host      : ip-172-16-4-140.us-west-2.compute.internal\n",
      "  rank      : 0 (local_rank: 0)\n",
      "  exitcode  : 1 (pid: 10478)\n",
      "  error_file: <N/A>\n",
      "  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "!CUDA_VISIBLE_DEVICES=0,1,2,3 torchrun --nproc_per_node=4 \\\n",
    "../../scripts/local_run_qlora.py \\\n",
    "--config accelerator_config/local_llama_3_8b_qlora.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "441607e4-f93f-438e-8985-99a76233fe47",
   "metadata": {},
   "source": [
    "## 4. Î≤†Ïù¥Ïä§ Î™®Îç∏Í≥º ÌõàÎ†®Îêú Î™®Îç∏ Î®∏ÏßÄ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cebdb212",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_id: meta-llama/Llama-3.1-8B-Instruct\n",
      "output_dir: /home/ec2-user/SageMaker/models/llama-3-8b-qlora-naver-news\n"
     ]
    }
   ],
   "source": [
    "print (f'model_id: {model_id}')\n",
    "print (f'output_dir: {output_dir}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3a8acc4",
   "metadata": {},
   "source": [
    "### Î™®Îç∏ Î®∏ÏßÄ Î∞è Î°úÏª¨Ïóê Ï†ÄÏû•\n",
    "- ÏïΩ 2Î∂Ñ Í±∏Î¶º"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "19543636",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3aa36ec906384b3782d46906c081a7e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from peft import AutoPeftModelForCausalLM\n",
    "import torch\n",
    "\n",
    "# Load PEFT model on CPU\n",
    "\n",
    "model = AutoPeftModelForCausalLM.from_pretrained(\n",
    "    output_dir,\n",
    "    torch_dtype=torch.float16, ## why bfloat16Ïù¥ ÏïÑÎãàÏßÄ?\n",
    "    low_cpu_mem_usage=True,\n",
    ")  \n",
    "# Merge LoRA and base model and save\n",
    "merged_model = model.merge_and_unload()\n",
    "merged_model.save_pretrained(output_dir,safe_serialization=True, max_shard_size=\"2GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9d0e3827",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(device(type='cpu'), device(type='cpu'))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.device, merged_model.device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aff0ea42",
   "metadata": {},
   "source": [
    "### Î®∏ÏßÄÎêú Î™®Îç∏ Î°úÎî©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a1a6607d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7827def278f4e33b6c3b9a1f7cab31d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from peft import AutoPeftModelForCausalLM\n",
    "from transformers import AutoTokenizer \n",
    "\n",
    "\n",
    "# Load Model with PEFT adapter\n",
    "model = AutoPeftModelForCausalLM.from_pretrained(\n",
    "  pretrained_model_name_or_path = output_dir,\n",
    "  torch_dtype=torch.float16,\n",
    "  quantization_config= {\"load_in_4bit\": True},\n",
    "  device_map=\"auto\"\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6908046b",
   "metadata": {},
   "source": [
    "## 5. Ï∂îÎ°†"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5388b34",
   "metadata": {},
   "source": [
    "### ÌÖåÏä§Ìä∏ Îç∞Ïù¥ÌÑ∞ ÏÖã Î°úÎî©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9eacd377",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d8a2983e2b54ca48c88ddb6ee424762",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rand_idx:  75\n",
      "messages: \n",
      " [{'content': 'You are an AI assistant specialized in news articles.Your role is to provide accurate summaries and insights in Korean. Please analyze the given text and provide concise, informative summaries that highlight the key goals and findings.', 'role': 'system'}, {'content': 'Please summarize the goals for journalist in this text:\\n\\nÏÇ∞ÏóÖÌòÑÏû•ÏóêÏÑú ÏÑ∏Ï¢ÖÌÖîÎ†àÏΩ§Ïùò Ïä§ÎßàÌä∏ ÏïàÏ†Ñ ÌîåÎû´Ìèº ÏÜîÎ£®ÏÖò ÏùÑ ÌôúÏö©ÌïòÍ≥† ÏûàÎã§. ÏÑ∏Ï¢ÖÌÖîÎ†àÏΩ§ Ï†úÍ≥µ ÏÑ∏Ï¢ÖÌÖîÎ†àÏΩ§ÏùÄ ÌÉúÏòÅÍ±¥ÏÑ§Ïóê Ï§ëÎåÄÏû¨Ìï¥Ï≤òÎ≤åÎ≤ï ÎåÄÏùëÏóê ÏµúÏ†ÅÌôîÎêú Ïä§ÎßàÌä∏ ÏïàÏ†Ñ ÌîåÎû´Ìèº ÏÜîÎ£®ÏÖò ÎÇ©Ìíà Í≥ÑÏïΩÏùÑ Ï≤¥Í≤∞ÌñàÎã§Í≥† 4Ïùº Î∞ùÌòîÎã§. Ïö∞ÏÑ† ÌÉúÏòÅÍ±¥ÏÑ§ Ï†ÑÍµ≠ ÏÇ∞ÏóÖÌòÑÏû•Ïóê Ïä§ÎßàÌä∏ ÏïàÏ†Ñ ÏÜîÎ£®ÏÖò 500ÎåÄ ÎÇ©Ìíà Í≥ÑÏïΩÏùÑ Ï≤¥Í≤∞ÌñàÍ≥† Ï∂îÍ∞Ä Íµ¨Ï∂ïÏùÑ ÌòëÏùò Ï§ëÏù¥Îã§. ÏßÄÎÇú 1Ïõî Î≥∏Í≤© ÏãúÌñâÎêú Ï§ëÎåÄÏû¨Ìï¥Ï≤òÎ≤åÎ≤ïÏùÄ ÏÇ∞ÏóÖÌòÑÏû•ÏóêÏÑú Ïù∏Î™ÖÏÇ¨Í≥† Î∞úÏÉù Ïãú Í≤ΩÏòÅÏßÑÏù¥ÎÇò Î≤ïÏù∏ÏóêÍ≤å Ï±ÖÏûÑÏùÑ Î¨ºÏùÑ Ïàò ÏûàÎèÑÎ°ù Í∑úÏ†ïÌïú Î≤ïÏù¥Îã§. Ìï¥Îãπ Î≤ïÏùÄ ÏïàÏ†Ñ Î≥¥Í±¥ Í¥ÄÎ†® Í¥ÄÎ¶¨ÏÉÅÏùò Ï°∞Ïπò Íµ¨Ï∂ïÏùÑ ÏùòÎ¨¥ÌôîÌïòÍ≥† ÏûàÏúºÎÇò Í∏∞ÏóÖÏùò ÌïúÏ†ïÏ†Å ÏûêÏõêÍ≥º Î∂ÄÏ°±Ìïú Ïù∏Î†• Î¨∏Ï†úÎ°ú Ïñ¥Î†§ÏõÄÏùÑ Í≤™Í≥† ÏûàÎã§. ÏÑ∏Ï¢ÖÌÖîÎ†àÏΩ§Ïùò Ïä§ÎßàÌä∏ ÏïàÏ†Ñ ÌîåÎû´Ìèº ÏÜîÎ£®ÏÖòÏùÄ Ï∂úÏûÖÍ¥ÄÎ¶¨Î∂ÄÌÑ∞ CCTV Í∞ÄÏä§ÌÉêÏßÄ Í∞ÅÏ¢Ö ÏÑºÏÑú Îì±ÏùÑ ÌïòÎÇòÎ°ú ÌÜµÌï©Ìï¥ ÌòÑÏû•ÏùÑ Ï¢ÖÌï© Í¥ÄÎ¶¨Ìï† Ïàò ÏûàÎã§. LBS ÏúÑÏπòÍ∏∞Î∞ò IoT ÏÇ¨Î¨ºÏù∏ÌÑ∞ÎÑ∑ Îì± Ïä§ÎßàÌä∏ Í∏∞Ïà†ÏùÑ ÏúµÌï©ÌñàÎã§. ÏïàÏ†Ñ Í¥ÄÎ¶¨ Îã¥ÎãπÏûêÎäî Í∞Å ÌòÑÏû•ÎßàÎã§ ÏÑ§ÏπòÎêú Ïπ¥Î©îÎùº Î∞è CCTV Í∞úÏÜåÎ≥Ñ ÏÑºÏÑúÏôÄ ÌÜµÏã† Ïù∏ÌîÑÎùºÎ•º ÌÜµÌï¥ ÌòÑÏû• Ï†ïÎ≥¥Î•º Ïã§ÏãúÍ∞ÑÏúºÎ°ú ÌôïÏù∏ÌïòÍ≥† ÎπÑÏÉÅ ÏÉÅÌô© ÏãúÏóêÎäî Ï†ÑÏ≤¥ ÌòÑÏû• ÎòêÎäî Ìï¥Îãπ Íµ¨Ïó≠ ÏÉÅÌô©Ïã§ ÏãúÏä§ÌÖúÏù¥ÎÇò Î™®Î∞îÏùºÎ°ú Í∑ºÎ°úÏûêÏóêÍ≤å ÏïàÏ†Ñ Ï°∞ÏπòÏÇ¨Ìï≠ÏùÑ ÏßÄÏãúÌï† Ïàò ÏûàÎã§. Ïù¥ÏôÄ Ìï®Íªò ÌÉÄÏõåÌÅ¨Î†àÏù∏Ïóê ÏÑ§ÏπòÌïú 360ÎèÑ Ïπ¥Î©îÎùºÎ•º ÌÜµÌï¥ ÌòÑÏû•Ïùò Î∂àÏïàÏ†Ñ ÏöîÏÜåÎ•º Î∞úÍ≤¨ÌïòÎ©¥ Í¥ÄÍ≥ÑÏûêÏóêÍ≤å ÏïåÎ¶ºÏùÑ Î≥¥ÎÇº Ïàò ÏûàÎã§. ÏßÄÌïò ÏûëÏóÖÏóêÏÑúÎäî Ïù¥ÎèôÌòï Ïä§ÎßàÌä∏ ÏòÅÏÉÅ Ïû•ÎπÑÎ°ú ÏïàÏ†Ñ ÏÇ¨Í∞ÅÏßÄÎåÄÎ•º ÏÇ¥ÌïÑ Ïàò ÏûàÍ≥† Î∞ÄÌèêÎêú ÏûëÏóÖ Í≥µÍ∞ÑÏóêÏÑúÎäî Í∞ÄÏä§ ÏÑºÏÑúÏôÄ Ïã†Ìò∏Îì±Ìòï Ï†ÑÍ¥ëÌåêÏùÑ ÏÑ§ÏπòÌï¥ Ïã§ÏãúÍ∞ÑÏúºÎ°ú Ïä§ÎßàÌä∏ ÏÉÅÌô©ÌåêÏóê Í∞ÄÏä§ ÎÜçÎèÑÎ•º Ï†ÑÏÜ°ÌïúÎã§. Ïú†Ìï¥Í∞ÄÏä§Í∞Ä ÌóàÏö© ÎÜçÎèÑÎ•º Ï¥àÍ≥ºÌïòÎ©¥ ÌòÑÏû•ÏóêÏÑú ÌôòÍ∏∞ ÏãúÏä§ÌÖúÏù¥ ÏûêÎèôÏúºÎ°ú ÏûëÎèôÌïúÎã§. ÌòÑÏû• ÎÇ¥ Ï∂îÎùΩ ÏÇ¨Í≥†Í∞Ä Î∞úÏÉùÌï† Ïàò ÏûàÎäî Í∞úÍµ¨Î∂ÄÏóê Î∂ÄÏ∞©Ìïú ÏÑºÏÑúÎäî Í∞úÍµ¨Î∂ÄÍ∞Ä ÎπÑÏ†ïÏÉÅÏ†ÅÏúºÎ°ú Í∞úÌèêÎêêÏùÑ Îïå Í≤ΩÍ≥†ÏùåÏùÑ Î≥¥ÎÇ¥ ÏúÑÌóòÏÉÅÌô©ÏùÑ ÏïåÎ¶∞Îã§. Í∞ïÌö®ÏÉÅ ÏÑ∏Ï¢ÖÌÖîÎ†àÏΩ§ ÌÜµÏã†ÏÇ¨ÏóÖÎ≥∏Î∂ÄÏû•ÏùÄ Ï§ëÎåÄÏû¨Ìï¥Ï≤òÎ≤åÎ≤ï ÏãúÌñâÏóê Îî∞Î•∏ Í±¥ÏÑ§ ÌòÑÏû•Ïùò ÏïàÏ†ÑÏÇ¨Í≥† ÏòàÎ∞©ÏùÑ ÏúÑÌïú ÏµúÏ†ÅÏùò ÏÜîÎ£®ÏÖòÏù¥ ÏöîÍµ¨ÎêòÍ≥† ÏûàÎäî ÏãúÏ†êÏóêÏÑú Ïä§ÎßàÌä∏ ÏïàÏ†Ñ ÏÜîÎ£®ÏÖò ÏòàÎ∞© ÌîåÎû´Ìèº Íµ¨Ï∂ïÏùò ÏùòÏùòÍ∞Ä ÌÅ¨Îã§ Í≥† ÎßêÌñàÎã§.', 'role': 'user'}]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset \n",
    "from random import randint\n",
    "\n",
    "def get_message_from_dataset(sample_dataset_json_file):\n",
    "    # Load our test dataset\n",
    "    full_test_dataset = load_dataset(\"json\", data_files=sample_dataset_json_file, split=\"train\")\n",
    "\n",
    "    # Test on sample \n",
    "    rand_idx = randint(0, len(full_test_dataset)-1)\n",
    "    rand_idx = 75\n",
    "    print(\"rand_idx: \", rand_idx)\n",
    "    messages = full_test_dataset[rand_idx][\"messages\"][:2]\n",
    "    # messages = test_dataset[rand_idx][\"text\"][:2]\n",
    "    print(\"messages: \\n\", messages)\n",
    "\n",
    "    return messages, full_test_dataset, rand_idx\n",
    "\n",
    "messages, full_test_dataset, rand_idx = get_message_from_dataset(sample_dataset_json_file = full_test_data_json)    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba15b47b",
   "metadata": {},
   "source": [
    "### Ï∂îÎ°†"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ddaf71a0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Query:**\n",
      "Please summarize the goals for journalist in this text:\n",
      "\n",
      "ÏÇ∞ÏóÖÌòÑÏû•ÏóêÏÑú ÏÑ∏Ï¢ÖÌÖîÎ†àÏΩ§Ïùò Ïä§ÎßàÌä∏ ÏïàÏ†Ñ ÌîåÎû´Ìèº ÏÜîÎ£®ÏÖò ÏùÑ ÌôúÏö©ÌïòÍ≥† ÏûàÎã§. ÏÑ∏Ï¢ÖÌÖîÎ†àÏΩ§ Ï†úÍ≥µ ÏÑ∏Ï¢ÖÌÖîÎ†àÏΩ§ÏùÄ ÌÉúÏòÅÍ±¥ÏÑ§Ïóê Ï§ëÎåÄÏû¨Ìï¥Ï≤òÎ≤åÎ≤ï ÎåÄÏùëÏóê ÏµúÏ†ÅÌôîÎêú Ïä§ÎßàÌä∏ ÏïàÏ†Ñ ÌîåÎû´Ìèº ÏÜîÎ£®ÏÖò ÎÇ©Ìíà Í≥ÑÏïΩÏùÑ Ï≤¥Í≤∞ÌñàÎã§Í≥† 4Ïùº Î∞ùÌòîÎã§. Ïö∞ÏÑ† ÌÉúÏòÅÍ±¥ÏÑ§ Ï†ÑÍµ≠ ÏÇ∞ÏóÖÌòÑÏû•Ïóê Ïä§ÎßàÌä∏ ÏïàÏ†Ñ ÏÜîÎ£®ÏÖò 500ÎåÄ ÎÇ©Ìíà Í≥ÑÏïΩÏùÑ Ï≤¥Í≤∞ÌñàÍ≥† Ï∂îÍ∞Ä Íµ¨Ï∂ïÏùÑ ÌòëÏùò Ï§ëÏù¥Îã§. ÏßÄÎÇú 1Ïõî Î≥∏Í≤© ÏãúÌñâÎêú Ï§ëÎåÄÏû¨Ìï¥Ï≤òÎ≤åÎ≤ïÏùÄ ÏÇ∞ÏóÖÌòÑÏû•ÏóêÏÑú Ïù∏Î™ÖÏÇ¨Í≥† Î∞úÏÉù Ïãú Í≤ΩÏòÅÏßÑÏù¥ÎÇò Î≤ïÏù∏ÏóêÍ≤å Ï±ÖÏûÑÏùÑ Î¨ºÏùÑ Ïàò ÏûàÎèÑÎ°ù Í∑úÏ†ïÌïú Î≤ïÏù¥Îã§. Ìï¥Îãπ Î≤ïÏùÄ ÏïàÏ†Ñ Î≥¥Í±¥ Í¥ÄÎ†® Í¥ÄÎ¶¨ÏÉÅÏùò Ï°∞Ïπò Íµ¨Ï∂ïÏùÑ ÏùòÎ¨¥ÌôîÌïòÍ≥† ÏûàÏúºÎÇò Í∏∞ÏóÖÏùò ÌïúÏ†ïÏ†Å ÏûêÏõêÍ≥º Î∂ÄÏ°±Ìïú Ïù∏Î†• Î¨∏Ï†úÎ°ú Ïñ¥Î†§ÏõÄÏùÑ Í≤™Í≥† ÏûàÎã§. ÏÑ∏Ï¢ÖÌÖîÎ†àÏΩ§Ïùò Ïä§ÎßàÌä∏ ÏïàÏ†Ñ ÌîåÎû´Ìèº ÏÜîÎ£®ÏÖòÏùÄ Ï∂úÏûÖÍ¥ÄÎ¶¨Î∂ÄÌÑ∞ CCTV Í∞ÄÏä§ÌÉêÏßÄ Í∞ÅÏ¢Ö ÏÑºÏÑú Îì±ÏùÑ ÌïòÎÇòÎ°ú ÌÜµÌï©Ìï¥ ÌòÑÏû•ÏùÑ Ï¢ÖÌï© Í¥ÄÎ¶¨Ìï† Ïàò ÏûàÎã§. LBS ÏúÑÏπòÍ∏∞Î∞ò IoT ÏÇ¨Î¨ºÏù∏ÌÑ∞ÎÑ∑ Îì± Ïä§ÎßàÌä∏ Í∏∞Ïà†ÏùÑ ÏúµÌï©ÌñàÎã§. ÏïàÏ†Ñ Í¥ÄÎ¶¨ Îã¥ÎãπÏûêÎäî Í∞Å ÌòÑÏû•ÎßàÎã§ ÏÑ§ÏπòÎêú Ïπ¥Î©îÎùº Î∞è CCTV Í∞úÏÜåÎ≥Ñ ÏÑºÏÑúÏôÄ ÌÜµÏã† Ïù∏ÌîÑÎùºÎ•º ÌÜµÌï¥ ÌòÑÏû• Ï†ïÎ≥¥Î•º Ïã§ÏãúÍ∞ÑÏúºÎ°ú ÌôïÏù∏ÌïòÍ≥† ÎπÑÏÉÅ ÏÉÅÌô© ÏãúÏóêÎäî Ï†ÑÏ≤¥ ÌòÑÏû• ÎòêÎäî Ìï¥Îãπ Íµ¨Ïó≠ ÏÉÅÌô©Ïã§ ÏãúÏä§ÌÖúÏù¥ÎÇò Î™®Î∞îÏùºÎ°ú Í∑ºÎ°úÏûêÏóêÍ≤å ÏïàÏ†Ñ Ï°∞ÏπòÏÇ¨Ìï≠ÏùÑ ÏßÄÏãúÌï† Ïàò ÏûàÎã§. Ïù¥ÏôÄ Ìï®Íªò ÌÉÄÏõåÌÅ¨Î†àÏù∏Ïóê ÏÑ§ÏπòÌïú 360ÎèÑ Ïπ¥Î©îÎùºÎ•º ÌÜµÌï¥ ÌòÑÏû•Ïùò Î∂àÏïàÏ†Ñ ÏöîÏÜåÎ•º Î∞úÍ≤¨ÌïòÎ©¥ Í¥ÄÍ≥ÑÏûêÏóêÍ≤å ÏïåÎ¶ºÏùÑ Î≥¥ÎÇº Ïàò ÏûàÎã§. ÏßÄÌïò ÏûëÏóÖÏóêÏÑúÎäî Ïù¥ÎèôÌòï Ïä§ÎßàÌä∏ ÏòÅÏÉÅ Ïû•ÎπÑÎ°ú ÏïàÏ†Ñ ÏÇ¨Í∞ÅÏßÄÎåÄÎ•º ÏÇ¥ÌïÑ Ïàò ÏûàÍ≥† Î∞ÄÌèêÎêú ÏûëÏóÖ Í≥µÍ∞ÑÏóêÏÑúÎäî Í∞ÄÏä§ ÏÑºÏÑúÏôÄ Ïã†Ìò∏Îì±Ìòï Ï†ÑÍ¥ëÌåêÏùÑ ÏÑ§ÏπòÌï¥ Ïã§ÏãúÍ∞ÑÏúºÎ°ú Ïä§ÎßàÌä∏ ÏÉÅÌô©ÌåêÏóê Í∞ÄÏä§ ÎÜçÎèÑÎ•º Ï†ÑÏÜ°ÌïúÎã§. Ïú†Ìï¥Í∞ÄÏä§Í∞Ä ÌóàÏö© ÎÜçÎèÑÎ•º Ï¥àÍ≥ºÌïòÎ©¥ ÌòÑÏû•ÏóêÏÑú ÌôòÍ∏∞ ÏãúÏä§ÌÖúÏù¥ ÏûêÎèôÏúºÎ°ú ÏûëÎèôÌïúÎã§. ÌòÑÏû• ÎÇ¥ Ï∂îÎùΩ ÏÇ¨Í≥†Í∞Ä Î∞úÏÉùÌï† Ïàò ÏûàÎäî Í∞úÍµ¨Î∂ÄÏóê Î∂ÄÏ∞©Ìïú ÏÑºÏÑúÎäî Í∞úÍµ¨Î∂ÄÍ∞Ä ÎπÑÏ†ïÏÉÅÏ†ÅÏúºÎ°ú Í∞úÌèêÎêêÏùÑ Îïå Í≤ΩÍ≥†ÏùåÏùÑ Î≥¥ÎÇ¥ ÏúÑÌóòÏÉÅÌô©ÏùÑ ÏïåÎ¶∞Îã§. Í∞ïÌö®ÏÉÅ ÏÑ∏Ï¢ÖÌÖîÎ†àÏΩ§ ÌÜµÏã†ÏÇ¨ÏóÖÎ≥∏Î∂ÄÏû•ÏùÄ Ï§ëÎåÄÏû¨Ìï¥Ï≤òÎ≤åÎ≤ï ÏãúÌñâÏóê Îî∞Î•∏ Í±¥ÏÑ§ ÌòÑÏû•Ïùò ÏïàÏ†ÑÏÇ¨Í≥† ÏòàÎ∞©ÏùÑ ÏúÑÌïú ÏµúÏ†ÅÏùò ÏÜîÎ£®ÏÖòÏù¥ ÏöîÍµ¨ÎêòÍ≥† ÏûàÎäî ÏãúÏ†êÏóêÏÑú Ïä§ÎßàÌä∏ ÏïàÏ†Ñ ÏÜîÎ£®ÏÖò ÏòàÎ∞© ÌîåÎû´Ìèº Íµ¨Ï∂ïÏùò ÏùòÏùòÍ∞Ä ÌÅ¨Îã§ Í≥† ÎßêÌñàÎã§.\n",
      "\n",
      "**Original Answer:**\n",
      "ÏÑ∏Ï¢ÖÌÖîÎ†àÏΩ§ÏùÄ ÌÉúÏòÅÍ±¥ÏÑ§Ïóê Ï∂úÏûÖÍ¥ÄÎ¶¨Î∂ÄÌÑ∞ CCTV Í∞ÄÏä§ÌÉêÏßÄ Í∞ÅÏ¢Ö ÏÑºÏÑú Îì±ÏùÑ ÌïòÎÇòÎ°ú ÌÜµÌï©Ìï¥ ÌòÑÏû•ÏùÑ Ï¢ÖÌï© Í¥ÄÎ¶¨Ìï† Ïàò ÏûàÎäî Ïä§ÎßàÌä∏ ÏïàÏ†Ñ ÌîåÎû´Ìèº ÏÜîÎ£®ÏÖò ÎÇ©Ìíà Í≥ÑÏïΩÏùÑ Ï≤¥Í≤∞ÌñàÎã§Í≥† 4Ïùº Î∞ùÌòîÏúºÎ©∞ ÌÉúÏòÅÍ±¥ÏÑ§ Ï†ÑÍµ≠ ÏÇ∞ÏóÖÌòÑÏû•Ïóê Ïä§ÎßàÌä∏ ÏïàÏ†Ñ ÏÜîÎ£®ÏÖò 500ÎåÄ ÎÇ©Ìíà Í≥ÑÏïΩÏùÑ Ï≤¥Í≤∞ÌñàÍ≥† Ï∂îÍ∞Ä Íµ¨Ï∂ïÏùÑ ÌòëÏùò Ï§ëÏù¥Îã§.\n",
      "\n",
      "**Generated Answer:**\n",
      " Ï†ÄÎäî ÏÇ∞ÏóÖÌòÑÏû•ÏóêÏÑú ÏïàÏ†ÑÏÇ¨Í≥†Î•º ÏòàÎ∞©ÌïòÍ≥† Ï§ëÎåÄÏû¨Ìï¥Ï≤òÎ≤åÎ≤ïÏóê Î∂ÄÌï©ÌïòÎäî Ïä§ÎßàÌä∏ ÏïàÏ†Ñ ÌîåÎû´Ìèº ÏÜîÎ£®ÏÖòÏùÑ Íµ¨Ï∂ïÌïòÎäî ÌÉúÏòÅÍ±¥ÏÑ§Í≥º ÏÑ∏Ï¢ÖÌÖîÎ†àÏΩ§Ïùò Í≥ÑÏïΩÏùÑ ÏÜåÍ∞úÌï©ÎãàÎã§. \n",
      "\n",
      "ÌÉúÏòÅÍ±¥ÏÑ§ÏùÄ ÏÑ∏Ï¢ÖÌÖîÎ†àÏΩ§Ïùò Ïä§ÎßàÌä∏ ÏïàÏ†Ñ ÌîåÎû´Ìèº ÏÜîÎ£®ÏÖòÏùÑ Ï†ÑÍµ≠ ÏÇ∞ÏóÖÌòÑÏû•Ïóê 500ÎåÄ ÎÇ©Ìíà Í≥ÑÏïΩÏùÑ Ï≤¥Í≤∞ÌñàÏäµÎãàÎã§. Ï∂îÍ∞Ä Íµ¨Ï∂ïÏùÑ ÌòëÏùò Ï§ëÏûÖÎãàÎã§. \n",
      "\n",
      "Ïù¥ Í≥ÑÏïΩÏùÄ Ï§ëÎåÄÏû¨Ìï¥Ï≤òÎ≤åÎ≤ïÏóê Î∂ÄÌï©ÌïòÎäî ÏïàÏ†Ñ Î≥¥Í±¥ Í¥ÄÎ†® Í¥ÄÎ¶¨ÏÉÅÏùò Ï°∞Ïπò Íµ¨Ï∂ïÏùÑ ÏùòÎ¨¥ÌôîÌïòÍ≥† ÏûàÏäµÎãàÎã§. Í∏∞ÏóÖÏùò ÌïúÏ†ïÏ†Å ÏûêÏõêÍ≥º Î∂ÄÏ°±Ìïú Ïù∏Î†• Î¨∏Ï†úÎ°ú Ïñ¥Î†§ÏõÄÏùÑ Í≤™Í≥† ÏûàÏäµÎãàÎã§. \n",
      "\n",
      "ÏÑ∏Ï¢ÖÌÖîÎ†àÏΩ§Ïùò Ïä§ÎßàÌä∏ ÏïàÏ†Ñ ÌîåÎû´Ìèº ÏÜîÎ£®ÏÖòÏùÄ Ï∂úÏûÖÍ¥ÄÎ¶¨Î∂ÄÌÑ∞ CCTV, Í∞ÄÏä§ÌÉêÏßÄ Í∞ÅÏ¢Ö ÏÑºÏÑú Îì±ÏùÑ ÌïòÎÇòÎ°ú ÌÜµÌï©Ìï¥ ÌòÑÏû•ÏùÑ Ï¢ÖÌï© Í¥ÄÎ¶¨Ìï† Ïàò ÏûàÏäµÎãàÎã§. LBS ÏúÑÏπòÍ∏∞Î∞ò IoT ÏÇ¨Î¨ºÏù∏ÌÑ∞ÎÑ∑ Îì± Ïä§ÎßàÌä∏ Í∏∞Ïà†ÏùÑ ÏúµÌï©ÌñàÎã§. \n",
      "\n",
      "Ïù¥ ÌîåÎû´ÌèºÏùÄ ÏïàÏ†Ñ Í¥ÄÎ¶¨ Îã¥ÎãπÏûêÍ∞Ä Í∞Å ÌòÑÏû•ÎßàÎã§ ÏÑ§ÏπòÎêú Ïπ¥Î©îÎùº Î∞è CCTV Í∞úÏÜåÎ≥Ñ ÏÑºÏÑúÏôÄ ÌÜµÏã† Ïù∏ÌîÑÎùºÎ•º ÌÜµÌï¥ ÌòÑÏû• Ï†ïÎ≥¥Î•º Ïã§ÏãúÍ∞ÑÏúºÎ°ú ÌôïÏù∏Ìï† Ïàò ÏûàÍ≥†, ÎπÑÏÉÅ ÏÉÅÌô© ÏãúÏóêÎäî Ï†ÑÏ≤¥ ÌòÑÏû• ÎòêÎäî Ìï¥Îãπ Íµ¨Ïó≠ ÏÉÅÌô©Ïã§ ÏãúÏä§ÌÖúÏù¥ÎÇò Î™®Î∞îÏùºÎ°ú Í∑ºÎ°úÏûêÏóêÍ≤å ÏïàÏ†Ñ Ï°∞ÏπòÏÇ¨Ìï≠ÏùÑ ÏßÄÏãúÌï† Ïàò ÏûàÏäµÎãàÎã§. \n",
      "\n",
      "Ïù¥ ÌîåÎû´ÌèºÏùÄ ÌòÑÏû• ÎÇ¥ Ï∂îÎùΩ ÏÇ¨Í≥†Í∞Ä Î∞úÏÉùÌï† Ïàò ÏûàÎäî Í∞úÍµ¨Î∂ÄÏóê Î∂ÄÏ∞©Ìïú ÏÑºÏÑúÎ•º ÌÜµÌï¥ Í∞úÍµ¨Î∂ÄÍ∞Ä ÎπÑÏ†ïÏÉÅÏ†ÅÏúºÎ°ú Í∞úÌèêÎêêÏùÑ Îïå Í≤ΩÍ≥†ÏùåÏùÑ Î≥¥ÎÇ¥ ÏúÑÌóòÏÉÅÌô©ÏùÑ ÏïåÎ¶ΩÎãàÎã§. \n",
      "\n",
      "Í∞ïÌö®ÏÉÅ ÏÑ∏Ï¢ÖÌÖîÎ†àÏΩ§ ÌÜµÏã†ÏÇ¨ÏóÖÎ≥∏Î∂ÄÏû•ÏùÄ Ï§ëÎåÄÏû¨Ìï¥Ï≤òÎ≤åÎ≤ï ÏãúÌñâÏóê Îî∞Î•∏ Í±¥ÏÑ§ ÌòÑÏû•Ïùò ÏïàÏ†ÑÏÇ¨Í≥† ÏòàÎ∞©ÏùÑ ÏúÑÌïú ÏµúÏ†ÅÏùò ÏÜîÎ£®ÏÖòÏù¥ ÏöîÍµ¨ÎêòÍ≥† ÏûàÎäî ÏãúÏ†êÏóêÏÑú Ïä§ÎßàÌä∏ ÏïàÏ†Ñ ÏÜîÎ£®ÏÖò ÏòàÎ∞© ÌîåÎû´Ìèº Íµ¨Ï∂ïÏùò ÏùòÏùòÍ∞Ä ÌÅ¨Îã§ Í≥† ÎßêÌñàÎã§.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def generate_response(messages, model, tokenizer, full_test_dataset, rand_idx):\n",
    "    input_ids = tokenizer.apply_chat_template(messages,add_generation_prompt=True,return_tensors=\"pt\").to(model.device)\n",
    "    outputs = model.generate(\n",
    "        input_ids,\n",
    "        max_new_tokens=512,\n",
    "        eos_token_id= tokenizer.eos_token_id,\n",
    "        do_sample=True,\n",
    "        temperature=0.6,\n",
    "        top_p=0.9,\n",
    "    )\n",
    "    response = outputs[0][input_ids.shape[-1]:]\n",
    "\n",
    "    print(f\"**Query:**\\n{full_test_dataset[rand_idx]['messages'][1]['content']}\\n\")\n",
    "    # print(f\"**Query:**\\n{test_dataset[rand_idx]['text'][1]['content']}\\n\")\n",
    "    # print(f\"**Original Answer:**\\n{test_dataset[rand_idx]['text'][2]['content']}\\n\")\n",
    "    print(f\"**Original Answer:**\\n{full_test_dataset[rand_idx]['messages'][2]['content']}\\n\")\n",
    "    print(f\"**Generated Answer:**\\n{tokenizer.decode(response,skip_special_tokens=True)}\")\n",
    "\n",
    "generate_response(messages, model, tokenizer, full_test_dataset, rand_idx)    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "360223a2-02b4-429d-8e47-6ec5ca55247e",
   "metadata": {},
   "source": [
    "### Ìï†ÎãπÎêú CUDA memoryÎ•º Release"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "14070303-ea9f-40e5-a56e-cca1bf56f7f8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "del model\n",
    "del tokenizer\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f0a95c1-e28e-463d-8fba-a76a09de2105",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "finetune-image",
   "language": "python",
   "name": "finetune-image"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "6daafc7ae2313787fa97137de7504cfa7c5a594d29476828201b4f7d7fb5c4e1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
