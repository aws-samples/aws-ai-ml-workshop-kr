{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe4872e4-c90b-434b-bfe5-f88292fba385",
   "metadata": {},
   "source": [
    "# ë¡œì»¬ì—ì„œ í›ˆë ¨ í•˜ê¸°\n",
    "- ì´ ë…¸íŠ¸ë¶ì€ ë¡œì»¬ (í˜„ì¬ ë¨¸ì‹ ) ì—ì„œ Hugging Face Accelerator + PyTorch FSDP ë¡œ íŒŒì¸ íŠœë‹ í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5da7aa7-1a2d-4db1-b011-59217c32a83a",
   "metadata": {},
   "source": [
    "## 1. í™˜ê²½ ì…‹ì—…"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c268a663-63a3-4d9e-8bd3-8025dec88254",
   "metadata": {},
   "source": [
    "### Hugging Face Token ì…ë ¥\n",
    "- [ì¤‘ìš”] HF Key ê°€ ë…¸ì¶œì´ ì•ˆë˜ë„ë¡ ì¡°ì‹¬í•˜ì„¸ìš”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3ee8f554-0ac4-499a-bc35-7fbae4b96b98",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "datasets                  3.0.0\n",
      "sagemaker                 2.232.1\n",
      "sagemaker-core            1.0.9\n",
      "torch                     2.4.1\n",
      "transformers              4.40.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip list | grep -E \"torch|datasets|transformers|sagemaker\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "542f1cbe-5b7e-4fae-bf2d-4a6ac2f3dfe1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to /home/ec2-user/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def set_hf_key_env_vars(hf_key_name, key_val):\n",
    "    os.environ[hf_key_name] = key_val\n",
    "\n",
    "def get_hf_key_env_vars(hf_key_name):\n",
    "    HF_key_value = os.environ.get(hf_key_name)\n",
    "\n",
    "    return HF_key_value\n",
    "\n",
    "\n",
    "is_sagemaker_notebook = True\n",
    "if is_sagemaker_notebook:\n",
    "    hf_key_name = \"HF_KEY\"\n",
    "    key_val = \"<Type Your HF Key>\"\n",
    "    set_hf_key_env_vars(hf_key_name, key_val)\n",
    "    HF_TOKEN = get_hf_key_env_vars(hf_key_name)\n",
    "else: # VS Code\n",
    "    from dotenv import load_dotenv\n",
    "    HF_TOKEN = os.getenv('HF_TOKEN')\n",
    "\n",
    "\n",
    "# Log in to HF\n",
    "!huggingface-cli login --token {HF_TOKEN}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebcc7a0b-4829-4b2c-88b6-fd423732c53a",
   "metadata": {},
   "source": [
    "### ì €ì¥ëœ ë³€ìˆ˜ ë¡œë”©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "24bce431",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_folder:  ../data/naver-news-summarization-ko\n",
      "train_data_json:  ../data/naver-news-summarization-ko/train/train_dataset.json\n",
      "validation_data_json:  ../data/naver-news-summarization-ko/validation/validation_dataset.json\n",
      "test_data_json:  ../data/naver-news-summarization-ko/test/test_dataset.json\n",
      "full_train_data_json:  ../data/naver-news-summarization-ko/full_train/train_dataset.json\n",
      "full_validation_data_json:  ../data/naver-news-summarization-ko/full_validation/validation_dataset.json\n",
      "full_test_data_json:  ../data/naver-news-summarization-ko/full_test/test_dataset.json\n"
     ]
    }
   ],
   "source": [
    "%store -r data_folder\n",
    "%store -r train_data_json \n",
    "%store -r validation_data_json \n",
    "%store -r test_data_json \n",
    "%store -r full_train_data_json \n",
    "%store -r full_validation_data_json \n",
    "%store -r full_test_data_json\n",
    "\n",
    "\n",
    "print(\"data_folder: \", data_folder)\n",
    "print(\"train_data_json: \", train_data_json)\n",
    "print(\"validation_data_json: \", validation_data_json)\n",
    "print(\"test_data_json: \", test_data_json)\n",
    "print(\"full_train_data_json: \", full_train_data_json)\n",
    "print(\"full_validation_data_json: \", full_validation_data_json)\n",
    "print(\"full_test_data_json: \", full_test_data_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a396bc4f-0b0a-4ffb-8c59-18ed6d0a968d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/finetune_fsdp_image/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from datasets import Dataset, load_dataset\n",
    "from datasets import load_dataset\n",
    "from transformers import set_seed\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a78f53c-d21d-4fca-b69d-6a85d966353c",
   "metadata": {},
   "source": [
    "## 2. ë² ì´ìŠ¤ ëª¨ë¸ ì¤€ë¹„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d514e3df",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#model_id = \"meta-llama/Meta-Llama-3-8B\"\n",
    "model_id = \"MLP-KTLim/llama-3-Korean-Bllossom-8B\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5e8a20c2-ef30-4214-89e2-c58fe7e250b5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prefix = \"llama-3-8b-fsdp-qlora-naver-news\"\n",
    "output_dir = f\"/home/ec2-user/SageMaker/models/{prefix}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2403f6b8",
   "metadata": {},
   "source": [
    "### Config YAML íŒŒì¼ ìƒì„±"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "72d30120",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting accelerator_config/local_llama_3_8b_fsdp_qlora.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile accelerator_config/local_llama_3_8b_fsdp_qlora.yaml\n",
    "# script parameters\n",
    "#model_id:  \"meta-llama/Meta-Llama-3-8B\" # Hugging Face model id\n",
    "model_id: \"MLP-KTLim/llama-3-Korean-Bllossom-8B\"\n",
    "#model_id: \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
    "###########################\n",
    "# small samples for Debug\n",
    "###########################\n",
    "train_dataset_path: \"../data/naver-news-summarization-ko/train\"                      # path to dataset\n",
    "validation_dataset_path: \"../data/naver-news-summarization-ko/validation\"                      # path to dataset\n",
    "#test_dataset_path: \"../data/naver-news-summarization-ko/test\"                      # path to dataset\n",
    "per_device_train_batch_size: 1         # batch size per device during training\n",
    "per_device_eval_batch_size: 1          # batch size for evaluation\n",
    "gradient_accumulation_steps: 1         # number of steps before performing a backward/update pass\n",
    "###########################\n",
    "# large samples for evaluation\n",
    "###########################\n",
    "# train_dataset_path: \"../data/naver-news-summarization-ko/full_train\"                      # path to dataset\n",
    "# validation_dataset_path: \"../data/naver-news-summarization-ko/full_validation\"                      # path to dataset\n",
    "# test_dataset_path: \"../data/naver-news-summarization-ko/full_test\"                      # path to dataset\n",
    "# per_device_train_batch_size: 16         # batch size per device during training\n",
    "# per_device_eval_batch_size: 1          # batch size for evaluation\n",
    "# gradient_accumulation_steps: 2         # number of steps before performing a backward/update pass\n",
    "###########################\n",
    "max_seq_length:  2048              # max sequence length for model and packing of the dataset\n",
    "\n",
    "\n",
    "# training parameters\n",
    "output_dir: \"/home/ec2-user/SageMaker/models/llama-3-8b-fsdp-qlora-naver-news\" # Temporary output directory for model checkpoints\n",
    "report_to: \"tensorboard\"               # report metrics to tensorboard\n",
    "logging_dir: \"/home/ec2-user/SageMaker/logs/llama-3-8b-fsdp-qlora-naver-news\" # log folder for tensorboard\n",
    "learning_rate: 0.0002                  # learning rate 2e-4\n",
    "lr_scheduler_type: \"constant\"          # learning rate scheduler\n",
    "num_train_epochs: 1                    # number of training epochs\n",
    "optim: adamw_torch                     # use torch adamw optimizer\n",
    "logging_steps: 10                      # log every 10 steps\n",
    "save_strategy: epoch                   # save checkpoint every epoch\n",
    "evaluation_strategy: epoch             # evaluate every epoch\n",
    "max_grad_norm: 0.3                     # max gradient norm\n",
    "warmup_ratio: 0.03                     # warmup ratio\n",
    "bf16: true                             # use bfloat16 precision\n",
    "tf32: true                             # use tf32 precision\n",
    "gradient_checkpointing: true          # use gradient checkpointing to save memory\n",
    "# FSDP parameters: https://huggingface.co/docs/transformers/main/en/fsdp\n",
    "fsdp: \"full_shard auto_wrap offload\" # remove offload if enough GPU memory\n",
    "fsdp_config:\n",
    "    backward_prefetch: \"backward_pre\"\n",
    "    forward_prefetch: \"false\"\n",
    "    use_orig_params: \"false\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c851f75d",
   "metadata": {},
   "source": [
    "## 3. í›ˆë ¨ Script ì‹¤í–‰\n",
    "\n",
    "ì•„ë˜ëŠ” Hugging Face ì˜ Accelerator ë¥¼ ì´ìš©í•œ PyTorch FSDP ë¥¼ ì‹¤í–‰í•˜ê¸° ìœ„í•œ ëª…ë ¹ì–´ ì…ë‹ˆë‹¤.\n",
    "- í˜„ì¬ ë¨¸ì‹ ì— 4ê°œì˜ GPU ê°€ ìˆëŠ” ê²½ìš° ì…ë‹ˆë‹¤. GPU ê°€ 1ê°œ ì´ë©´ nproc_per_node=1 ë¡œ ìˆ˜ì •í•´ì„œ ì‹¤í–‰ í•˜ì„¸ìš”. \n",
    "\n",
    "```\n",
    "!ACCELERATE_USE_FSDP=1 FSDP_CPU_RAM_EFFICIENT_LOADING=1 torchrun --nproc_per_node=4 \\\n",
    "../../scripts/local_run_fsdp_qlora.py \\\n",
    "--config config_folder_name/local_llama_3_8b_fsdp_qlora.yaml\n",
    "```\n",
    "- ì°¸ê³ \n",
    "    - Launching your ğŸ¤— Accelerate scripts, [Link](https://huggingface.co/docs/accelerate/en/basic_tutorials/launch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "650a43f8-666d-4176-94b5-885f2bcb58cd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "config_folder_name = \"accelerator_config\"\n",
    "os.makedirs(config_folder_name, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cb9be15-e2bc-4b03-a80b-8849e77d9e19",
   "metadata": {},
   "source": [
    "### Hugging Face  Accelerator ì— ì œê³µí•  config.yaml ì…ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccd6de05-6af0-45d0-8ecf-68e796771b96",
   "metadata": {},
   "source": [
    "### IMPORTANT!! Set use reentrant to True when we use FSDP\n",
    "```\n",
    "if training_args.gradient_checkpointing:\n",
    "    training_args.gradient_checkpointing_kwargs = {\"use_reentrant\":True}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9f81f6fe-4e96-4a6a-a354-d89d2f248351",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W0928 02:11:28.354000 140696742291264 torch/distributed/run.py:779] \n",
      "W0928 02:11:28.354000 140696742291264 torch/distributed/run.py:779] *****************************************\n",
      "W0928 02:11:28.354000 140696742291264 torch/distributed/run.py:779] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "W0928 02:11:28.354000 140696742291264 torch/distributed/run.py:779] *****************************************\n",
      "## script_args: \n",
      " ScriptArguments(train_dataset_path='../data/naver-news-summarization-ko/train', validation_dataset_path='../data/naver-news-summarization-ko/validation', model_id='MLP-KTLim/llama-3-Korean-Bllossom-8B', max_seq_length=2048)\n",
      "## training_args: \n",
      " TrainingArguments(\n",
      "_n_gpu=1,\n",
      "accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'gradient_accumulation_kwargs': None},\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "auto_find_batch_size=False,\n",
      "bf16=True,\n",
      "bf16_full_eval=False,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_persistent_workers=False,\n",
      "dataloader_pin_memory=True,\n",
      "dataloader_prefetch_factor=None,\n",
      "ddp_backend=None,\n",
      "ddp_broadcast_buffers=None,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "ddp_timeout=1800,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "dispatch_batches=None,\n",
      "do_eval=True,\n",
      "do_predict=False,\n",
      "do_train=False,\n",
      "eval_accumulation_steps=None,\n",
      "eval_delay=0,\n",
      "eval_do_concat_batches=True,\n",
      "eval_steps=None,\n",
      "evaluation_strategy=epoch,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "fsdp=[<FSDPOption.FULL_SHARD: 'full_shard'>, <FSDPOption.AUTO_WRAP: 'auto_wrap'>, <FSDPOption.OFFLOAD: 'offload'>],\n",
      "fsdp_config={'backward_prefetch': 'backward_pre', 'forward_prefetch': 'false', 'use_orig_params': 'false', 'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\n",
      "fsdp_min_num_params=0,\n",
      "fsdp_transformer_layer_cls_to_wrap=None,\n",
      "full_determinism=False,\n",
      "gradient_accumulation_steps=1,\n",
      "gradient_checkpointing=True,\n",
      "gradient_checkpointing_kwargs=None,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_always_push=False,\n",
      "hub_model_id=None,\n",
      "hub_private_repo=False,\n",
      "hub_strategy=every_save,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "include_inputs_for_metrics=False,\n",
      "include_num_input_tokens_seen=False,\n",
      "include_tokens_per_second=False,\n",
      "jit_mode_eval=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=0.0002,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=0,\n",
      "log_level=passive,\n",
      "log_level_replica=warning,\n",
      "log_on_each_node=True,\n",
      "logging_dir=/home/ec2-user/SageMaker/logs/llama-3-8b-fsdp-qlora-naver-news,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=10,\n",
      "logging_strategy=steps,\n",
      "lr_scheduler_kwargs={},\n",
      "lr_scheduler_type=constant,\n",
      "max_grad_norm=0.3,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "neftune_noise_alpha=None,\n",
      "no_cuda=False,\n",
      "num_train_epochs=1,\n",
      "optim=adamw_torch,\n",
      "optim_args=None,\n",
      "optim_target_modules=None,\n",
      "output_dir=/home/ec2-user/SageMaker/models/llama-3-8b-fsdp-qlora-naver-news,\n",
      "overwrite_output_dir=False,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=1,\n",
      "per_device_train_batch_size=1,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "ray_scope=last,\n",
      "remove_unused_columns=True,\n",
      "report_to=['tensorboard'],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=/home/ec2-user/SageMaker/models/llama-3-8b-fsdp-qlora-naver-news,\n",
      "save_on_each_node=False,\n",
      "save_only_model=False,\n",
      "save_safetensors=True,\n",
      "save_steps=500,\n",
      "save_strategy=epoch,\n",
      "save_total_limit=None,\n",
      "seed=42,\n",
      "skip_memory_metrics=True,\n",
      "split_batches=None,\n",
      "tf32=True,\n",
      "torch_compile=False,\n",
      "torch_compile_backend=None,\n",
      "torch_compile_mode=None,\n",
      "torchdynamo=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_cpu=False,\n",
      "use_ipex=False,\n",
      "use_legacy_prediction_loop=False,\n",
      "use_mps_device=False,\n",
      "warmup_ratio=0.03,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      ")\n",
      "Generating train split: 10 examples [00:00, 4395.16 examples/s]\n",
      "Generating train split: 10 examples [00:00, 8002.87 examples/s]\n",
      "/home/ec2-user/anaconda3/envs/finetune_fsdp_image/lib/python3.10/site-packages/huggingface_hub/file_download.py:1142: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:00<00:00, 380.11 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:00<00:00, 2694.18 examples/s]\n",
      "You are an AI assistant specialized in news articles.Your role is to provide accurate summaries and insights in Korean. Please analyze the given text and provide concise, informative summaries that highlight the key goals and findings.\n",
      "\n",
      "Human: Please summarize the goals for journalist in this text:\n",
      "\n",
      "êµ­ë‚´ ìµœëŒ€ê·œëª¨ ë‚˜ë…¸ ì „ì‹œíšŒì´ì ì„¸ê³„ 3ëŒ€ ë‚˜ë…¸í–‰ì‚¬ ë‚˜ë…¸ì½”ë¦¬ì•„ 2022 ê°€ 6ì¼ë¶€í„° 3ì¼ê°„ ê²½ê¸°ë„ í‚¨í…ìŠ¤ ì œ1ì „ì‹œì¥ 4Â·5í™€ ì—ì„œ ê°œìµœëœë‹¤. ì˜¬í•´ 20íšŒë¥¼ ë§ëŠ” ë‚˜ë…¸ì½”ë¦¬ì•„ëŠ” ì‚°ì—…í†µìƒìì›ë¶€ ê³¼í•™ê¸°ìˆ ì •ë³´í†µì‹ ë¶€ê°€ ê³µë™ ì£¼ìµœí•œë‹¤. ë‚˜ë…¸ìœµí•©ì‚°ì—…ì—°êµ¬ì¡°í•©ê³¼ ë‚˜ë…¸ê¸°ìˆ ì—°êµ¬í˜‘ì˜íšŒê°€ ì£¼ê´€í•œë‹¤. ë‚˜ë…¸ê¸°ìˆ ê³¼ ì‚°ì—…ì˜ í˜„ì¬ ë¯¸ë˜ íŠ¸ëœë“œë¥¼ ì¡°ë§í•˜ëŠ” ê¸°ì¡° ê°•ì—°ì„ ì‹œì‘ìœ¼ë¡œ ë‚˜ë…¸ ìœµí•© ì „ì‹œí™”ì™€ êµ­ì œ ì‹¬í¬ì§€ì—„ í–‰ì‚¬ê°€ ë‹¤ì–‘í•˜ê²Œ ê°œìµœëœë‹¤. ê°•ë¯¼ì„ LGì´ë…¸í… ë¶€ì‚¬ì¥ì´ ììœ¨ì£¼í–‰ì‚°ì—… ë™í–¥ì— ë”°ë¥¸ ë‚˜ë…¸ê¸°ìˆ ê³¼ ì¸ê³µì§€ëŠ¥ AI ì˜ í™œìš© ì„ ì£¼ì œë¡œ ê¸°ì¡°ê°•ì—°ì„ í•œë‹¤. ì•Œë² ë¥´í˜ë¥´ í”„ë‘ìŠ¤ íŒŒë¦¬ ìŠˆë“œëŒ€ êµìˆ˜ë„ ê¸°ì¡° ê°•ì—°ì„ ë§¡ëŠ”ë‹¤. ì „ì‹œê·œëª¨ëŠ” ì½”ë¡œë‚˜19 ì´ì „ ìˆ˜ì¤€ìœ¼ë¡œ íšŒë³µëë‹¤. ì‚¼ì„±ì „ì LG ë“± ì£¼ìš” ê¸°ì—…ê³¼ ë‚˜ë…¸ ê¸°ìˆ  ê¸°ì—… ë“± ì´ 360ê°œì‚¬ê°€ ì°¸ì—¬í•œë‹¤. ë‚˜ë…¸ 20ì£¼ë…„ íŠ¹ë³„ ê¸°ë…ê´€ë„ ë§ˆë ¨ëœë‹¤. 20ì£¼ë…„ íŠ¹ë³„ ê¸°ë…ê´€ì—ëŠ” ì°¨ì„¸ëŒ€ ë°˜ë„ì²´ ë¯¸ë˜ì°¨ 6ì„¸ëŒ€ 6G ì´ë™í†µì‹  íƒ„ì†Œì¤‘ë¦½ ë””ì§€í„¸ ë°”ì´ì˜¤ ë“± 6ê°œ ë¶„ì•¼ í˜ì‹  ê¸°ìˆ ì´ ì†Œê°œëœë‹¤. ì‚°ì—…í™” ì„¸ì…˜ì—ì„œëŠ” ì§€ì† ê°€ëŠ¥ ì„±ì¥ì„ ìœ„í•œ ESG ë‚˜ë…¸ìœµí•©ê¸°ìˆ ì„ ì£¼ì œë¡œ ì´ˆì²­ ê°•ì—°ì´ ì—´ë¦°ë‹¤. ë‚˜ë…¸ì œí’ˆê±°ë˜ìƒë‹´íšŒ ì „ì‹œíšŒí…Œí¬ë‹ˆì»¬íˆ¬ì–´ ìµœì‹ ê¸°ìˆ ë°œí‘œíšŒ ë“± ë‹¤ì–‘í•œ ë‚˜ë…¸ ê´€ë ¨ ë¶€ëŒ€ í–‰ì‚¬ë„ ì¤€ë¹„ëœë‹¤.<|eot_id|>\n",
      "\n",
      "Assistant: êµ­ë‚´ 3ëŒ€ ë‚˜ë…¸í–‰ì‚¬ ì¤‘ í•˜ë‚˜ì¸ ë‚˜ë…¸ì½”ë¦¬ì•„ 2022 ê°€ 6ì¼ë¶€í„° 3ì¼ê°„ ê²½ê¸°ë„ í‚¨í…ìŠ¤ ì œ1ì „ì‹œì¥ 4Â·5í™€ì—ì„œ ê°œìµœë˜ëŠ”ë°, ì‚¼ì„±ì „ì ë“± ì£¼ìš” ê¸°ì—…ê³¼ ë‚˜ë…¸ ê¸°ìˆ  ê¸°ì—… ë“± ì´ 360ê°œì‚¬ê°€ ì°¸ì—¬í•  ì˜ˆì •ì´ë©° ë‚˜ë…¸ê¸°ìˆ ê³¼ ì‚°ì—…ì˜ í˜„ì¬ ë¯¸ë˜ íŠ¸ëœë“œë¥¼ ì¡°ë§í•˜ëŠ” ê¸°ì¡° ê°•ì—°ì„ ì‹œì‘ìœ¼ë¡œ ë‚˜ë…¸ ìœµí•© ì „ì‹œí™”ì™€ êµ­ì œ ì‹¬í¬ì§€ì—„ í–‰ì‚¬ê°€ ë‹¤ì–‘í•˜ê²Œ ê°œìµœëœë‹¤.<|eot_id|>\n",
      "You are an AI assistant specialized in news articles.Your role is to provide accurate summaries and insights in Korean. Please analyze the given text and provide concise, informative summaries that highlight the key goals and findings.\n",
      "\n",
      "Human: Please summarize the goals for journalist in this text:\n",
      "\n",
      "4ì¼ë¶€í„° ì„ë‹¬ê°„ ì¦ê¶Œì‚¬ ì‹ ìš©ìœµìë‹´ë³´ë¹„ìœ¨ ìœ ì§€ì˜ë¬´ ë©´ì œ ê¸ˆìœµë‹¹êµ­ì´ ì½”ìŠ¤í”¼ì§€ìˆ˜ê°€ ì¥ì¤‘ 2300 ì•„ë˜ê¹Œì§€ ë–¨ì–´ì§€ì ì£¼ì‹ì‹œì¥ ë³€ë™ì„±ì„ ì™„í™”í•˜ëŠ” ì¡°ì¹˜ë¥¼ ì‹œí–‰í•˜ê¸°ë¡œ í–ˆë‹¤. ê¸ˆìœµìœ„ì›íšŒëŠ” 1ì¼ ì£¼ì‹ì‹œì¥ ë§ˆê° í›„ ê¹€ì†Œì˜ ë¶€ìœ„ì›ì¥ ì£¼ì¬ë¡œ ì¦ê¶Œ ìœ ê´€ê¸°ê´€ê³¼ ê¸ˆìœµì‹œì¥í•©ë™ì ê²€íšŒì˜ë¥¼ ì—´ê³  ë³€ë™ì„± ì™„í™”ì¡°ì¹˜ë¥¼ ì‹œí–‰í•˜ê¸°ë¡œ í–ˆë‹¤ê³  ë°í˜”ë‹¤. ì´ì— ë”°ë¼ ë‹¤ìŒ ì£¼ì‹ì‹œì¥ ê°œì¥ì¼ì¸ ì˜¤ëŠ” 4ì¼ë¶€í„° 9ì›”30ì¼ê¹Œì§€ 3ê°œì›”ê°„ ì¦ê¶Œì‚¬ì˜ ì‹ ìš©ìœµìë‹´ë³´ë¹„ìœ¨ ìœ ì§€ì˜ë¬´ê°€ ë©´ì œëœë‹¤. ì‹ ìš©ìœµìë‹´ë³´ë¹„ìœ¨ ìœ ì§€ì˜ë¬´ë€ ì¦ê¶Œì‚¬ê°€ ì¼ëª… â€˜ë¹šíˆ¬â€™ ë¹šë‚´ì„œ íˆ¬ì ìê¸ˆì¸ ì‹ ìš©ìœµìë¥¼ ì‹œí–‰í•  ë•Œ ë‹´ë³´ë¥¼ 140% ì´ìƒ í™•ë³´í•˜ê³  ë‚´ê·œì—ì„œ ì •í•œ ë‹´ë³´ë¹„ìœ¨ì„ ìœ ì§€í•  ê²ƒì„ ìš”êµ¬í•˜ëŠ” ê·œì œë‹¤. ìœ ì§€ì˜ë¬´ ë©´ì œëŠ” ì¦ê¶Œì‚¬ì˜ ì‹ ìš©ìœµì ë‹´ë³´ì£¼ì‹ì— ëŒ€í•œ ê³¼ë„í•œ ë°˜ëŒ€ë§¤ë§¤ë¥¼ ì–µì œí•˜ê¸° ìœ„í•œ ì¡°ì¹˜ë‹¤. ê¸ˆìœµë‹¹êµ­ì€ ì½”ë¡œë‚˜19 ì‚¬íƒœê°€ ë³¸ê²©í™”í•œ 2020ë…„ 3ì›”ì—ë„ ì´ ê°™ì€ ì¡°ì¹˜ë¥¼ 6ê°œì›”ê°„ ì‹œí–‰í•œ ë°” ìˆë‹¤. 7ì¼ë¶€í„° 10ì›”6ì¼ê¹Œì§€ëŠ” ìƒì¥ê¸°ì—…ì˜ í•˜ë£¨ ìê¸°ì£¼ì‹ ë§¤ìˆ˜ ì£¼ë¬¸ ìˆ˜ëŸ‰ í•œë„ ì œí•œë„ ì™„í™”ëœë‹¤. ì‹ íƒì·¨ë“ ì£¼ì‹ë„ ë°œí–‰ì£¼ì‹ì´ìˆ˜ì˜ 1% ì´ë‚´ì—ì„œ ì‹ íƒì¬ì‚° ì´ì•¡ ë²”ìœ„ ë‚´ë¡œ í•œì‹œì ìœ¼ë¡œ í™•ëŒ€ëœë‹¤. ì´ì™€ í•¨ê»˜ ê¸ˆìœµìœ„ëŠ” ê¸ˆìœµê°ë…ì›ê³¼ í•œêµ­ê±°ë˜ì†Œ í•©ë™ìœ¼ë¡œ ê³µë§¤ë„ íŠ¹ë³„ì ê²€ì„ ì‹¤ì‹œí•´ ê³µë§¤ë„ í˜„í™©ê³¼ ì‹œì¥êµë€ ê°€ëŠ¥ì„± ë“±ì„ ì‚´í´ë³´ê¸°ë¡œ í–ˆë‹¤. í˜„ì¬ ê³µë§¤ë„ëŠ” ì§€ë‚œí•´ 5ì›” ì´í›„ ì œí•œì ìœ¼ë¡œ ì‹¤ì‹œë˜ê³  ìˆë‹¤. ê¸ˆìœµìœ„ì™€ ê¸ˆê°ì›ì€ ë§¤ì£¼ ê¸ˆìš”ì¼ ê¸ˆìœµì‹œì¥í•©ë™ì ê²€íšŒì˜ë¥¼ ì—´ì–´ ì¦ì‹œ ë“± ê¸ˆìœµì‹œì¥ìƒí™©ì„ ì ê²€í•  ì˜ˆì •ì´ë‹¤. ê¸ˆìœµë‹¹êµ­ì€ â€œì»¨í‹´ì „ì‹œí”Œëœì— ë”°ë¼ í•„ìš”í•œ ì‹œì¥ ë³€ë™ì„± ì™„í™” ì¡°ì¹˜ë¥¼ ê²€í† Â·ì‹œí–‰í•´ ë‚˜ê°ˆ ê²ƒâ€ì´ë¼ê³  ë§í–ˆë‹¤.<|eot_id|>\n",
      "\n",
      "Assistant: 1ì¼ 1ì¼ ê¸ˆìœµìœ„ì›íšŒëŠ” ì¦ê¶Œ ìœ ê´€ê¸°ê´€ê³¼ ê¸ˆìœµì‹œì¥í•©ë™ì ê²€íšŒì˜ë¥¼ ì—´ê³  ì½”ìŠ¤í”¼ì§€ìˆ˜ê°€ ì¥ì¤‘ 2300 ì•„ë˜ê¹Œì§€ ë–¨ì–´ì§€ì ì£¼ì‹ì‹œì¥ ë³€ë™ì„±ì„ ì™„í™”í•˜ëŠ” ì¡°ì¹˜ë¥¼ ì‹œí–‰í•˜ê¸°ë¡œ í•˜ì˜€ìœ¼ë©°, ì´ì— ë”°ë¼ ë‹¤ìŒ ì£¼ì‹ì‹œì¥ ê°œì¥ì¼ì¸ ì˜¤ëŠ” 4ì¼ë¶€í„° 9ì›”30ì¼ê¹Œì§€ 3ê°œì›”ê°„ ì¦ê¶Œì‚¬ì˜ ì‹ ìš©ìœµìë‹´ë³´ë¹„ìœ¨ ìœ ì§€ì˜ë¬´ê°€ ë©´ì œëœë‹¤.<|eot_id|>\n",
      "/home/ec2-user/anaconda3/envs/finetune_fsdp_image/lib/python3.10/site-packages/huggingface_hub/file_download.py:1142: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/home/ec2-user/anaconda3/envs/finetune_fsdp_image/lib/python3.10/site-packages/huggingface_hub/file_download.py:1142: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/home/ec2-user/anaconda3/envs/finetune_fsdp_image/lib/python3.10/site-packages/huggingface_hub/file_download.py:1142: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "You are an AI assistant specialized in news articles.Your role is to provide accurate summaries and insights in Korean. Please analyze the given text and provide concise, informative summaries that highlight the key goals and findings.\n",
      "\n",
      "Human: Please summarize the goals for journalist in this text:\n",
      "\n",
      "êµ­ë‚´ ìµœëŒ€ê·œëª¨ ë‚˜ë…¸ ì „ì‹œíšŒì´ì ì„¸ê³„ 3ëŒ€ ë‚˜ë…¸í–‰ì‚¬ ë‚˜ë…¸ì½”ë¦¬ì•„ 2022 ê°€ 6ì¼ë¶€í„° 3ì¼ê°„ ê²½ê¸°ë„ í‚¨í…ìŠ¤ ì œ1ì „ì‹œì¥ 4Â·5í™€ ì—ì„œ ê°œìµœëœë‹¤. ì˜¬í•´ 20íšŒë¥¼ ë§ëŠ” ë‚˜ë…¸ì½”ë¦¬ì•„ëŠ” ì‚°ì—…í†µìƒìì›ë¶€ ê³¼í•™ê¸°ìˆ ì •ë³´í†µì‹ ë¶€ê°€ ê³µë™ ì£¼ìµœí•œë‹¤. ë‚˜ë…¸ìœµí•©ì‚°ì—…ì—°êµ¬ì¡°í•©ê³¼ ë‚˜ë…¸ê¸°ìˆ ì—°êµ¬í˜‘ì˜íšŒê°€ ì£¼ê´€í•œë‹¤. ë‚˜ë…¸ê¸°ìˆ ê³¼ ì‚°ì—…ì˜ í˜„ì¬ ë¯¸ë˜ íŠ¸ëœë“œë¥¼ ì¡°ë§í•˜ëŠ” ê¸°ì¡° ê°•ì—°ì„ ì‹œì‘ìœ¼ë¡œ ë‚˜ë…¸ ìœµí•© ì „ì‹œí™”ì™€ êµ­ì œ ì‹¬í¬ì§€ì—„ í–‰ì‚¬ê°€ ë‹¤ì–‘í•˜ê²Œ ê°œìµœëœë‹¤. ê°•ë¯¼ì„ LGì´ë…¸í… ë¶€ì‚¬ì¥ì´ ììœ¨ì£¼í–‰ì‚°ì—… ë™í–¥ì— ë”°ë¥¸ ë‚˜ë…¸ê¸°ìˆ ê³¼ ì¸ê³µì§€ëŠ¥ AI ì˜ í™œìš© ì„ ì£¼ì œë¡œ ê¸°ì¡°ê°•ì—°ì„ í•œë‹¤. ì•Œë² ë¥´í˜ë¥´ í”„ë‘ìŠ¤ íŒŒë¦¬ ìŠˆë“œëŒ€ êµìˆ˜ë„ ê¸°ì¡° ê°•ì—°ì„ ë§¡ëŠ”ë‹¤. ì „ì‹œê·œëª¨ëŠ” ì½”ë¡œë‚˜19 ì´ì „ ìˆ˜ì¤€ìœ¼ë¡œ íšŒë³µëë‹¤. ì‚¼ì„±ì „ì LG ë“± ì£¼ìš” ê¸°ì—…ê³¼ ë‚˜ë…¸ ê¸°ìˆ  ê¸°ì—… ë“± ì´ 360ê°œì‚¬ê°€ ì°¸ì—¬í•œë‹¤. ë‚˜ë…¸ 20ì£¼ë…„ íŠ¹ë³„ ê¸°ë…ê´€ë„ ë§ˆë ¨ëœë‹¤. 20ì£¼ë…„ íŠ¹ë³„ ê¸°ë…ê´€ì—ëŠ” ì°¨ì„¸ëŒ€ ë°˜ë„ì²´ ë¯¸ë˜ì°¨ 6ì„¸ëŒ€ 6G ì´ë™í†µì‹  íƒ„ì†Œì¤‘ë¦½ ë””ì§€í„¸ ë°”ì´ì˜¤ ë“± 6ê°œ ë¶„ì•¼ í˜ì‹  ê¸°ìˆ ì´ ì†Œê°œëœë‹¤. ì‚°ì—…í™” ì„¸ì…˜ì—ì„œëŠ” ì§€ì† ê°€ëŠ¥ ì„±ì¥ì„ ìœ„í•œ ESG ë‚˜ë…¸ìœµí•©ê¸°ìˆ ì„ ì£¼ì œë¡œ ì´ˆì²­ ê°•ì—°ì´ ì—´ë¦°ë‹¤. ë‚˜ë…¸ì œí’ˆê±°ë˜ìƒë‹´íšŒ ì „ì‹œíšŒí…Œí¬ë‹ˆì»¬íˆ¬ì–´ ìµœì‹ ê¸°ìˆ ë°œí‘œíšŒ ë“± ë‹¤ì–‘í•œ ë‚˜ë…¸ ê´€ë ¨ ë¶€ëŒ€ í–‰ì‚¬ë„ ì¤€ë¹„ëœë‹¤.<|eot_id|>\n",
      "\n",
      "Assistant: êµ­ë‚´ 3ëŒ€ ë‚˜ë…¸í–‰ì‚¬ ì¤‘ í•˜ë‚˜ì¸ ë‚˜ë…¸ì½”ë¦¬ì•„ 2022 ê°€ 6ì¼ë¶€í„° 3ì¼ê°„ ê²½ê¸°ë„ í‚¨í…ìŠ¤ ì œ1ì „ì‹œì¥ 4Â·5í™€ì—ì„œ ê°œìµœë˜ëŠ”ë°, ì‚¼ì„±ì „ì ë“± ì£¼ìš” ê¸°ì—…ê³¼ ë‚˜ë…¸ ê¸°ìˆ  ê¸°ì—… ë“± ì´ 360ê°œì‚¬ê°€ ì°¸ì—¬í•  ì˜ˆì •ì´ë©° ë‚˜ë…¸ê¸°ìˆ ê³¼ ì‚°ì—…ì˜ í˜„ì¬ ë¯¸ë˜ íŠ¸ëœë“œë¥¼ ì¡°ë§í•˜ëŠ” ê¸°ì¡° ê°•ì—°ì„ ì‹œì‘ìœ¼ë¡œ ë‚˜ë…¸ ìœµí•© ì „ì‹œí™”ì™€ êµ­ì œ ì‹¬í¬ì§€ì—„ í–‰ì‚¬ê°€ ë‹¤ì–‘í•˜ê²Œ ê°œìµœëœë‹¤.<|eot_id|>You are an AI assistant specialized in news articles.Your role is to provide accurate summaries and insights in Korean. Please analyze the given text and provide concise, informative summaries that highlight the key goals and findings.\n",
      "\n",
      "Human: Please summarize the goals for journalist in this text:\n",
      "\n",
      "êµ­ë‚´ ìµœëŒ€ê·œëª¨ ë‚˜ë…¸ ì „ì‹œíšŒì´ì ì„¸ê³„ 3ëŒ€ ë‚˜ë…¸í–‰ì‚¬ ë‚˜ë…¸ì½”ë¦¬ì•„ 2022 ê°€ 6ì¼ë¶€í„° 3ì¼ê°„ ê²½ê¸°ë„ í‚¨í…ìŠ¤ ì œ1ì „ì‹œì¥ 4Â·5í™€ ì—ì„œ ê°œìµœëœë‹¤. ì˜¬í•´ 20íšŒë¥¼ ë§ëŠ” ë‚˜ë…¸ì½”ë¦¬ì•„ëŠ” ì‚°ì—…í†µìƒìì›ë¶€ ê³¼í•™ê¸°ìˆ ì •ë³´í†µì‹ ë¶€ê°€ ê³µë™ ì£¼ìµœí•œë‹¤. ë‚˜ë…¸ìœµí•©ì‚°ì—…ì—°êµ¬ì¡°í•©ê³¼ ë‚˜ë…¸ê¸°ìˆ ì—°êµ¬í˜‘ì˜íšŒê°€ ì£¼ê´€í•œë‹¤. ë‚˜ë…¸ê¸°ìˆ ê³¼ ì‚°ì—…ì˜ í˜„ì¬ ë¯¸ë˜ íŠ¸ëœë“œë¥¼ ì¡°ë§í•˜ëŠ” ê¸°ì¡° ê°•ì—°ì„ ì‹œì‘ìœ¼ë¡œ ë‚˜ë…¸ ìœµí•© ì „ì‹œí™”ì™€ êµ­ì œ ì‹¬í¬ì§€ì—„ í–‰ì‚¬ê°€ ë‹¤ì–‘í•˜ê²Œ ê°œìµœëœë‹¤. ê°•ë¯¼ì„ LGì´ë…¸í… ë¶€ì‚¬ì¥ì´ ììœ¨ì£¼í–‰ì‚°ì—… ë™í–¥ì— ë”°ë¥¸ ë‚˜ë…¸ê¸°ìˆ ê³¼ ì¸ê³µì§€ëŠ¥ AI ì˜ í™œìš© ì„ ì£¼ì œë¡œ ê¸°ì¡°ê°•ì—°ì„ í•œë‹¤. ì•Œë² ë¥´í˜ë¥´ í”„ë‘ìŠ¤ íŒŒë¦¬ ìŠˆë“œëŒ€ êµìˆ˜ë„ ê¸°ì¡° ê°•ì—°ì„ ë§¡ëŠ”ë‹¤. ì „ì‹œê·œëª¨ëŠ” ì½”ë¡œë‚˜19 ì´ì „ ìˆ˜ì¤€ìœ¼ë¡œ íšŒë³µëë‹¤. ì‚¼ì„±ì „ì LG ë“± ì£¼ìš” ê¸°ì—…ê³¼ ë‚˜ë…¸ ê¸°ìˆ  ê¸°ì—… ë“± ì´ 360ê°œì‚¬ê°€ ì°¸ì—¬í•œë‹¤. ë‚˜ë…¸ 20ì£¼ë…„ íŠ¹ë³„ ê¸°ë…ê´€ë„ ë§ˆë ¨ëœë‹¤. 20ì£¼ë…„ íŠ¹ë³„ ê¸°ë…ê´€ì—ëŠ” ì°¨ì„¸ëŒ€ ë°˜ë„ì²´ ë¯¸ë˜ì°¨ 6ì„¸ëŒ€ 6G ì´ë™í†µì‹  íƒ„ì†Œì¤‘ë¦½ ë””ì§€í„¸ ë°”ì´ì˜¤ ë“± 6ê°œ ë¶„ì•¼ í˜ì‹  ê¸°ìˆ ì´ ì†Œê°œëœë‹¤. ì‚°ì—…í™” ì„¸ì…˜ì—ì„œëŠ” ì§€ì† ê°€ëŠ¥ ì„±ì¥ì„ ìœ„í•œ ESG ë‚˜ë…¸ìœµí•©ê¸°ìˆ ì„ ì£¼ì œë¡œ ì´ˆì²­ ê°•ì—°ì´ ì—´ë¦°ë‹¤. ë‚˜ë…¸ì œí’ˆê±°ë˜ìƒë‹´íšŒ ì „ì‹œíšŒí…Œí¬ë‹ˆì»¬íˆ¬ì–´ ìµœì‹ ê¸°ìˆ ë°œí‘œíšŒ ë“± ë‹¤ì–‘í•œ ë‚˜ë…¸ ê´€ë ¨ ë¶€ëŒ€ í–‰ì‚¬ë„ ì¤€ë¹„ëœë‹¤.<|eot_id|>\n",
      "\n",
      "Assistant: êµ­ë‚´ 3ëŒ€ ë‚˜ë…¸í–‰ì‚¬ ì¤‘ í•˜ë‚˜ì¸ ë‚˜ë…¸ì½”ë¦¬ì•„ 2022 ê°€ 6ì¼ë¶€í„° 3ì¼ê°„ ê²½ê¸°ë„ í‚¨í…ìŠ¤ ì œ1ì „ì‹œì¥ 4Â·5í™€ì—ì„œ ê°œìµœë˜ëŠ”ë°, ì‚¼ì„±ì „ì ë“± ì£¼ìš” ê¸°ì—…ê³¼ ë‚˜ë…¸ ê¸°ìˆ  ê¸°ì—… ë“± ì´ 360ê°œì‚¬ê°€ ì°¸ì—¬í•  ì˜ˆì •ì´ë©° ë‚˜ë…¸ê¸°ìˆ ê³¼ ì‚°ì—…ì˜ í˜„ì¬ ë¯¸ë˜ íŠ¸ëœë“œë¥¼ ì¡°ë§í•˜ëŠ” ê¸°ì¡° ê°•ì—°ì„ ì‹œì‘ìœ¼ë¡œ ë‚˜ë…¸ ìœµí•© ì „ì‹œí™”ì™€ êµ­ì œ ì‹¬í¬ì§€ì—„ í–‰ì‚¬ê°€ ë‹¤ì–‘í•˜ê²Œ ê°œìµœëœë‹¤.<|eot_id|>You are an AI assistant specialized in news articles.Your role is to provide accurate summaries and insights in Korean. Please analyze the given text and provide concise, informative summaries that highlight the key goals and findings.\n",
      "\n",
      "Human: Please summarize the goals for journalist in this text:\n",
      "\n",
      "êµ­ë‚´ ìµœëŒ€ê·œëª¨ ë‚˜ë…¸ ì „ì‹œíšŒì´ì ì„¸ê³„ 3ëŒ€ ë‚˜ë…¸í–‰ì‚¬ ë‚˜ë…¸ì½”ë¦¬ì•„ 2022 ê°€ 6ì¼ë¶€í„° 3ì¼ê°„ ê²½ê¸°ë„ í‚¨í…ìŠ¤ ì œ1ì „ì‹œì¥ 4Â·5í™€ ì—ì„œ ê°œìµœëœë‹¤. ì˜¬í•´ 20íšŒë¥¼ ë§ëŠ” ë‚˜ë…¸ì½”ë¦¬ì•„ëŠ” ì‚°ì—…í†µìƒìì›ë¶€ ê³¼í•™ê¸°ìˆ ì •ë³´í†µì‹ ë¶€ê°€ ê³µë™ ì£¼ìµœí•œë‹¤. ë‚˜ë…¸ìœµí•©ì‚°ì—…ì—°êµ¬ì¡°í•©ê³¼ ë‚˜ë…¸ê¸°ìˆ ì—°êµ¬í˜‘ì˜íšŒê°€ ì£¼ê´€í•œë‹¤. ë‚˜ë…¸ê¸°ìˆ ê³¼ ì‚°ì—…ì˜ í˜„ì¬ ë¯¸ë˜ íŠ¸ëœë“œë¥¼ ì¡°ë§í•˜ëŠ” ê¸°ì¡° ê°•ì—°ì„ ì‹œì‘ìœ¼ë¡œ ë‚˜ë…¸ ìœµí•© ì „ì‹œí™”ì™€ êµ­ì œ ì‹¬í¬ì§€ì—„ í–‰ì‚¬ê°€ ë‹¤ì–‘í•˜ê²Œ ê°œìµœëœë‹¤. ê°•ë¯¼ì„ LGì´ë…¸í… ë¶€ì‚¬ì¥ì´ ììœ¨ì£¼í–‰ì‚°ì—… ë™í–¥ì— ë”°ë¥¸ ë‚˜ë…¸ê¸°ìˆ ê³¼ ì¸ê³µì§€ëŠ¥ AI ì˜ í™œìš© ì„ ì£¼ì œë¡œ ê¸°ì¡°ê°•ì—°ì„ í•œë‹¤. ì•Œë² ë¥´í˜ë¥´ í”„ë‘ìŠ¤ íŒŒë¦¬ ìŠˆë“œëŒ€ êµìˆ˜ë„ ê¸°ì¡° ê°•ì—°ì„ ë§¡ëŠ”ë‹¤. ì „ì‹œê·œëª¨ëŠ” ì½”ë¡œë‚˜19 ì´ì „ ìˆ˜ì¤€ìœ¼ë¡œ íšŒë³µëë‹¤. ì‚¼ì„±ì „ì LG ë“± ì£¼ìš” ê¸°ì—…ê³¼ ë‚˜ë…¸ ê¸°ìˆ  ê¸°ì—… ë“± ì´ 360ê°œì‚¬ê°€ ì°¸ì—¬í•œë‹¤. ë‚˜ë…¸ 20ì£¼ë…„ íŠ¹ë³„ ê¸°ë…ê´€ë„ ë§ˆë ¨ëœë‹¤. 20ì£¼ë…„ íŠ¹ë³„ ê¸°ë…ê´€ì—ëŠ” ì°¨ì„¸ëŒ€ ë°˜ë„ì²´ ë¯¸ë˜ì°¨ 6ì„¸ëŒ€ 6G ì´ë™í†µì‹  íƒ„ì†Œì¤‘ë¦½ ë””ì§€í„¸ ë°”ì´ì˜¤ ë“± 6ê°œ ë¶„ì•¼ í˜ì‹  ê¸°ìˆ ì´ ì†Œê°œëœë‹¤. ì‚°ì—…í™” ì„¸ì…˜ì—ì„œëŠ” ì§€ì† ê°€ëŠ¥ ì„±ì¥ì„ ìœ„í•œ ESG ë‚˜ë…¸ìœµí•©ê¸°ìˆ ì„ ì£¼ì œë¡œ ì´ˆì²­ ê°•ì—°ì´ ì—´ë¦°ë‹¤. ë‚˜ë…¸ì œí’ˆê±°ë˜ìƒë‹´íšŒ ì „ì‹œíšŒí…Œí¬ë‹ˆì»¬íˆ¬ì–´ ìµœì‹ ê¸°ìˆ ë°œí‘œíšŒ ë“± ë‹¤ì–‘í•œ ë‚˜ë…¸ ê´€ë ¨ ë¶€ëŒ€ í–‰ì‚¬ë„ ì¤€ë¹„ëœë‹¤.<|eot_id|>\n",
      "\n",
      "Assistant: êµ­ë‚´ 3ëŒ€ ë‚˜ë…¸í–‰ì‚¬ ì¤‘ í•˜ë‚˜ì¸ ë‚˜ë…¸ì½”ë¦¬ì•„ 2022 ê°€ 6ì¼ë¶€í„° 3ì¼ê°„ ê²½ê¸°ë„ í‚¨í…ìŠ¤ ì œ1ì „ì‹œì¥ 4Â·5í™€ì—ì„œ ê°œìµœë˜ëŠ”ë°, ì‚¼ì„±ì „ì ë“± ì£¼ìš” ê¸°ì—…ê³¼ ë‚˜ë…¸ ê¸°ìˆ  ê¸°ì—… ë“± ì´ 360ê°œì‚¬ê°€ ì°¸ì—¬í•  ì˜ˆì •ì´ë©° ë‚˜ë…¸ê¸°ìˆ ê³¼ ì‚°ì—…ì˜ í˜„ì¬ ë¯¸ë˜ íŠ¸ëœë“œë¥¼ ì¡°ë§í•˜ëŠ” ê¸°ì¡° ê°•ì—°ì„ ì‹œì‘ìœ¼ë¡œ ë‚˜ë…¸ ìœµí•© ì „ì‹œí™”ì™€ êµ­ì œ ì‹¬í¬ì§€ì—„ í–‰ì‚¬ê°€ ë‹¤ì–‘í•˜ê²Œ ê°œìµœëœë‹¤.<|eot_id|>\n",
      "\n",
      "\n",
      "You are an AI assistant specialized in news articles.Your role is to provide accurate summaries and insights in Korean. Please analyze the given text and provide concise, informative summaries that highlight the key goals and findings.\n",
      "\n",
      "Human: Please summarize the goals for journalist in this text:\n",
      "\n",
      "4ì¼ë¶€í„° ì„ë‹¬ê°„ ì¦ê¶Œì‚¬ ì‹ ìš©ìœµìë‹´ë³´ë¹„ìœ¨ ìœ ì§€ì˜ë¬´ ë©´ì œ ê¸ˆìœµë‹¹êµ­ì´ ì½”ìŠ¤í”¼ì§€ìˆ˜ê°€ ì¥ì¤‘ 2300 ì•„ë˜ê¹Œì§€ ë–¨ì–´ì§€ì ì£¼ì‹ì‹œì¥ ë³€ë™ì„±ì„ ì™„í™”í•˜ëŠ” ì¡°ì¹˜ë¥¼ ì‹œí–‰í•˜ê¸°ë¡œ í–ˆë‹¤. ê¸ˆìœµìœ„ì›íšŒëŠ” 1ì¼ ì£¼ì‹ì‹œì¥ ë§ˆê° í›„ ê¹€ì†Œì˜ ë¶€ìœ„ì›ì¥ ì£¼ì¬ë¡œ ì¦ê¶Œ ìœ ê´€ê¸°ê´€ê³¼ ê¸ˆìœµì‹œì¥í•©ë™ì ê²€íšŒì˜ë¥¼ ì—´ê³  ë³€ë™ì„± ì™„í™”ì¡°ì¹˜ë¥¼ ì‹œí–‰í•˜ê¸°ë¡œ í–ˆë‹¤ê³  ë°í˜”ë‹¤. ì´ì— ë”°ë¼ ë‹¤ìŒ ì£¼ì‹ì‹œì¥ ê°œì¥ì¼ì¸ ì˜¤ëŠ” 4ì¼ë¶€í„° 9ì›”30ì¼ê¹Œì§€ 3ê°œì›”ê°„ ì¦ê¶Œì‚¬ì˜ ì‹ ìš©ìœµìë‹´ë³´ë¹„ìœ¨ ìœ ì§€ì˜ë¬´ê°€ ë©´ì œëœë‹¤. ì‹ ìš©ìœµìë‹´ë³´ë¹„ìœ¨ ìœ ì§€ì˜ë¬´ë€ ì¦ê¶Œì‚¬ê°€ ì¼ëª… â€˜ë¹šíˆ¬â€™ ë¹šë‚´ì„œ íˆ¬ì ìê¸ˆì¸ ì‹ ìš©ìœµìë¥¼ ì‹œí–‰í•  ë•Œ ë‹´ë³´ë¥¼ 140% ì´ìƒ í™•ë³´í•˜ê³  ë‚´ê·œì—ì„œ ì •í•œ ë‹´ë³´ë¹„ìœ¨ì„ ìœ ì§€í•  ê²ƒì„ ìš”êµ¬í•˜ëŠ” ê·œì œë‹¤. ìœ ì§€ì˜ë¬´ ë©´ì œëŠ” ì¦ê¶Œì‚¬ì˜ ì‹ ìš©ìœµì ë‹´ë³´ì£¼ì‹ì— ëŒ€í•œ ê³¼ë„í•œ ë°˜ëŒ€ë§¤ë§¤ë¥¼ ì–µì œí•˜ê¸° ìœ„í•œ ì¡°ì¹˜ë‹¤. ê¸ˆìœµë‹¹êµ­ì€ ì½”ë¡œë‚˜19 ì‚¬íƒœê°€ ë³¸ê²©í™”í•œ 2020ë…„ 3ì›”ì—ë„ ì´ ê°™ì€ ì¡°ì¹˜ë¥¼ 6ê°œì›”ê°„ ì‹œí–‰í•œ ë°” ìˆë‹¤. 7ì¼ë¶€í„° 10ì›”6ì¼ê¹Œì§€ëŠ” ìƒì¥ê¸°ì—…ì˜ í•˜ë£¨ ìê¸°ì£¼ì‹ ë§¤ìˆ˜ ì£¼ë¬¸ ìˆ˜ëŸ‰ í•œë„ ì œí•œë„ ì™„í™”ëœë‹¤. ì‹ íƒì·¨ë“ ì£¼ì‹ë„ ë°œí–‰ì£¼ì‹ì´ìˆ˜ì˜ 1% ì´ë‚´ì—ì„œ ì‹ íƒì¬ì‚° ì´ì•¡ ë²”ìœ„ ë‚´ë¡œ í•œì‹œì ìœ¼ë¡œ í™•ëŒ€ëœë‹¤. ì´ì™€ í•¨ê»˜ ê¸ˆìœµìœ„ëŠ” ê¸ˆìœµê°ë…ì›ê³¼ í•œêµ­ê±°ë˜ì†Œ í•©ë™ìœ¼ë¡œ ê³µë§¤ë„ íŠ¹ë³„ì ê²€ì„ ì‹¤ì‹œí•´ ê³µë§¤ë„ í˜„í™©ê³¼ ì‹œì¥êµë€ ê°€ëŠ¥ì„± ë“±ì„ ì‚´í´ë³´ê¸°ë¡œ í–ˆë‹¤. í˜„ì¬ ê³µë§¤ë„ëŠ” ì§€ë‚œí•´ 5ì›” ì´í›„ ì œí•œì ìœ¼ë¡œ ì‹¤ì‹œë˜ê³  ìˆë‹¤. ê¸ˆìœµìœ„ì™€ ê¸ˆê°ì›ì€ ë§¤ì£¼ ê¸ˆìš”ì¼ ê¸ˆìœµì‹œì¥í•©ë™ì ê²€íšŒì˜ë¥¼ ì—´ì–´ ì¦ì‹œ ë“± ê¸ˆìœµì‹œì¥ìƒí™©ì„ ì ê²€í•  ì˜ˆì •ì´ë‹¤. ê¸ˆìœµë‹¹êµ­ì€ â€œì»¨í‹´ì „ì‹œí”Œëœì— ë”°ë¼ í•„ìš”í•œ ì‹œì¥ ë³€ë™ì„± ì™„í™” ì¡°ì¹˜ë¥¼ ê²€í† Â·ì‹œí–‰í•´ ë‚˜ê°ˆ ê²ƒâ€ì´ë¼ê³  ë§í–ˆë‹¤.<|eot_id|>\n",
      "\n",
      "Assistant: 1ì¼ 1ì¼ ê¸ˆìœµìœ„ì›íšŒëŠ” ì¦ê¶Œ ìœ ê´€ê¸°ê´€ê³¼ ê¸ˆìœµì‹œì¥í•©ë™ì ê²€íšŒì˜ë¥¼ ì—´ê³  ì½”ìŠ¤í”¼ì§€ìˆ˜ê°€ ì¥ì¤‘ 2300 ì•„ë˜ê¹Œì§€ ë–¨ì–´ì§€ì ì£¼ì‹ì‹œì¥ ë³€ë™ì„±ì„ ì™„í™”í•˜ëŠ” ì¡°ì¹˜ë¥¼ ì‹œí–‰í•˜ê¸°ë¡œ í•˜ì˜€ìœ¼ë©°, ì´ì— ë”°ë¼ ë‹¤ìŒ ì£¼ì‹ì‹œì¥ ê°œì¥ì¼ì¸ ì˜¤ëŠ” 4ì¼ë¶€í„° 9ì›”30ì¼ê¹Œì§€ 3ê°œì›”ê°„ ì¦ê¶Œì‚¬ì˜ ì‹ ìš©ìœµìë‹´ë³´ë¹„ìœ¨ ìœ ì§€ì˜ë¬´ê°€ ë©´ì œëœë‹¤.<|eot_id|>You are an AI assistant specialized in news articles.Your role is to provide accurate summaries and insights in Korean. Please analyze the given text and provide concise, informative summaries that highlight the key goals and findings.\n",
      "\n",
      "Human: Please summarize the goals for journalist in this text:\n",
      "\n",
      "4ì¼ë¶€í„° ì„ë‹¬ê°„ ì¦ê¶Œì‚¬ ì‹ ìš©ìœµìë‹´ë³´ë¹„ìœ¨ ìœ ì§€ì˜ë¬´ ë©´ì œ ê¸ˆìœµë‹¹êµ­ì´ ì½”ìŠ¤í”¼ì§€ìˆ˜ê°€ ì¥ì¤‘ 2300 ì•„ë˜ê¹Œì§€ ë–¨ì–´ì§€ì ì£¼ì‹ì‹œì¥ ë³€ë™ì„±ì„ ì™„í™”í•˜ëŠ” ì¡°ì¹˜ë¥¼ ì‹œí–‰í•˜ê¸°ë¡œ í–ˆë‹¤. ê¸ˆìœµìœ„ì›íšŒëŠ” 1ì¼ ì£¼ì‹ì‹œì¥ ë§ˆê° í›„ ê¹€ì†Œì˜ ë¶€ìœ„ì›ì¥ ì£¼ì¬ë¡œ ì¦ê¶Œ ìœ ê´€ê¸°ê´€ê³¼ ê¸ˆìœµì‹œì¥í•©ë™ì ê²€íšŒì˜ë¥¼ ì—´ê³  ë³€ë™ì„± ì™„í™”ì¡°ì¹˜ë¥¼ ì‹œí–‰í•˜ê¸°ë¡œ í–ˆë‹¤ê³  ë°í˜”ë‹¤. ì´ì— ë”°ë¼ ë‹¤ìŒ ì£¼ì‹ì‹œì¥ ê°œì¥ì¼ì¸ ì˜¤ëŠ” 4ì¼ë¶€í„° 9ì›”30ì¼ê¹Œì§€ 3ê°œì›”ê°„ ì¦ê¶Œì‚¬ì˜ ì‹ ìš©ìœµìë‹´ë³´ë¹„ìœ¨ ìœ ì§€ì˜ë¬´ê°€ ë©´ì œëœë‹¤. ì‹ ìš©ìœµìë‹´ë³´ë¹„ìœ¨ ìœ ì§€ì˜ë¬´ë€ ì¦ê¶Œì‚¬ê°€ ì¼ëª… â€˜ë¹šíˆ¬â€™ ë¹šë‚´ì„œ íˆ¬ì ìê¸ˆì¸ ì‹ ìš©ìœµìë¥¼ ì‹œí–‰í•  ë•Œ ë‹´ë³´ë¥¼ 140% ì´ìƒ í™•ë³´í•˜ê³  ë‚´ê·œì—ì„œ ì •í•œ ë‹´ë³´ë¹„ìœ¨ì„ ìœ ì§€í•  ê²ƒì„ ìš”êµ¬í•˜ëŠ” ê·œì œë‹¤. ìœ ì§€ì˜ë¬´ ë©´ì œëŠ” ì¦ê¶Œì‚¬ì˜ ì‹ ìš©ìœµì ë‹´ë³´ì£¼ì‹ì— ëŒ€í•œ ê³¼ë„í•œ ë°˜ëŒ€ë§¤ë§¤ë¥¼ ì–µì œí•˜ê¸° ìœ„í•œ ì¡°ì¹˜ë‹¤. ê¸ˆìœµë‹¹êµ­ì€ ì½”ë¡œë‚˜19 ì‚¬íƒœê°€ ë³¸ê²©í™”í•œ 2020ë…„ 3ì›”ì—ë„ ì´ ê°™ì€ ì¡°ì¹˜ë¥¼ 6ê°œì›”ê°„ ì‹œí–‰í•œ ë°” ìˆë‹¤. 7ì¼ë¶€í„° 10ì›”6ì¼ê¹Œì§€ëŠ” ìƒì¥ê¸°ì—…ì˜ í•˜ë£¨ ìê¸°ì£¼ì‹ ë§¤ìˆ˜ ì£¼ë¬¸ ìˆ˜ëŸ‰ í•œë„ ì œí•œë„ ì™„í™”ëœë‹¤. ì‹ íƒì·¨ë“ ì£¼ì‹ë„ ë°œí–‰ì£¼ì‹ì´ìˆ˜ì˜ 1% ì´ë‚´ì—ì„œ ì‹ íƒì¬ì‚° ì´ì•¡ ë²”ìœ„ ë‚´ë¡œ í•œì‹œì ìœ¼ë¡œ í™•ëŒ€ëœë‹¤. ì´ì™€ í•¨ê»˜ ê¸ˆìœµìœ„ëŠ” ê¸ˆìœµê°ë…ì›ê³¼ í•œêµ­ê±°ë˜ì†Œ í•©ë™ìœ¼ë¡œ ê³µë§¤ë„ íŠ¹ë³„ì ê²€ì„ ì‹¤ì‹œí•´ ê³µë§¤ë„ í˜„í™©ê³¼ ì‹œì¥êµë€ ê°€ëŠ¥ì„± ë“±ì„ ì‚´í´ë³´ê¸°ë¡œ í–ˆë‹¤. í˜„ì¬ ê³µë§¤ë„ëŠ” ì§€ë‚œí•´ 5ì›” ì´í›„ ì œí•œì ìœ¼ë¡œ ì‹¤ì‹œë˜ê³  ìˆë‹¤. ê¸ˆìœµìœ„ì™€ ê¸ˆê°ì›ì€ ë§¤ì£¼ ê¸ˆìš”ì¼ ê¸ˆìœµì‹œì¥í•©ë™ì ê²€íšŒì˜ë¥¼ ì—´ì–´ ì¦ì‹œ ë“± ê¸ˆìœµì‹œì¥ìƒí™©ì„ ì ê²€í•  ì˜ˆì •ì´ë‹¤. ê¸ˆìœµë‹¹êµ­ì€ â€œì»¨í‹´ì „ì‹œí”Œëœì— ë”°ë¼ í•„ìš”í•œ ì‹œì¥ ë³€ë™ì„± ì™„í™” ì¡°ì¹˜ë¥¼ ê²€í† Â·ì‹œí–‰í•´ ë‚˜ê°ˆ ê²ƒâ€ì´ë¼ê³  ë§í–ˆë‹¤.<|eot_id|>\n",
      "\n",
      "Assistant: 1ì¼ 1ì¼ ê¸ˆìœµìœ„ì›íšŒëŠ” ì¦ê¶Œ ìœ ê´€ê¸°ê´€ê³¼ ê¸ˆìœµì‹œì¥í•©ë™ì ê²€íšŒì˜ë¥¼ ì—´ê³  ì½”ìŠ¤í”¼ì§€ìˆ˜ê°€ ì¥ì¤‘ 2300 ì•„ë˜ê¹Œì§€ ë–¨ì–´ì§€ì ì£¼ì‹ì‹œì¥ ë³€ë™ì„±ì„ ì™„í™”í•˜ëŠ” ì¡°ì¹˜ë¥¼ ì‹œí–‰í•˜ê¸°ë¡œ í•˜ì˜€ìœ¼ë©°, ì´ì— ë”°ë¼ ë‹¤ìŒ ì£¼ì‹ì‹œì¥ ê°œì¥ì¼ì¸ ì˜¤ëŠ” 4ì¼ë¶€í„° 9ì›”30ì¼ê¹Œì§€ 3ê°œì›”ê°„ ì¦ê¶Œì‚¬ì˜ ì‹ ìš©ìœµìë‹´ë³´ë¹„ìœ¨ ìœ ì§€ì˜ë¬´ê°€ ë©´ì œëœë‹¤.<|eot_id|>\n",
      "You are an AI assistant specialized in news articles.Your role is to provide accurate summaries and insights in Korean. Please analyze the given text and provide concise, informative summaries that highlight the key goals and findings.\n",
      "\n",
      "Human: Please summarize the goals for journalist in this text:\n",
      "\n",
      "4ì¼ë¶€í„° ì„ë‹¬ê°„ ì¦ê¶Œì‚¬ ì‹ ìš©ìœµìë‹´ë³´ë¹„ìœ¨ ìœ ì§€ì˜ë¬´ ë©´ì œ ê¸ˆìœµë‹¹êµ­ì´ ì½”ìŠ¤í”¼ì§€ìˆ˜ê°€ ì¥ì¤‘ 2300 ì•„ë˜ê¹Œì§€ ë–¨ì–´ì§€ì ì£¼ì‹ì‹œì¥ ë³€ë™ì„±ì„ ì™„í™”í•˜ëŠ” ì¡°ì¹˜ë¥¼ ì‹œí–‰í•˜ê¸°ë¡œ í–ˆë‹¤. ê¸ˆìœµìœ„ì›íšŒëŠ” 1ì¼ ì£¼ì‹ì‹œì¥ ë§ˆê° í›„ ê¹€ì†Œì˜ ë¶€ìœ„ì›ì¥ ì£¼ì¬ë¡œ ì¦ê¶Œ ìœ ê´€ê¸°ê´€ê³¼ ê¸ˆìœµì‹œì¥í•©ë™ì ê²€íšŒì˜ë¥¼ ì—´ê³  ë³€ë™ì„± ì™„í™”ì¡°ì¹˜ë¥¼ ì‹œí–‰í•˜ê¸°ë¡œ í–ˆë‹¤ê³  ë°í˜”ë‹¤. ì´ì— ë”°ë¼ ë‹¤ìŒ ì£¼ì‹ì‹œì¥ ê°œì¥ì¼ì¸ ì˜¤ëŠ” 4ì¼ë¶€í„° 9ì›”30ì¼ê¹Œì§€ 3ê°œì›”ê°„ ì¦ê¶Œì‚¬ì˜ ì‹ ìš©ìœµìë‹´ë³´ë¹„ìœ¨ ìœ ì§€ì˜ë¬´ê°€ ë©´ì œëœë‹¤. ì‹ ìš©ìœµìë‹´ë³´ë¹„ìœ¨ ìœ ì§€ì˜ë¬´ë€ ì¦ê¶Œì‚¬ê°€ ì¼ëª… â€˜ë¹šíˆ¬â€™ ë¹šë‚´ì„œ íˆ¬ì ìê¸ˆì¸ ì‹ ìš©ìœµìë¥¼ ì‹œí–‰í•  ë•Œ ë‹´ë³´ë¥¼ 140% ì´ìƒ í™•ë³´í•˜ê³  ë‚´ê·œì—ì„œ ì •í•œ ë‹´ë³´ë¹„ìœ¨ì„ ìœ ì§€í•  ê²ƒì„ ìš”êµ¬í•˜ëŠ” ê·œì œë‹¤. ìœ ì§€ì˜ë¬´ ë©´ì œëŠ” ì¦ê¶Œì‚¬ì˜ ì‹ ìš©ìœµì ë‹´ë³´ì£¼ì‹ì— ëŒ€í•œ ê³¼ë„í•œ ë°˜ëŒ€ë§¤ë§¤ë¥¼ ì–µì œí•˜ê¸° ìœ„í•œ ì¡°ì¹˜ë‹¤. ê¸ˆìœµë‹¹êµ­ì€ ì½”ë¡œë‚˜19 ì‚¬íƒœê°€ ë³¸ê²©í™”í•œ 2020ë…„ 3ì›”ì—ë„ ì´ ê°™ì€ ì¡°ì¹˜ë¥¼ 6ê°œì›”ê°„ ì‹œí–‰í•œ ë°” ìˆë‹¤. 7ì¼ë¶€í„° 10ì›”6ì¼ê¹Œì§€ëŠ” ìƒì¥ê¸°ì—…ì˜ í•˜ë£¨ ìê¸°ì£¼ì‹ ë§¤ìˆ˜ ì£¼ë¬¸ ìˆ˜ëŸ‰ í•œë„ ì œí•œë„ ì™„í™”ëœë‹¤. ì‹ íƒì·¨ë“ ì£¼ì‹ë„ ë°œí–‰ì£¼ì‹ì´ìˆ˜ì˜ 1% ì´ë‚´ì—ì„œ ì‹ íƒì¬ì‚° ì´ì•¡ ë²”ìœ„ ë‚´ë¡œ í•œì‹œì ìœ¼ë¡œ í™•ëŒ€ëœë‹¤. ì´ì™€ í•¨ê»˜ ê¸ˆìœµìœ„ëŠ” ê¸ˆìœµê°ë…ì›ê³¼ í•œêµ­ê±°ë˜ì†Œ í•©ë™ìœ¼ë¡œ ê³µë§¤ë„ íŠ¹ë³„ì ê²€ì„ ì‹¤ì‹œí•´ ê³µë§¤ë„ í˜„í™©ê³¼ ì‹œì¥êµë€ ê°€ëŠ¥ì„± ë“±ì„ ì‚´í´ë³´ê¸°ë¡œ í–ˆë‹¤. í˜„ì¬ ê³µë§¤ë„ëŠ” ì§€ë‚œí•´ 5ì›” ì´í›„ ì œí•œì ìœ¼ë¡œ ì‹¤ì‹œë˜ê³  ìˆë‹¤. ê¸ˆìœµìœ„ì™€ ê¸ˆê°ì›ì€ ë§¤ì£¼ ê¸ˆìš”ì¼ ê¸ˆìœµì‹œì¥í•©ë™ì ê²€íšŒì˜ë¥¼ ì—´ì–´ ì¦ì‹œ ë“± ê¸ˆìœµì‹œì¥ìƒí™©ì„ ì ê²€í•  ì˜ˆì •ì´ë‹¤. ê¸ˆìœµë‹¹êµ­ì€ â€œì»¨í‹´ì „ì‹œí”Œëœì— ë”°ë¼ í•„ìš”í•œ ì‹œì¥ ë³€ë™ì„± ì™„í™” ì¡°ì¹˜ë¥¼ ê²€í† Â·ì‹œí–‰í•´ ë‚˜ê°ˆ ê²ƒâ€ì´ë¼ê³  ë§í–ˆë‹¤.<|eot_id|>\n",
      "\n",
      "Assistant: 1ì¼ 1ì¼ ê¸ˆìœµìœ„ì›íšŒëŠ” ì¦ê¶Œ ìœ ê´€ê¸°ê´€ê³¼ ê¸ˆìœµì‹œì¥í•©ë™ì ê²€íšŒì˜ë¥¼ ì—´ê³  ì½”ìŠ¤í”¼ì§€ìˆ˜ê°€ ì¥ì¤‘ 2300 ì•„ë˜ê¹Œì§€ ë–¨ì–´ì§€ì ì£¼ì‹ì‹œì¥ ë³€ë™ì„±ì„ ì™„í™”í•˜ëŠ” ì¡°ì¹˜ë¥¼ ì‹œí–‰í•˜ê¸°ë¡œ í•˜ì˜€ìœ¼ë©°, ì´ì— ë”°ë¼ ë‹¤ìŒ ì£¼ì‹ì‹œì¥ ê°œì¥ì¼ì¸ ì˜¤ëŠ” 4ì¼ë¶€í„° 9ì›”30ì¼ê¹Œì§€ 3ê°œì›”ê°„ ì¦ê¶Œì‚¬ì˜ ì‹ ìš©ìœµìë‹´ë³´ë¹„ìœ¨ ìœ ì§€ì˜ë¬´ê°€ ë©´ì œëœë‹¤.<|eot_id|>\n",
      "\n",
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:11<00:00,  2.97s/it]\n",
      "/home/ec2-user/anaconda3/envs/finetune_fsdp_image/lib/python3.10/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': dataset_text_field, max_seq_length, packing, dataset_kwargs. Will not be supported from version '1.0.0'.\n",
      "\n",
      "Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "Loading checkpoint shards:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 3/4 [00:11<00:03,  3.80s/it]/home/ec2-user/anaconda3/envs/finetune_fsdp_image/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:195: UserWarning: You passed a `packing` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n",
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:12<00:00,  3.18s/it]\n",
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:11<00:00,  2.96s/it]\n",
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:12<00:00,  3.17s/it]\n",
      "/home/ec2-user/anaconda3/envs/finetune_fsdp_image/lib/python3.10/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': dataset_text_field, max_seq_length, packing, dataset_kwargs. Will not be supported from version '1.0.0'.\n",
      "\n",
      "Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "/home/ec2-user/anaconda3/envs/finetune_fsdp_image/lib/python3.10/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': dataset_text_field, max_seq_length, packing, dataset_kwargs. Will not be supported from version '1.0.0'.\n",
      "\n",
      "Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "/home/ec2-user/anaconda3/envs/finetune_fsdp_image/lib/python3.10/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': dataset_text_field, max_seq_length, packing, dataset_kwargs. Will not be supported from version '1.0.0'.\n",
      "\n",
      "Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "/home/ec2-user/anaconda3/envs/finetune_fsdp_image/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:195: UserWarning: You passed a `packing` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n",
      "/home/ec2-user/anaconda3/envs/finetune_fsdp_image/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:195: UserWarning: You passed a `packing` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n",
      "/home/ec2-user/anaconda3/envs/finetune_fsdp_image/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:195: UserWarning: You passed a `packing` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n",
      "/home/ec2-user/anaconda3/envs/finetune_fsdp_image/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:283: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n",
      "/home/ec2-user/anaconda3/envs/finetune_fsdp_image/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:321: UserWarning: You passed a `dataset_text_field` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n",
      "/home/ec2-user/anaconda3/envs/finetune_fsdp_image/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:327: UserWarning: You passed a `dataset_kwargs` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n",
      "/home/ec2-user/anaconda3/envs/finetune_fsdp_image/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:283: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n",
      "/home/ec2-user/anaconda3/envs/finetune_fsdp_image/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:321: UserWarning: You passed a `dataset_text_field` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n",
      "/home/ec2-user/anaconda3/envs/finetune_fsdp_image/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:327: UserWarning: You passed a `dataset_kwargs` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n",
      "/home/ec2-user/anaconda3/envs/finetune_fsdp_image/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:283: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n",
      "/home/ec2-user/anaconda3/envs/finetune_fsdp_image/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:321: UserWarning: You passed a `dataset_text_field` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n",
      "/home/ec2-user/anaconda3/envs/finetune_fsdp_image/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:327: UserWarning: You passed a `dataset_kwargs` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n",
      "/home/ec2-user/anaconda3/envs/finetune_fsdp_image/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:283: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n",
      "/home/ec2-user/anaconda3/envs/finetune_fsdp_image/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:321: UserWarning: You passed a `dataset_text_field` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n",
      "/home/ec2-user/anaconda3/envs/finetune_fsdp_image/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:327: UserWarning: You passed a `dataset_kwargs` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n",
      "Generating train split: 3 examples [00:00, 265.01 examples/s]\n",
      "Generating train split: 3 examples [00:00, 413.38 examples/s]\n",
      "trainer.is_fsdp_enabled True\n",
      "trainable params: 41,943,040 || all params: 8,072,204,288 || trainable%: 0.5196\n",
      "trainer.is_fsdp_enabled True\n",
      "trainer.is_fsdp_enabled True\n",
      "trainer.is_fsdp_enabled True\n",
      "/home/ec2-user/anaconda3/envs/finetune_fsdp_image/lib/python3.10/site-packages/accelerate/accelerator.py:1557: UserWarning: Upcasted low precision parameters in Linear because mixed precision turned on in FSDP. Affects: weight.\n",
      "  warnings.warn(\n",
      "/home/ec2-user/anaconda3/envs/finetune_fsdp_image/lib/python3.10/site-packages/accelerate/accelerator.py:1563: UserWarning: FSDP upcast of low precision parameters may affect the precision of model checkpoints.\n",
      "  warnings.warn(\n",
      "  0%|                                                     | 0/1 [00:00<?, ?it/s]/home/ec2-user/anaconda3/envs/finetune_fsdp_image/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "/home/ec2-user/anaconda3/envs/finetune_fsdp_image/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "/home/ec2-user/anaconda3/envs/finetune_fsdp_image/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "/home/ec2-user/anaconda3/envs/finetune_fsdp_image/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:17<00:00, 17.35s/it]\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 2.9633944034576416, 'eval_runtime': 2.2392, 'eval_samples_per_second': 1.34, 'eval_steps_per_second': 0.447, 'epoch': 1.0}\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:19<00:00, 17.35s/it]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 20.52it/s]\u001b[A\n",
      "                                                                                \u001b[A/home/ec2-user/anaconda3/envs/finetune_fsdp_image/lib/python3.10/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:689: FutureWarning: FSDP.state_dict_type() and FSDP.set_state_dict_type() are being deprecated. Please use APIs, get_state_dict() and set_state_dict(), which can support different parallelisms, FSDP1, FSDP2, DDP. API doc: https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_state_dict .Tutorial: https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html .\n",
      "  warnings.warn(\n",
      "/home/ec2-user/anaconda3/envs/finetune_fsdp_image/lib/python3.10/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:689: FutureWarning: FSDP.state_dict_type() and FSDP.set_state_dict_type() are being deprecated. Please use APIs, get_state_dict() and set_state_dict(), which can support different parallelisms, FSDP1, FSDP2, DDP. API doc: https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_state_dict .Tutorial: https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html .\n",
      "  warnings.warn(\n",
      "/home/ec2-user/anaconda3/envs/finetune_fsdp_image/lib/python3.10/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:689: FutureWarning: FSDP.state_dict_type() and FSDP.set_state_dict_type() are being deprecated. Please use APIs, get_state_dict() and set_state_dict(), which can support different parallelisms, FSDP1, FSDP2, DDP. API doc: https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_state_dict .Tutorial: https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html .\n",
      "  warnings.warn(\n",
      "/home/ec2-user/anaconda3/envs/finetune_fsdp_image/lib/python3.10/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:689: FutureWarning: FSDP.state_dict_type() and FSDP.set_state_dict_type() are being deprecated. Please use APIs, get_state_dict() and set_state_dict(), which can support different parallelisms, FSDP1, FSDP2, DDP. API doc: https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_state_dict .Tutorial: https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html .\n",
      "  warnings.warn(\n",
      "/home/ec2-user/anaconda3/envs/finetune_fsdp_image/lib/python3.10/site-packages/huggingface_hub/file_download.py:1142: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/home/ec2-user/anaconda3/envs/finetune_fsdp_image/lib/python3.10/site-packages/huggingface_hub/file_download.py:1142: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/home/ec2-user/anaconda3/envs/finetune_fsdp_image/lib/python3.10/site-packages/huggingface_hub/file_download.py:1142: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/home/ec2-user/anaconda3/envs/finetune_fsdp_image/lib/python3.10/site-packages/huggingface_hub/file_download.py:1142: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "{'train_runtime': 42.7643, 'train_samples_per_second': 0.07, 'train_steps_per_second': 0.023, 'train_loss': 4.180337905883789, 'epoch': 1.0}\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:42<00:00, 42.76s/it]\n",
      "/home/ec2-user/anaconda3/envs/finetune_fsdp_image/lib/python3.10/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:689: FutureWarning: FSDP.state_dict_type() and FSDP.set_state_dict_type() are being deprecated. Please use APIs, get_state_dict() and set_state_dict(), which can support different parallelisms, FSDP1, FSDP2, DDP. API doc: https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_state_dict .Tutorial: https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html .\n",
      "  warnings.warn(\n",
      "/home/ec2-user/anaconda3/envs/finetune_fsdp_image/lib/python3.10/site-packages/huggingface_hub/file_download.py:1142: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "!CUDA_VISIBLE_DEVICES=0,1,2,3 ACCELERATE_USE_FSDP=1 FSDP_CPU_RAM_EFFICIENT_LOADING=1 torchrun --nproc_per_node=4 \\\n",
    "../../scripts/local_run_fsdp_qlora.py \\\n",
    "--config accelerator_config/local_llama_3_8b_fsdp_qlora.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "441607e4-f93f-438e-8985-99a76233fe47",
   "metadata": {},
   "source": [
    "## 4. ë² ì´ìŠ¤ ëª¨ë¸ê³¼ í›ˆë ¨ëœ ëª¨ë¸ ë¨¸ì§€"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cebdb212",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_id: MLP-KTLim/llama-3-Korean-Bllossom-8B\n",
      "output_dir: /home/ec2-user/SageMaker/models/llama-3-8b-fsdp-qlora-naver-news\n"
     ]
    }
   ],
   "source": [
    "print (f'model_id: {model_id}')\n",
    "print (f'output_dir: {output_dir}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3a8acc4",
   "metadata": {},
   "source": [
    "### ëª¨ë¸ ë¨¸ì§€ ë° ë¡œì»¬ì— ì €ì¥\n",
    "- ì•½ 2ë¶„ ê±¸ë¦¼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "19543636",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:01<00:00,  2.01it/s]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from peft import AutoPeftModelForCausalLM\n",
    "import torch\n",
    "\n",
    "# Load PEFT model on CPU\n",
    "\n",
    "model = AutoPeftModelForCausalLM.from_pretrained(\n",
    "    output_dir,\n",
    "    torch_dtype=torch.float16, ## why bfloat16ì´ ì•„ë‹ˆì§€?\n",
    "    low_cpu_mem_usage=True,\n",
    ")  \n",
    "# Merge LoRA and base model and save\n",
    "merged_model = model.merge_and_unload()\n",
    "merged_model.save_pretrained(output_dir,safe_serialization=True, max_shard_size=\"2GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9d0e3827",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(device(type='cpu'), device(type='cpu'))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.device, merged_model.device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aff0ea42",
   "metadata": {},
   "source": [
    "### ë¨¸ì§€ëœ ëª¨ë¸ ë¡œë”©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a1a6607d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/finetune_fsdp_image/lib/python3.10/site-packages/huggingface_hub/file_download.py:1142: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:12<00:00,  3.18s/it]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from peft import AutoPeftModelForCausalLM\n",
    "from transformers import AutoTokenizer \n",
    "\n",
    "\n",
    "# Load Model with PEFT adapter\n",
    "model = AutoPeftModelForCausalLM.from_pretrained(\n",
    "  pretrained_model_name_or_path = output_dir,\n",
    "  torch_dtype=torch.float16,\n",
    "  quantization_config= {\"load_in_4bit\": True},\n",
    "  device_map=\"auto\"\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6908046b",
   "metadata": {},
   "source": [
    "## 5. ì¶”ë¡ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5388b34",
   "metadata": {},
   "source": [
    "### í…ŒìŠ¤íŠ¸ ë°ì´í„° ì…‹ ë¡œë”©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9eacd377",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rand_idx:  75\n",
      "messages: \n",
      " [{'content': 'You are an AI assistant specialized in news articles.Your role is to provide accurate summaries and insights in Korean. Please analyze the given text and provide concise, informative summaries that highlight the key goals and findings.', 'role': 'system'}, {'content': 'Please summarize the goals for journalist in this text:\\n\\nì‚°ì—…í˜„ì¥ì—ì„œ ì„¸ì¢…í…”ë ˆì½¤ì˜ ìŠ¤ë§ˆíŠ¸ ì•ˆì „ í”Œë«í¼ ì†”ë£¨ì…˜ ì„ í™œìš©í•˜ê³  ìˆë‹¤. ì„¸ì¢…í…”ë ˆì½¤ ì œê³µ ì„¸ì¢…í…”ë ˆì½¤ì€ íƒœì˜ê±´ì„¤ì— ì¤‘ëŒ€ì¬í•´ì²˜ë²Œë²• ëŒ€ì‘ì— ìµœì í™”ëœ ìŠ¤ë§ˆíŠ¸ ì•ˆì „ í”Œë«í¼ ì†”ë£¨ì…˜ ë‚©í’ˆ ê³„ì•½ì„ ì²´ê²°í–ˆë‹¤ê³  4ì¼ ë°í˜”ë‹¤. ìš°ì„  íƒœì˜ê±´ì„¤ ì „êµ­ ì‚°ì—…í˜„ì¥ì— ìŠ¤ë§ˆíŠ¸ ì•ˆì „ ì†”ë£¨ì…˜ 500ëŒ€ ë‚©í’ˆ ê³„ì•½ì„ ì²´ê²°í–ˆê³  ì¶”ê°€ êµ¬ì¶•ì„ í˜‘ì˜ ì¤‘ì´ë‹¤. ì§€ë‚œ 1ì›” ë³¸ê²© ì‹œí–‰ëœ ì¤‘ëŒ€ì¬í•´ì²˜ë²Œë²•ì€ ì‚°ì—…í˜„ì¥ì—ì„œ ì¸ëª…ì‚¬ê³  ë°œìƒ ì‹œ ê²½ì˜ì§„ì´ë‚˜ ë²•ì¸ì—ê²Œ ì±…ì„ì„ ë¬¼ì„ ìˆ˜ ìˆë„ë¡ ê·œì •í•œ ë²•ì´ë‹¤. í•´ë‹¹ ë²•ì€ ì•ˆì „ ë³´ê±´ ê´€ë ¨ ê´€ë¦¬ìƒì˜ ì¡°ì¹˜ êµ¬ì¶•ì„ ì˜ë¬´í™”í•˜ê³  ìˆìœ¼ë‚˜ ê¸°ì—…ì˜ í•œì •ì  ìì›ê³¼ ë¶€ì¡±í•œ ì¸ë ¥ ë¬¸ì œë¡œ ì–´ë ¤ì›€ì„ ê²ªê³  ìˆë‹¤. ì„¸ì¢…í…”ë ˆì½¤ì˜ ìŠ¤ë§ˆíŠ¸ ì•ˆì „ í”Œë«í¼ ì†”ë£¨ì…˜ì€ ì¶œì…ê´€ë¦¬ë¶€í„° CCTV ê°€ìŠ¤íƒì§€ ê°ì¢… ì„¼ì„œ ë“±ì„ í•˜ë‚˜ë¡œ í†µí•©í•´ í˜„ì¥ì„ ì¢…í•© ê´€ë¦¬í•  ìˆ˜ ìˆë‹¤. LBS ìœ„ì¹˜ê¸°ë°˜ IoT ì‚¬ë¬¼ì¸í„°ë„· ë“± ìŠ¤ë§ˆíŠ¸ ê¸°ìˆ ì„ ìœµí•©í–ˆë‹¤. ì•ˆì „ ê´€ë¦¬ ë‹´ë‹¹ìëŠ” ê° í˜„ì¥ë§ˆë‹¤ ì„¤ì¹˜ëœ ì¹´ë©”ë¼ ë° CCTV ê°œì†Œë³„ ì„¼ì„œì™€ í†µì‹  ì¸í”„ë¼ë¥¼ í†µí•´ í˜„ì¥ ì •ë³´ë¥¼ ì‹¤ì‹œê°„ìœ¼ë¡œ í™•ì¸í•˜ê³  ë¹„ìƒ ìƒí™© ì‹œì—ëŠ” ì „ì²´ í˜„ì¥ ë˜ëŠ” í•´ë‹¹ êµ¬ì—­ ìƒí™©ì‹¤ ì‹œìŠ¤í…œì´ë‚˜ ëª¨ë°”ì¼ë¡œ ê·¼ë¡œìì—ê²Œ ì•ˆì „ ì¡°ì¹˜ì‚¬í•­ì„ ì§€ì‹œí•  ìˆ˜ ìˆë‹¤. ì´ì™€ í•¨ê»˜ íƒ€ì›Œí¬ë ˆì¸ì— ì„¤ì¹˜í•œ 360ë„ ì¹´ë©”ë¼ë¥¼ í†µí•´ í˜„ì¥ì˜ ë¶ˆì•ˆì „ ìš”ì†Œë¥¼ ë°œê²¬í•˜ë©´ ê´€ê³„ìì—ê²Œ ì•Œë¦¼ì„ ë³´ë‚¼ ìˆ˜ ìˆë‹¤. ì§€í•˜ ì‘ì—…ì—ì„œëŠ” ì´ë™í˜• ìŠ¤ë§ˆíŠ¸ ì˜ìƒ ì¥ë¹„ë¡œ ì•ˆì „ ì‚¬ê°ì§€ëŒ€ë¥¼ ì‚´í•„ ìˆ˜ ìˆê³  ë°€íëœ ì‘ì—… ê³µê°„ì—ì„œëŠ” ê°€ìŠ¤ ì„¼ì„œì™€ ì‹ í˜¸ë“±í˜• ì „ê´‘íŒì„ ì„¤ì¹˜í•´ ì‹¤ì‹œê°„ìœ¼ë¡œ ìŠ¤ë§ˆíŠ¸ ìƒí™©íŒì— ê°€ìŠ¤ ë†ë„ë¥¼ ì „ì†¡í•œë‹¤. ìœ í•´ê°€ìŠ¤ê°€ í—ˆìš© ë†ë„ë¥¼ ì´ˆê³¼í•˜ë©´ í˜„ì¥ì—ì„œ í™˜ê¸° ì‹œìŠ¤í…œì´ ìë™ìœ¼ë¡œ ì‘ë™í•œë‹¤. í˜„ì¥ ë‚´ ì¶”ë½ ì‚¬ê³ ê°€ ë°œìƒí•  ìˆ˜ ìˆëŠ” ê°œêµ¬ë¶€ì— ë¶€ì°©í•œ ì„¼ì„œëŠ” ê°œêµ¬ë¶€ê°€ ë¹„ì •ìƒì ìœ¼ë¡œ ê°œíëì„ ë•Œ ê²½ê³ ìŒì„ ë³´ë‚´ ìœ„í—˜ìƒí™©ì„ ì•Œë¦°ë‹¤. ê°•íš¨ìƒ ì„¸ì¢…í…”ë ˆì½¤ í†µì‹ ì‚¬ì—…ë³¸ë¶€ì¥ì€ ì¤‘ëŒ€ì¬í•´ì²˜ë²Œë²• ì‹œí–‰ì— ë”°ë¥¸ ê±´ì„¤ í˜„ì¥ì˜ ì•ˆì „ì‚¬ê³  ì˜ˆë°©ì„ ìœ„í•œ ìµœì ì˜ ì†”ë£¨ì…˜ì´ ìš”êµ¬ë˜ê³  ìˆëŠ” ì‹œì ì—ì„œ ìŠ¤ë§ˆíŠ¸ ì•ˆì „ ì†”ë£¨ì…˜ ì˜ˆë°© í”Œë«í¼ êµ¬ì¶•ì˜ ì˜ì˜ê°€ í¬ë‹¤ ê³  ë§í–ˆë‹¤.', 'role': 'user'}]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset \n",
    "from random import randint\n",
    "\n",
    "def get_message_from_dataset(sample_dataset_json_file):\n",
    "    # Load our test dataset\n",
    "    full_test_dataset = load_dataset(\"json\", data_files=sample_dataset_json_file, split=\"train\")\n",
    "\n",
    "    # Test on sample \n",
    "    rand_idx = randint(0, len(full_test_dataset)-1)\n",
    "    rand_idx = 75\n",
    "    print(\"rand_idx: \", rand_idx)\n",
    "    messages = full_test_dataset[rand_idx][\"messages\"][:2]\n",
    "    # messages = test_dataset[rand_idx][\"text\"][:2]\n",
    "    print(\"messages: \\n\", messages)\n",
    "\n",
    "    return messages, full_test_dataset, rand_idx\n",
    "\n",
    "messages, full_test_dataset, rand_idx = get_message_from_dataset(sample_dataset_json_file = full_test_data_json)    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba15b47b",
   "metadata": {},
   "source": [
    "### ì¶”ë¡ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ddaf71a0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "/home/ec2-user/anaconda3/envs/finetune_fsdp_image/lib/python3.10/site-packages/bitsandbytes/nn/modules.py:452: UserWarning: Input type into Linear4bit is torch.float16, but bnb_4bit_compute_dtype=torch.float32 (default). This will lead to slow inference or training speed.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Query:**\n",
      "Please summarize the goals for journalist in this text:\n",
      "\n",
      "ì‚°ì—…í˜„ì¥ì—ì„œ ì„¸ì¢…í…”ë ˆì½¤ì˜ ìŠ¤ë§ˆíŠ¸ ì•ˆì „ í”Œë«í¼ ì†”ë£¨ì…˜ ì„ í™œìš©í•˜ê³  ìˆë‹¤. ì„¸ì¢…í…”ë ˆì½¤ ì œê³µ ì„¸ì¢…í…”ë ˆì½¤ì€ íƒœì˜ê±´ì„¤ì— ì¤‘ëŒ€ì¬í•´ì²˜ë²Œë²• ëŒ€ì‘ì— ìµœì í™”ëœ ìŠ¤ë§ˆíŠ¸ ì•ˆì „ í”Œë«í¼ ì†”ë£¨ì…˜ ë‚©í’ˆ ê³„ì•½ì„ ì²´ê²°í–ˆë‹¤ê³  4ì¼ ë°í˜”ë‹¤. ìš°ì„  íƒœì˜ê±´ì„¤ ì „êµ­ ì‚°ì—…í˜„ì¥ì— ìŠ¤ë§ˆíŠ¸ ì•ˆì „ ì†”ë£¨ì…˜ 500ëŒ€ ë‚©í’ˆ ê³„ì•½ì„ ì²´ê²°í–ˆê³  ì¶”ê°€ êµ¬ì¶•ì„ í˜‘ì˜ ì¤‘ì´ë‹¤. ì§€ë‚œ 1ì›” ë³¸ê²© ì‹œí–‰ëœ ì¤‘ëŒ€ì¬í•´ì²˜ë²Œë²•ì€ ì‚°ì—…í˜„ì¥ì—ì„œ ì¸ëª…ì‚¬ê³  ë°œìƒ ì‹œ ê²½ì˜ì§„ì´ë‚˜ ë²•ì¸ì—ê²Œ ì±…ì„ì„ ë¬¼ì„ ìˆ˜ ìˆë„ë¡ ê·œì •í•œ ë²•ì´ë‹¤. í•´ë‹¹ ë²•ì€ ì•ˆì „ ë³´ê±´ ê´€ë ¨ ê´€ë¦¬ìƒì˜ ì¡°ì¹˜ êµ¬ì¶•ì„ ì˜ë¬´í™”í•˜ê³  ìˆìœ¼ë‚˜ ê¸°ì—…ì˜ í•œì •ì  ìì›ê³¼ ë¶€ì¡±í•œ ì¸ë ¥ ë¬¸ì œë¡œ ì–´ë ¤ì›€ì„ ê²ªê³  ìˆë‹¤. ì„¸ì¢…í…”ë ˆì½¤ì˜ ìŠ¤ë§ˆíŠ¸ ì•ˆì „ í”Œë«í¼ ì†”ë£¨ì…˜ì€ ì¶œì…ê´€ë¦¬ë¶€í„° CCTV ê°€ìŠ¤íƒì§€ ê°ì¢… ì„¼ì„œ ë“±ì„ í•˜ë‚˜ë¡œ í†µí•©í•´ í˜„ì¥ì„ ì¢…í•© ê´€ë¦¬í•  ìˆ˜ ìˆë‹¤. LBS ìœ„ì¹˜ê¸°ë°˜ IoT ì‚¬ë¬¼ì¸í„°ë„· ë“± ìŠ¤ë§ˆíŠ¸ ê¸°ìˆ ì„ ìœµí•©í–ˆë‹¤. ì•ˆì „ ê´€ë¦¬ ë‹´ë‹¹ìëŠ” ê° í˜„ì¥ë§ˆë‹¤ ì„¤ì¹˜ëœ ì¹´ë©”ë¼ ë° CCTV ê°œì†Œë³„ ì„¼ì„œì™€ í†µì‹  ì¸í”„ë¼ë¥¼ í†µí•´ í˜„ì¥ ì •ë³´ë¥¼ ì‹¤ì‹œê°„ìœ¼ë¡œ í™•ì¸í•˜ê³  ë¹„ìƒ ìƒí™© ì‹œì—ëŠ” ì „ì²´ í˜„ì¥ ë˜ëŠ” í•´ë‹¹ êµ¬ì—­ ìƒí™©ì‹¤ ì‹œìŠ¤í…œì´ë‚˜ ëª¨ë°”ì¼ë¡œ ê·¼ë¡œìì—ê²Œ ì•ˆì „ ì¡°ì¹˜ì‚¬í•­ì„ ì§€ì‹œí•  ìˆ˜ ìˆë‹¤. ì´ì™€ í•¨ê»˜ íƒ€ì›Œí¬ë ˆì¸ì— ì„¤ì¹˜í•œ 360ë„ ì¹´ë©”ë¼ë¥¼ í†µí•´ í˜„ì¥ì˜ ë¶ˆì•ˆì „ ìš”ì†Œë¥¼ ë°œê²¬í•˜ë©´ ê´€ê³„ìì—ê²Œ ì•Œë¦¼ì„ ë³´ë‚¼ ìˆ˜ ìˆë‹¤. ì§€í•˜ ì‘ì—…ì—ì„œëŠ” ì´ë™í˜• ìŠ¤ë§ˆíŠ¸ ì˜ìƒ ì¥ë¹„ë¡œ ì•ˆì „ ì‚¬ê°ì§€ëŒ€ë¥¼ ì‚´í•„ ìˆ˜ ìˆê³  ë°€íëœ ì‘ì—… ê³µê°„ì—ì„œëŠ” ê°€ìŠ¤ ì„¼ì„œì™€ ì‹ í˜¸ë“±í˜• ì „ê´‘íŒì„ ì„¤ì¹˜í•´ ì‹¤ì‹œê°„ìœ¼ë¡œ ìŠ¤ë§ˆíŠ¸ ìƒí™©íŒì— ê°€ìŠ¤ ë†ë„ë¥¼ ì „ì†¡í•œë‹¤. ìœ í•´ê°€ìŠ¤ê°€ í—ˆìš© ë†ë„ë¥¼ ì´ˆê³¼í•˜ë©´ í˜„ì¥ì—ì„œ í™˜ê¸° ì‹œìŠ¤í…œì´ ìë™ìœ¼ë¡œ ì‘ë™í•œë‹¤. í˜„ì¥ ë‚´ ì¶”ë½ ì‚¬ê³ ê°€ ë°œìƒí•  ìˆ˜ ìˆëŠ” ê°œêµ¬ë¶€ì— ë¶€ì°©í•œ ì„¼ì„œëŠ” ê°œêµ¬ë¶€ê°€ ë¹„ì •ìƒì ìœ¼ë¡œ ê°œíëì„ ë•Œ ê²½ê³ ìŒì„ ë³´ë‚´ ìœ„í—˜ìƒí™©ì„ ì•Œë¦°ë‹¤. ê°•íš¨ìƒ ì„¸ì¢…í…”ë ˆì½¤ í†µì‹ ì‚¬ì—…ë³¸ë¶€ì¥ì€ ì¤‘ëŒ€ì¬í•´ì²˜ë²Œë²• ì‹œí–‰ì— ë”°ë¥¸ ê±´ì„¤ í˜„ì¥ì˜ ì•ˆì „ì‚¬ê³  ì˜ˆë°©ì„ ìœ„í•œ ìµœì ì˜ ì†”ë£¨ì…˜ì´ ìš”êµ¬ë˜ê³  ìˆëŠ” ì‹œì ì—ì„œ ìŠ¤ë§ˆíŠ¸ ì•ˆì „ ì†”ë£¨ì…˜ ì˜ˆë°© í”Œë«í¼ êµ¬ì¶•ì˜ ì˜ì˜ê°€ í¬ë‹¤ ê³  ë§í–ˆë‹¤.\n",
      "\n",
      "**Original Answer:**\n",
      "ì„¸ì¢…í…”ë ˆì½¤ì€ íƒœì˜ê±´ì„¤ì— ì¶œì…ê´€ë¦¬ë¶€í„° CCTV ê°€ìŠ¤íƒì§€ ê°ì¢… ì„¼ì„œ ë“±ì„ í•˜ë‚˜ë¡œ í†µí•©í•´ í˜„ì¥ì„ ì¢…í•© ê´€ë¦¬í•  ìˆ˜ ìˆëŠ” ìŠ¤ë§ˆíŠ¸ ì•ˆì „ í”Œë«í¼ ì†”ë£¨ì…˜ ë‚©í’ˆ ê³„ì•½ì„ ì²´ê²°í–ˆë‹¤ê³  4ì¼ ë°í˜”ìœ¼ë©° íƒœì˜ê±´ì„¤ ì „êµ­ ì‚°ì—…í˜„ì¥ì— ìŠ¤ë§ˆíŠ¸ ì•ˆì „ ì†”ë£¨ì…˜ 500ëŒ€ ë‚©í’ˆ ê³„ì•½ì„ ì²´ê²°í–ˆê³  ì¶”ê°€ êµ¬ì¶•ì„ í˜‘ì˜ ì¤‘ì´ë‹¤.\n",
      "\n",
      "**Generated Answer:**\n",
      " Here is a summary of the goals for journalists in Korean:\n",
      "\n",
      "\"ì„¸ì¢…í…”ë ˆì½¤ì€ ì‚°ì—…í˜„ì¥ì—ì„œ ìŠ¤ë§ˆíŠ¸ ì•ˆì „ í”Œë«í¼ ì†”ë£¨ì…˜ì„ ì œê³µí•˜ê³  ìˆë‹¤. 500ëŒ€ ë‚©í’ˆ ê³„ì•½ì„ ì²´ê²°í•˜ê³  ì¶”ê°€ êµ¬ì¶•ì„ í˜‘ì˜ ì¤‘ì´ë‹¤. ì¤‘ëŒ€ì¬í•´ì²˜ë²Œë²•ì€ ì‚°ì—…í˜„ì¥ì—ì„œ ì¸ëª…ì‚¬ê³  ë°œìƒ ì‹œ ê²½ì˜ì§„ì´ë‚˜ ë²•ì¸ì—ê²Œ ì±…ì„ì„ ë¬¼ì„ ìˆ˜ ìˆë„ë¡ ê·œì •í•œ ë²•ì´ë‹¤. ì„¸ì¢…í…”ë ˆì½¤ì˜ ìŠ¤ë§ˆíŠ¸ ì•ˆì „ í”Œë«í¼ ì†”ë£¨ì…˜ì€ ì¶œì…ê´€ë¦¬ë¶€í„° CCTV ê°€ìŠ¤íƒì§€ ê°ì¢… ì„¼ì„œ ë“±ì„ í•˜ë‚˜ë¡œ í†µí•©í•´ í˜„ì¥ì„ ì¢…í•© ê´€ë¦¬í•  ìˆ˜ ìˆë‹¤. LBS ìœ„ì¹˜ê¸°ë°˜ IoT ì‚¬ë¬¼ì¸í„°ãƒãƒƒãƒˆ ë“± ìŠ¤ë§ˆíŠ¸ ê¸°ìˆ ì„ ìœµí•©í–ˆë‹¤. ì•ˆì „ ê´€ë¦¬ ë‹´ë‹¹ìëŠ” ê° í˜„ì¥ë§ˆë‹¤ ì„¤ì¹˜ëœ ì¹´ë©”ë¼ ë° CCTV ê°œì†Œë³„ ì„¼ì„œì™€ í†µì‹  ì¸í”„ë¼ë¥¼ í†µí•´ í˜„ì¥ ì •ë³´ë¥¼ ì‹¤ì‹œê°„ìœ¼ë¡œ í™•ì¸í•˜ê³  ë¹„ìƒ ìƒí™© ì‹œì—ëŠ” ì „ì²´ í˜„ì¥ ë˜ëŠ” í•´ë‹¹ êµ¬ì—­ ìƒí™©ì‹¤ ì‹œìŠ¤í…œì´ë‚˜ ëª¨ë°”ì¼ë¡œ ê·¼ë¡œìì—ê²Œ ì•ˆì „ ì¡°ì¹˜ì‚¬í•­ì„ ì§€ì‹œí•  ìˆ˜ ìˆë‹¤.\"\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def generate_response(messages, model, tokenizer, full_test_dataset, rand_idx):\n",
    "    input_ids = tokenizer.apply_chat_template(messages,add_generation_prompt=True,return_tensors=\"pt\").to(model.device)\n",
    "    outputs = model.generate(\n",
    "        input_ids,\n",
    "        max_new_tokens=512,\n",
    "        eos_token_id= tokenizer.eos_token_id,\n",
    "        do_sample=True,\n",
    "        temperature=0.6,\n",
    "        top_p=0.9,\n",
    "    )\n",
    "    response = outputs[0][input_ids.shape[-1]:]\n",
    "\n",
    "    print(f\"**Query:**\\n{full_test_dataset[rand_idx]['messages'][1]['content']}\\n\")\n",
    "    # print(f\"**Query:**\\n{test_dataset[rand_idx]['text'][1]['content']}\\n\")\n",
    "    # print(f\"**Original Answer:**\\n{test_dataset[rand_idx]['text'][2]['content']}\\n\")\n",
    "    print(f\"**Original Answer:**\\n{full_test_dataset[rand_idx]['messages'][2]['content']}\\n\")\n",
    "    print(f\"**Generated Answer:**\\n{tokenizer.decode(response,skip_special_tokens=True)}\")\n",
    "\n",
    "generate_response(messages, model, tokenizer, full_test_dataset, rand_idx)    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "360223a2-02b4-429d-8e47-6ec5ca55247e",
   "metadata": {},
   "source": [
    "### í• ë‹¹ëœ CUDA memoryë¥¼ Release"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "14070303-ea9f-40e5-a56e-cca1bf56f7f8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "del model\n",
    "del tokenizer\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f0a95c1-e28e-463d-8fba-a76a09de2105",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_finetune_fsdp_image",
   "language": "python",
   "name": "conda_finetune_fsdp_image"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "vscode": {
   "interpreter": {
    "hash": "6daafc7ae2313787fa97137de7504cfa7c5a594d29476828201b4f7d7fb5c4e1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
