{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5e0df272-7e0d-4235-8915-debd8bd732a4",
   "metadata": {},
   "source": [
    "# Korean LLM (Large Language Model) Serving on SageMaker with AWS Large Model Container DLC\n",
    "---\n",
    "\n",
    "- LLM GitHub: https://github.com/nlpai-lab/KULLM\n",
    "- Hugging Face model hub: https://huggingface.co/nlpai-lab/kullm-polyglot-5.8b-v2\n",
    "- [AWS Blog: Deploy large models on Amazon SageMaker using DJLServing and DeepSpeed model parallel inference](https://aws.amazon.com/ko/blogs/machine-learning/deploy-large-models-on-amazon-sagemaker-using-djlserving-and-deepspeed-model-parallel-inference)\n",
    "\n",
    "## Overview \n",
    "\n",
    "기존에 SageMaker 호스팅 인스턴스에서 모델 서빙 시에는 모델 아티팩트를 model.tar.gz 형식으로 아카이브하여 압축하였습니다. 이 방법은 소규모 모델에는 효과적이지만, 수백억~수천억 개의 파라미터를 가진 파운데이션 모델에는 적합하지 않습니다. (참조: https://docs.aws.amazon.com/sagemaker/latest/dg/large-model-inference-uncompressed.html)\n",
    " \n",
    "본 핸즈온에서는 PEFT로 훈련된 추가 모델 파라미터를 베이스 모델 파라미터에 병합하여 SageMaker의 LMI(Large Model Inference) 컨테이너 환경에 모델을 배포하는 법을 알아봅니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffd3227d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import sys\n",
    "sys.path.append('./utils')\n",
    "sys.path.append('./templates')\n",
    "\n",
    "from common_lib import check_packages\n",
    "check_packages()\n",
    "\n",
    "%store -r train_job_name\n",
    "\n",
    "try:\n",
    "    train_job_name\n",
    "except NameError:\n",
    "    print(\"++++++++++++++++++++++++++++++++++++++++++++++++++++++++\")\n",
    "    print(\"[ERROR] 3번 모듈 노트북(3_sm-train-lora.ipynb)을 먼저 실행해 주세요.\")\n",
    "    print(\"++++++++++++++++++++++++++++++++++++++++++++++++++++++++\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73a8a853",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import boto3\n",
    "import torch\n",
    "import sagemaker, boto3, jinja2\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.model import Model\n",
    "from sagemaker.estimator import Estimator\n",
    "from sagemaker import serializers, deserializers\n",
    "from peft import AutoPeftModelForCausalLM\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "role = sagemaker.get_execution_role()  # execution role for the endpoint\n",
    "sess = sagemaker.session.Session()  # sagemaker session for interacting with different AWS APIs\n",
    "bucket = sess.default_bucket()  # bucket to house artifacts\n",
    "region = sess._region_name \n",
    "account_id = sess.account_id()  # account_id of the current SageMaker Studio environment\n",
    "s3_client = boto3.client(\"s3\")  # client to intreract with S3 API\n",
    "sm_client = boto3.client(\"sagemaker\")  # client to intreract with SageMaker\n",
    "smr_client = boto3.client(\"sagemaker-runtime\")  # client to intreract with SageMaker Endpoints\n",
    "\n",
    "estimator = Estimator.attach(train_job_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4b639dc-b1cc-475f-9f30-fb876379b95f",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## 1. Merge PEFT model\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82d3ff2f-c8bf-4bd8-86e3-ea801c2ea983",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "local_peft_model_dir = 'model_from_sagemaker'\n",
    "\n",
    "if not os.path.exists(local_peft_model_dir):\n",
    "    os.makedirs(local_peft_model_dir)\n",
    "\n",
    "!aws s3 cp {estimator.model_data} {local_peft_model_dir}/model.tar.gz\n",
    "!tar -xzf {local_peft_model_dir}/model.tar.gz -C {local_peft_model_dir}\n",
    "!rm {local_peft_model_dir}/model.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ddab477-5c3d-49d4-b025-3ab50a335b7c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def modify_base_model_path(peft_model_dir, base_model_path):\n",
    "    with open(f\"{peft_model_dir}/adapter_config.json\", \"r\") as jsonfile:\n",
    "        data = json.load(jsonfile)\n",
    "    data[\"base_model_name_or_path\"] = base_model_path\n",
    "    with open(f\"{peft_model_dir}/adapter_config.json\", \"w\") as jsonfile:\n",
    "        json.dump(data, jsonfile)\n",
    "\n",
    "def byte_transform(byte_size, to):\n",
    "    a = {'k': 1, 'm': 2, 'g': 3, 't': 4, 'p': 5, 'e': 6}\n",
    "    r = float(byte_size)\n",
    "    for i in range(a[to]):\n",
    "        r = r / 1024\n",
    "    return round(r, 4)\n",
    "\n",
    "size = 0\n",
    "for ele in os.scandir(local_peft_model_dir):\n",
    "    size += os.path.getsize(ele)\n",
    "gb_size = byte_transform(size, 'g')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e12ae953-6057-4587-8ead-987097cc6131",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if gb_size < 0.5:\n",
    "    base_model_path = \"/home/ec2-user/SageMaker/models/kullm-polyglot-12-8b-v2\"\n",
    "    modify_base_model_path(local_peft_model_dir, base_model_path)\n",
    "    \n",
    "    local_model_dir = \"model_from_sagemaker_merged\"    \n",
    "    print(f'Save merged model: {local_model_dir}')    \n",
    "    os.makedirs(local_model_dir, exist_ok=True)\n",
    "    model = AutoPeftModelForCausalLM.from_pretrained(local_peft_model_dir, low_cpu_mem_usage=True, torch_dtype=torch.bfloat16)\n",
    "    merged_model = model.merge_and_unload()\n",
    "    merged_model.save_pretrained(local_model_dir, safe_serialization=True)\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(local_peft_model_dir)\n",
    "    tokenizer.save_pretrained(local_model_dir)\n",
    "else:\n",
    "    local_model_dir = local_peft_model_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4406229a",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## 2. Upload LLM model to S3\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26c4d12b-05b1-43d6-91c6-9de80720c534",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_prefix = \"kkulm-qlora-12-8B\"\n",
    "model_tar_dir = f\"{os.getcwd()}/model_from_sagemaker_merged\"\n",
    "\n",
    "bucket_prefix = 'ko-llms/serving'    \n",
    "s3_code_prefix = f\"{bucket_prefix}/{model_prefix}/code\"  # folder within bucket where code artifact will go\n",
    "s3_model_prefix = f\"{bucket_prefix}/{model_prefix}/model\"  # folder where model checkpoint will go\n",
    "s3_model_artifact = f\"s3://{bucket}/{s3_model_prefix}\"\n",
    "\n",
    "print(f\"S3 code prefix \\n {s3_code_prefix}\")\n",
    "print(f\"S3 model prefix: \\n {s3_model_prefix}\")\n",
    "print(f\"S3 model artifact path: \\n {s3_model_artifact}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3974a980-1452-4823-98ac-68516bae28cb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "aws configure set default.s3.max_concurrent_requests 100\n",
    "aws configure set default.s3.max_queue_size 10000\n",
    "aws configure set default.s3.multipart_threshold 1GB\n",
    "aws configure set default.s3.multipart_chunksize 64MB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "040f782f-4815-4ee9-8605-d259d61e0e97",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!aws s3 sync {model_tar_dir} {s3_model_artifact}\n",
    "print(f\"Model uploaded to --- > {s3_model_artifact}\")\n",
    "print(f\"We will set option.s3url={s3_model_artifact}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09506187-4c72-49ee-a450-53855eb2f8d3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "src_path = f\"serving_src/{model_prefix}\"\n",
    "!rm -rf {src_path}\n",
    "os.makedirs(src_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51a8cbe6-b1eb-4618-8217-d0ecd8c6428e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile {src_path}/serving.properties\n",
    "option.s3url={{s3url}}\n",
    "\n",
    "engine=DeepSpeed\n",
    "\n",
    "# passing extra options to model.py or built-in handler\n",
    "job_queue_size=100\n",
    "batch_size=1\n",
    "max_batch_delay=1\n",
    "max_idle_time=60\n",
    "\n",
    "# Built-in entrypoint\n",
    "#option.entryPoint=djl_python.deepspeed\n",
    "\n",
    "# Hugging Face model id\n",
    "#option.model_id=EleutherAI/gpt-j-6B\n",
    "\n",
    "# defines custom environment variables\n",
    "#env=SERVING_NUMBER_OF_NETTY_THREADS=2\n",
    "\n",
    "# Allows to load DeepSpeed workers in parallel\n",
    "option.parallel_loading=true\n",
    "\n",
    "# specify tensor parallel degree (number of partitions)\n",
    "option.tensor_parallel_degree=4\n",
    "\n",
    "# specify per model timeout\n",
    "option.model_loading_timeout=600\n",
    "#option.predict_timeout=240\n",
    "\n",
    "# mark the model as failure after python process crashing 10 times\n",
    "retry_threshold=0\n",
    "\n",
    "option.task=text-generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89f67d0e-6bd4-4919-a400-16f38f7ebcbb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "jinja_env = jinja2.Environment()  \n",
    "# we plug in the appropriate model location into our `serving.properties` file based on the region in which this notebook is running\n",
    "template = jinja_env.from_string(Path(f\"{src_path}/serving.properties\").open().read())\n",
    "Path(f\"{src_path}/serving.properties\").open(\"w\").write(template.render(s3url=s3_model_artifact))\n",
    "!pygmentize {src_path}/serving.properties | cat -n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ff83d21-555f-47a4-8b73-24fe97db441e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile {src_path}/model.py\n",
    "from djl_python import Input, Output\n",
    "import os\n",
    "import deepspeed\n",
    "import torch\n",
    "import logging\n",
    "from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer\n",
    "from transformers import GPTNeoXLayer\n",
    "\n",
    "predictor = None\n",
    "\n",
    "def get_model(properties):\n",
    "    \n",
    "    tp_degree = properties[\"tensor_parallel_degree\"]\n",
    "    model_location = properties[\"model_dir\"]\n",
    "    if \"model_id\" in properties:\n",
    "        model_location = properties[\"model_id\"]\n",
    "    task = properties[\"task\"]\n",
    "    \n",
    "    logging.info(f\"Loading model in {model_location}\")    \n",
    "    local_rank = int(os.getenv(\"LOCAL_RANK\", \"0\"))\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_location)\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_location,\n",
    "        torch_dtype=torch.float16,\n",
    "        low_cpu_mem_usage=True,\n",
    "    )\n",
    "    \n",
    "    model.requires_grad_(False)\n",
    "    model.eval()\n",
    "    \n",
    "    ds_config = {\n",
    "        \"tensor_parallel\": {\"tp_size\": tp_degree},\n",
    "        \"dtype\": model.dtype,\n",
    "        \"injection_policy\": {\n",
    "            GPTNeoXLayer:('attention.dense', 'mlp.dense_4h_to_h')\n",
    "        }\n",
    "    }\n",
    "    logging.info(f\"Starting DeepSpeed init with TP={tp_degree}\")        \n",
    "    model = deepspeed.init_inference(model, ds_config)  \n",
    "    \n",
    "    generator = pipeline(\n",
    "        task=task, model=model, tokenizer=tokenizer, device=local_rank\n",
    "    )\n",
    "    # https://huggingface.co/docs/hub/models-tasks\n",
    "    return generator\n",
    "    \n",
    "def handle(inputs: Input) -> None:\n",
    "    \"\"\"\n",
    "    inputs: Contains the configurations from serving.properties\n",
    "    \"\"\"    \n",
    "    global predictor\n",
    "    if not predictor:\n",
    "        predictor = get_model(inputs.get_properties())\n",
    "\n",
    "    if inputs.is_empty():\n",
    "        # Model server makes an empty call to warmup the model on startup\n",
    "        logging.info(\"is_empty\")\n",
    "        return None\n",
    "\n",
    "    data = inputs.get_as_json() #inputs.get_as_string()\n",
    "    logging.info(\"data:\", data)\n",
    "    \n",
    "    input_prompt, params = data[\"inputs\"], data[\"parameters\"]\n",
    "    result = predictor(input_prompt, **params)\n",
    "    logging.info(\"result:\", result)\n",
    "\n",
    "    return Output().add_as_json(result) #Output().add(result)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfeca701-a44f-496b-a841-bdbb354bd11c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!rm -rf model.tar.gz\n",
    "!tar czvf model.tar.gz -C {src_path} ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8f234e3-5588-4bbd-a95f-9c74aa50944e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "s3_code_artifact = sess.upload_data(\"model.tar.gz\", bucket, s3_code_prefix)\n",
    "print(f\"S3 Code or Model tar ball uploaded to --- > {s3_code_artifact}\")\n",
    "!rm -rf model.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4834546-f482-41ae-b94e-901f8be01e5e",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## 3. Serve LLM Model on SageMaker\n",
    "---\n",
    "### Create SageMaker Model\n",
    "\n",
    "SageMaker 엔드포인트 생성 매개변수 VolumeSizeInGB를 지정할 때 마운트되는 Amazon EBS(Amazon Elastic Block Store) 볼륨에 /tmp를 매핑하기 때문에 컨테이너는 인스턴스의 `/tmp` 공간에 모델을 다운로드합니다. 이때 s5cmd (https://github.com/peak/s5cmd) 를 활용하므로 대용량 모델을 빠르게 다운로드할 수 있습니다.\n",
    "볼륨 인스턴스와 함께 미리 빌드되어 제공되는 p4dn과 같은 인스턴스의 경우 컨테이너의 `/tmp`를 계속 활용할 수 있습니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf1e0f9c-3eb4-441e-8f22-1e9292abf3bb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.utils import name_from_base\n",
    "from sagemaker import image_uris\n",
    "\n",
    "img_uri = image_uris.retrieve(framework=\"djl-deepspeed\", region=region, version=\"0.23.0\")\n",
    "model_name = name_from_base(f\"{model_prefix}\")\n",
    "print(model_name)\n",
    "\n",
    "model_response = sm_client.create_model(\n",
    "    ModelName=model_name,\n",
    "    ExecutionRoleArn=role,\n",
    "    PrimaryContainer={\"Image\": img_uri, \"ModelDataUrl\": s3_code_artifact},\n",
    ")\n",
    "model_arn = model_response[\"ModelArn\"]\n",
    "print(f\"Created Model: {model_arn}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7aee43a-87cf-4a0f-ad70-5715c745d7ad",
   "metadata": {},
   "source": [
    "### Create SageMaker Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c82f3d85-070a-4cd5-bbfa-6c2ec4afd7d5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "endpoint_config_name = f\"{model_name}-config\"\n",
    "endpoint_name = f\"{model_name}-endpoint\"\n",
    "variant_name = \"variant1\"\n",
    "instance_type = \"ml.g5.12xlarge\"\n",
    "initial_instance_count = 1\n",
    "\n",
    "prod_variants = [\n",
    "    {\n",
    "        \"VariantName\": \"variant1\",\n",
    "        \"ModelName\": model_name,\n",
    "        \"InstanceType\": instance_type,\n",
    "        \"InitialInstanceCount\": initial_instance_count,\n",
    "        # \"ModelDataDownloadTimeoutInSeconds\": 2400,\n",
    "        \"ContainerStartupHealthCheckTimeoutInSeconds\": 600,\n",
    "    }\n",
    "]\n",
    "\n",
    "endpoint_config_response = sm_client.create_endpoint_config(\n",
    "    EndpointConfigName=endpoint_config_name,\n",
    "    ProductionVariants=prod_variants\n",
    ")\n",
    "\n",
    "endpoint_response = sm_client.create_endpoint(\n",
    "    EndpointName=endpoint_name, EndpointConfigName=endpoint_config_name\n",
    ")\n",
    "print(f\"Created Endpoint: {endpoint_response['EndpointArn']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdf730b4-80ca-4553-be16-60d82be22ae1",
   "metadata": {},
   "source": [
    "엔드포인트가 생성되는 동안 아래의 문서를 같이 확인해 보세요.\n",
    "- https://docs.aws.amazon.com/sagemaker/latest/dg/large-model-inference-dlc.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80136e12-0108-49ce-9689-736fdd4a5602",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from IPython.display import display, HTML\n",
    "def make_console_link(region, endpoint_name, task='[SageMaker LLM Serving]'):\n",
    "    endpoint_link = f'<b> {task} <a target=\"blank\" href=\"https://console.aws.amazon.com/sagemaker/home?region={region}#/endpoints/{endpoint_name}\">Check Endpoint Status</a></b>'   \n",
    "    return endpoint_link\n",
    "\n",
    "endpoint_link = make_console_link(region, endpoint_name)\n",
    "display(HTML(endpoint_link))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b97b0056-5d95-441f-9c5c-1ec1908008c3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time \n",
    "from inference_lib import describe_endpoint, Prompter\n",
    "describe_endpoint(endpoint_name)         "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf69e880-441f-487c-b458-8bb12df5e0bd",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## 4. Inference\n",
    "---\n",
    "\n",
    "엔드포인트를 호출할 때 이 텍스트를 JSON 페이로드 내에 제공해야 합니다. 이 JSON 페이로드에는 length, sampling strategy, output token sequence restrictions을 제어하는 데 도움이 되는 원하는 추론 매개변수가 포함될 수 있습니다. 허깅페이스 트랜스포머 transformers 라이브러리에는 [사용 가능한 페이로드 매개변수](https://huggingface.co/docs/transformers/main_classes/text_generation#transformers.GenerationConfig)의 전체 목록이 정의되어 있지만, 중요한 페이로드 매개변수는 다음과 같이 정의되어 있습니다:\n",
    "\n",
    "* **do_sample (`bool`)** – logits sampling 활성화\n",
    "* **max_new_tokens (`int`)** – 생성된 토큰의 최대 수\n",
    "* **best_of (`int`)** – best_of 개의 시퀀스를 생성하고 가장 높은 토큰 로그 확률이 있는 경우 반환\n",
    "* **repetition_penalty (`float`)** – 반복 패널티에 대한 파라미터, 1.0은 패널티가 없음을 의미하여 Greedy 서치와 동일, 커질수록 다양한 결과를 얻을 수 있으며, 자세한 사항은 [this paper](https://arxiv.org/pdf/1909.05858.pdf)을 참고\n",
    "* **return_full_text (`bool`)** – 생성된 텍스트에 프롬프트를 추가할지 여부\n",
    "* **seed (`int`)** – Random sampling seed\n",
    "* **stop_sequences (`List[str]`)** – `stop_sequences` 가 생성되면 토큰 생성을 중지\n",
    "* **temperature (`float`)** – logits 분포 모듈화에 사용되는 값\n",
    "* **top_k (`int`)** – 상위 K개 만큼 가장 높은 확률 어휘 토큰의 수\n",
    "* **top_p (`float`)** – 1 보다 작게 설정하게 되며, 상위부터 정렬했을 때 가능한 토큰들의 확률을 합산하여 `top_p` 이상의 가장 작은 집합을 유지\n",
    "* **truncate (`int`)** – 입력 토큰을 지정된 크기로 잘라냄\n",
    "* **typical_p (`float`)** – typical Decoding 양으로, 자세한 사항은 [Typical Decoding for Natural Language Generation](https://arxiv.org/abs/2202.00666)을 참고\n",
    "* **watermark (`bool`)** –  [A Watermark for Large Language Models](https://arxiv.org/abs/2301.10226)가 Watermarking\n",
    "* **decoder_input_details (`bool`)** – decoder input token logprobs와 ids를 반환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebbb0f5e-3bbd-43d1-b482-9310cc2095f9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"do_sample\": True,\n",
    "    \"max_new_tokens\": 128,\n",
    "    \"temperature\": 0.7,\n",
    "    \"top_p\": 0.9,\n",
    "    \"return_full_text\": False,\n",
    "    \"repetition_penalty\": 1.1,\n",
    "    \"presence_penalty\": None,\n",
    "    \"eos_token_id\": 2,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40280725-bfbc-4057-9d31-33aec765d606",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from inference_lib import KoLLMSageMakerEndpoint, Prompter\n",
    "ep = KoLLMSageMakerEndpoint(endpoint_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3450e7d7-9c04-44f6-9ca6-d756fc9d3757",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "instruction = \"다음 글을 알기 쉽게 요약해 주세요.\"\n",
    "context = \"\"\"\n",
    "대규모 언어 모델(LLM; Large Language Models) 분야의 발전과 LLM이 가치 있는 인사이트를 제공하는 문제 세트 수가 계속 증가하고 있다는 소식을 들어보셨을 겁니다. \n",
    "대규모 데이터 세트와 여러 작업을 통해 훈련된 대규모 모델은 훈련되지 않은 특정 작업에도 잘 일반화됩니다. 이러한 모델을 파운데이션 모델(foundation model)이라고 하며, 스탠포드 HAI 연구소(Stanford Institute for Human-Centered Artificial Intelligence)에서 처음 대중화한 용어입니다. \n",
    "이러한 파운데이션 모델은 프롬프트 엔지니어링(prompt engineering) 기법의 도움으로 잘 활용할 수 있지만, 유스케이스가 도메인에 매우 특화되어 있거나 작업의 종류가 매우 다양하여 모델을 추가로 커스터마이징해야 하는 경우가 많습니다. \n",
    "특정 도메인이나 작업에 대한 대규모 모델의 성능을 개선하기 위한 한 가지 접근 방식은 더 작은 작업별 데이터 세트로 모델을 추가로 훈련하는 것입니다. 파인 튜닝(fine-tuning)으로 알려진 이 접근 방식은 LLM의 정확도를 성공적으로 개선하지만, 모든 모델 가중치를 수정해야 합니다. \n",
    "파인 튜닝 데이터 세트 크기가 훨씬 작기 때문에 사전 훈련(pre-training)하는 것 보다 훨씬 빠르지만 여전히 상당한 컴퓨팅 성능과 메모리가 필요합니다. \n",
    "파인 튜닝은 원본 모델의 모든 파라미터 가중치를 수정하므로 비용이 많이 들고 원본 모델과 동일한 크기의 모델을 생성하므로 스토리지 용량에도 부담이 됩니다.\n",
    "\"\"\"\n",
    "payload = ep.get_payload(instruction, context, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d0706d2-2918-4e4b-8536-ed90134c5d22",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "generated_text = ep.infer(payload, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a9a65c8-ec81-4e35-afe0-69e0862c2ac0",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## 5. Clean Up\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3dc8bbf-fd0f-4a76-8c39-068fd77e56d0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!rm -rf {local_peft_model_dir} {local_model_dir} serving_src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74de01d5-8b2c-43da-bb66-3063a7289cf4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# - Delete the end point\n",
    "sm_client.delete_endpoint(EndpointName=endpoint_name)\n",
    "# - In case the end point failed we still want to delete the model\n",
    "sm_client.delete_endpoint_config(EndpointConfigName=endpoint_config_name)\n",
    "sm_client.delete_model(ModelName=model_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
