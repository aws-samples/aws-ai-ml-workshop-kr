{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "13bdd09d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proceed.\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import sys\n",
    "sys.path.append('./utils')\n",
    "sys.path.append('./templates')\n",
    "\n",
    "from common_lib import check_packages\n",
    "check_packages()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "78d58e9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/ec2-user/.config/sagemaker/config.yaml\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import boto3\n",
    "import torch\n",
    "import sagemaker, boto3, jinja2\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.model import Model\n",
    "from sagemaker.estimator import Estimator\n",
    "from sagemaker import serializers, deserializers\n",
    "from peft import AutoPeftModelForCausalLM\n",
    "from transformers import AutoTokenizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd0c4ceb",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## 1. Local에서 학습한 PEFT 모델을 로드\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "649d81d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r output_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "24d48406",
   "metadata": {},
   "outputs": [],
   "source": [
    "local_peft_model_dir = output_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d077fa9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'output'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "local_peft_model_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b7cbac44",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf081c52",
   "metadata": {},
   "source": [
    "### Model 환경설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "456e2383",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from peft import PeftModel, PeftConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98c84652",
   "metadata": {},
   "source": [
    "### 만약 아래 로컬 학습 진행한 output 폴더에 adapter_config.json 파일의 경로가 없을 경우 output 폴더를 삭제하고 로컬 학습 과정을 순서대로 진행해주세요. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7ea5093e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LoraConfig(peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path='/home/ec2-user/SageMaker/models/kullm-polyglot-12-8b-v2', revision=None, task_type='CAUSAL_LM', inference_mode=True, r=8, target_modules={'xxx', 'query_key_value'}, lora_alpha=32, lora_dropout=0.05, fan_in_fan_out=False, bias='none', modules_to_save=None, init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', loftq_config={})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "peft_model_id = local_peft_model_dir\n",
    "config = PeftConfig.from_pretrained(peft_model_id)\n",
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1117c8d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base_model_name_or_path 설정이 정상입니다. : /home/ec2-user/SageMaker/models/kullm-polyglot-12-8b-v2\n"
     ]
    }
   ],
   "source": [
    "# 경고 메시지를 출력할 조건 확인\n",
    "expected_path = '/home/ec2-user/SageMaker/models/kullm-polyglot-12-8b-v2'\n",
    "if config.base_model_name_or_path != expected_path:\n",
    "    # 조건이 충족될 때 출력할 경고 메시지\n",
    "    print(\"경고: base_model_name_or_path가 예상된 경로와 다릅니다.\")\n",
    "    print(f\"현재 경로: {config.base_model_name_or_path}\")\n",
    "    print(f\"예상 경로: {expected_path}\")\n",
    "else :\n",
    "    print(f\"base_model_name_or_path 설정이 정상입니다. : {config.base_model_name_or_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "85403ccf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b62b8ea45374b7daaeea70ca9149a6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): GPTNeoXForCausalLM(\n",
       "      (gpt_neox): GPTNeoXModel(\n",
       "        (embed_in): Embedding(30080, 5120)\n",
       "        (emb_dropout): Dropout(p=0.0, inplace=False)\n",
       "        (layers): ModuleList(\n",
       "          (0-39): 40 x GPTNeoXLayer(\n",
       "            (input_layernorm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
       "            (post_attention_layernorm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
       "            (post_attention_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (post_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (attention): GPTNeoXAttention(\n",
       "              (rotary_emb): GPTNeoXRotaryEmbedding()\n",
       "              (query_key_value): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=5120, out_features=15360, bias=True)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=5120, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=15360, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (dense): Linear4bit(in_features=5120, out_features=5120, bias=True)\n",
       "              (attention_dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (mlp): GPTNeoXMLP(\n",
       "              (dense_h_to_4h): Linear4bit(in_features=5120, out_features=20480, bias=True)\n",
       "              (dense_4h_to_h): Linear4bit(in_features=20480, out_features=5120, bias=True)\n",
       "              (act): GELUActivation()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (embed_out): Linear(in_features=5120, out_features=30080, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(config.base_model_name_or_path, quantization_config=bnb_config, device_map={\"\":0})\n",
    "model = PeftModel.from_pretrained(model, peft_model_id)\n",
    "tokenizer = AutoTokenizer.from_pretrained(config.base_model_name_or_path)\n",
    "\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cda74072",
   "metadata": {},
   "source": [
    "### 로컬 모델 호출 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8230d6c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen(x):\n",
    "    q = f\"### 질문: {x}\\n\\n### 답변:\"\n",
    "    # print(q)\n",
    "    gened = model.generate(\n",
    "        **tokenizer(\n",
    "            q, \n",
    "            return_tensors='pt', \n",
    "            return_token_type_ids=False\n",
    "        ).to('cuda'), \n",
    "        max_new_tokens=512,\n",
    "        early_stopping=True,\n",
    "        do_sample=True,\n",
    "        eos_token_id=2,\n",
    "    )\n",
    "    print(tokenizer.decode(gened[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9705329d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:430: UserWarning: `num_beams` is set to 1. However, `early_stopping` is set to `True` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `early_stopping`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### 질문: 프로그래밍을 잘하기 위한 세 가지 방법은?\n",
      "\n",
      "### 답변:\n",
      "세 가지 방법은 모두 프로그래밍 실력을 향상시키는 데 도움이 될 수 있습니다. 다음은 고려할 수 있는 몇 가지입니다:\n",
      "1. 코드 리뷰: 코드 리뷰를 할 때는 다른 개발자, 팀 동료 또는 다른 소프트웨어 엔지니어에게 피드백을 제공해야 합니다. 피드백을 제공하면 다른 개발자가 나를 위해 코드를 개선하거나 코딩 방식을 바꾸도록 영감을 줄 수 있습니다. 또한 피드백은 잘못된 코드나 문제를 진단 및 수정하는 데 도움이 될 수 있습니다.\n",
      "2. 좋은 코딩 표준을 준수하세요: 좋은 코딩 표준은 소프트웨어 코드를 잘 작성하는 데 있어 중요한 기초입니다. 다음은 좋은 코딩 표준에 대한 몇 가지 일반적인 가이드라인입니다:\n",
      "- 간결성: 가능한 한 간단하게 작성하세요. 이렇게 하면 코드를 이해하고 유지 관리하기가 더 쉬워집니다.\n",
      "변국: 변수는 적은 수의 정수나 문자열이어야 하고 다른 타입의 변수나 복합 배열은 허용되지 않습니다.\t- 테스트: 프로그래머는 다양한 데이터 유형과 시나리오에서 작동하는지 확인하기 위해 테스트를 작성해야 합니다. 코딩 표준만 준수했다면 테스트는 필요하지 않습니다.\n",
      "- 오류 처리: 오류가 발생하면 프로그래머는 명확하고 이해하기 쉬운 메시지를 통해 오류를 해결해야 합니다. 이렇게 하면 다른 개발자가 오류를 해결하고 코드를 명확하게 유지하는 데 도움이 됩니다.\n",
      "3. 좋은 디버깅: 코드를 작성한 후에는 의도된 대로 작동하는지 확인하기 위해 디버깅해야 합니다. 이렇게 하면 소프트웨어에 문제가 발생하기 전에 문제를 식별하고 해결할 수 있습니다. 디버깅을 할 때는 코드에 대해 꼼꼼히 살펴보고 모든 버그 표시기에 도달해야 합니다. 디버그 관리자를 사용하거나 프로젝트 관리자를 사용함으로써 프로그래머는 코딩에 전념하면서 디버그 프로세스를 간소화할 수 있습니다. 디버깅은 코딩의 필수 단계이니 시간을 할애해야 한다는 점을 잊지 마세요.<|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "gen('프로그래밍을 잘하기 위한 세 가지 방법은?')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3fdf7ce",
   "metadata": {},
   "source": [
    "## SageMaker Endpoint를 위한 함수 처리 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fb486208",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os.path as osp\n",
    "import json\n",
    "from typing import Union\n",
    "\n",
    "def load_template(template_name: str):\n",
    "    file_name = osp.join(\"./templates\", f\"{template_name}.json\")\n",
    "    if not osp.exists(file_name):\n",
    "        raise ValueError(f\"Can't read {file_name}\")\n",
    "    with open(file_name, encoding='utf-8') as fp:  # 'utf-8' 인코딩으로 파일을 엽니다.\n",
    "        return json.load(fp)\n",
    "\n",
    "\n",
    "def generate_prompt(template, instruction: str, input_text: Union[None, str] = None, label: Union[None, str] = None) -> str:\n",
    "    if input_text:\n",
    "        res = template[\"prompt_input\"].format(instruction=instruction, input=input_text)\n",
    "    else:\n",
    "        res = template[\"prompt_no_input\"].format(instruction=instruction)\n",
    "    if label:\n",
    "        res = f\"{res}{label}\"\n",
    "    return res\n",
    "\n",
    "def get_payload(instruction, input_text, params, template_name=\"kullm\"):\n",
    "    \n",
    "    template = load_template(template_name)\n",
    "    prompt = generate_prompt(template, instruction, input_text)\n",
    "    payload = {\n",
    "        'inputs': prompt,\n",
    "        'parameters': params\n",
    "    }\n",
    "    payload_str = json.dumps(payload)\n",
    "    return payload_str.encode(\"utf-8\")\n",
    "\n",
    "def infer(self, payload, content_type=\"application/json\", verbose=True):\n",
    "    response = self.smr_client.invoke_endpoint(\n",
    "        EndpointName=self.endpoint_name,\n",
    "        ContentType=content_type,\n",
    "        Body=payload\n",
    "    )\n",
    "\n",
    "    res = json.loads(response['Body'].read().decode(\"utf-8\"))\n",
    "    generated_text = res[0][\"generated_text\"]\n",
    "    #generated_text = self.prompter.get_response(generated_text)\n",
    "\n",
    "    generated_text = generated_text.split('###')[0]\n",
    "    if verbose:\n",
    "        pprint.pprint(f'Response: {generated_text}')\n",
    "    return generated_text\n",
    "def parse_response(query_response):\n",
    "    \n",
    "    def traverse(o, tree_types=(list, tuple)):\n",
    "        if isinstance(o, tree_types):\n",
    "            for value in o:\n",
    "                for subvalue in traverse(value, tree_types):\n",
    "                    yield subvalue\n",
    "        else:\n",
    "            yield o\n",
    "\n",
    "    data = eval(query_response)\n",
    "    \n",
    "    listRes = []\n",
    "    for value in traverse(data):\n",
    "        listRes.append(value[\"generated_text\"])\n",
    "        \n",
    "    if len(listRes) >= 2: return listRes\n",
    "    else: return listRes[0].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bfa99db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"do_sample\": True,\n",
    "    \"max_new_tokens\": 512,\n",
    "    \"temperature\": 0.7,\n",
    "    \"top_p\": 0.9,\n",
    "    \"return_full_text\": False,\n",
    "    \"repetition_penalty\": 1.1,\n",
    "    \"presence_penalty\": None,\n",
    "    \"eos_token_id\": 2,\n",
    "}\n",
    "instruction = \"다음 글을 알기 쉽게 요약해 주세요.\"\n",
    "context = \"\"\"\n",
    "대규모 언어 모델(LLM; Large Language Models) 분야의 발전과 LLM이 가치 있는 인사이트를 제공하는 문제 세트 수가 계속 증가하고 있다는 소식을 들어보셨을 겁니다. \n",
    "대규모 데이터 세트와 여러 작업을 통해 훈련된 대규모 모델은 훈련되지 않은 특정 작업에도 잘 일반화됩니다. 이러한 모델을 파운데이션 모델(foundation model)이라고 하며, 스탠포드 HAI 연구소(Stanford Institute for Human-Centered Artificial Intelligence)에서 처음 대중화한 용어입니다. \n",
    "이러한 파운데이션 모델은 프롬프트 엔지니어링(prompt engineering) 기법의 도움으로 잘 활용할 수 있지만, 유스케이스가 도메인에 매우 특화되어 있거나 작업의 종류가 매우 다양하여 모델을 추가로 커스터마이징해야 하는 경우가 많습니다. \n",
    "특정 도메인이나 작업에 대한 대규모 모델의 성능을 개선하기 위한 한 가지 접근 방식은 더 작은 작업별 데이터 세트로 모델을 추가로 훈련하는 것입니다. 파인 튜닝(fine-tuning)으로 알려진 이 접근 방식은 LLM의 정확도를 성공적으로 개선하지만, 모든 모델 가중치를 수정해야 합니다. \n",
    "파인 튜닝 데이터 세트 크기가 훨씬 작기 때문에 사전 훈련(pre-training)하는 것 보다 훨씬 빠르지만 여전히 상당한 컴퓨팅 성능과 메모리가 필요합니다. \n",
    "파인 튜닝은 원본 모델의 모든 파라미터 가중치를 수정하므로 비용이 많이 들고 원본 모델과 동일한 크기의 모델을 생성하므로 스토리지 용량에도 부담이 됩니다.\n",
    "\"\"\"\n",
    "payload = get_payload(instruction, context, params, \"kullm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "279a2b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = json.loads(payload.decode('utf-8'))\n",
    "input_prompt, params = data[\"inputs\"], data[\"parameters\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "63cd0651",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'inputs': '아래는 작업을 설명하는 명령어와 추가 컨텍스트를 제공하는 입력이 짝을 이루는 예제입니다. 요청을 적절히 완료하는 응답을 작성하세요.\\n\\n### 명령어:\\n다음 글을 알기 쉽게 요약해 주세요.\\n\\n### 입력:\\n\\n대규모 언어 모델(LLM; Large Language Models) 분야의 발전과 LLM이 가치 있는 인사이트를 제공하는 문제 세트 수가 계속 증가하고 있다는 소식을 들어보셨을 겁니다. \\n대규모 데이터 세트와 여러 작업을 통해 훈련된 대규모 모델은 훈련되지 않은 특정 작업에도 잘 일반화됩니다. 이러한 모델을 파운데이션 모델(foundation model)이라고 하며, 스탠포드 HAI 연구소(Stanford Institute for Human-Centered Artificial Intelligence)에서 처음 대중화한 용어입니다. \\n이러한 파운데이션 모델은 프롬프트 엔지니어링(prompt engineering) 기법의 도움으로 잘 활용할 수 있지만, 유스케이스가 도메인에 매우 특화되어 있거나 작업의 종류가 매우 다양하여 모델을 추가로 커스터마이징해야 하는 경우가 많습니다. \\n특정 도메인이나 작업에 대한 대규모 모델의 성능을 개선하기 위한 한 가지 접근 방식은 더 작은 작업별 데이터 세트로 모델을 추가로 훈련하는 것입니다. 파인 튜닝(fine-tuning)으로 알려진 이 접근 방식은 LLM의 정확도를 성공적으로 개선하지만, 모든 모델 가중치를 수정해야 합니다. \\n파인 튜닝 데이터 세트 크기가 훨씬 작기 때문에 사전 훈련(pre-training)하는 것 보다 훨씬 빠르지만 여전히 상당한 컴퓨팅 성능과 메모리가 필요합니다. \\n파인 튜닝은 원본 모델의 모든 파라미터 가중치를 수정하므로 비용이 많이 들고 원본 모델과 동일한 크기의 모델을 생성하므로 스토리지 용량에도 부담이 됩니다.\\n\\n\\n### 응답:\\n',\n",
       " 'parameters': {'do_sample': True,\n",
       "  'max_new_tokens': 512,\n",
       "  'temperature': 0.7,\n",
       "  'top_p': 0.9,\n",
       "  'return_full_text': False,\n",
       "  'repetition_penalty': 1.1,\n",
       "  'presence_penalty': None,\n",
       "  'eos_token_id': 2}}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b0c41890",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'아래는 작업을 설명하는 명령어와 추가 컨텍스트를 제공하는 입력이 짝을 이루는 예제입니다. 요청을 적절히 완료하는 응답을 작성하세요.\\n\\n### 명령어:\\n다음 글을 알기 쉽게 요약해 주세요.\\n\\n### 입력:\\n\\n대규모 언어 모델(LLM; Large Language Models) 분야의 발전과 LLM이 가치 있는 인사이트를 제공하는 문제 세트 수가 계속 증가하고 있다는 소식을 들어보셨을 겁니다. \\n대규모 데이터 세트와 여러 작업을 통해 훈련된 대규모 모델은 훈련되지 않은 특정 작업에도 잘 일반화됩니다. 이러한 모델을 파운데이션 모델(foundation model)이라고 하며, 스탠포드 HAI 연구소(Stanford Institute for Human-Centered Artificial Intelligence)에서 처음 대중화한 용어입니다. \\n이러한 파운데이션 모델은 프롬프트 엔지니어링(prompt engineering) 기법의 도움으로 잘 활용할 수 있지만, 유스케이스가 도메인에 매우 특화되어 있거나 작업의 종류가 매우 다양하여 모델을 추가로 커스터마이징해야 하는 경우가 많습니다. \\n특정 도메인이나 작업에 대한 대규모 모델의 성능을 개선하기 위한 한 가지 접근 방식은 더 작은 작업별 데이터 세트로 모델을 추가로 훈련하는 것입니다. 파인 튜닝(fine-tuning)으로 알려진 이 접근 방식은 LLM의 정확도를 성공적으로 개선하지만, 모든 모델 가중치를 수정해야 합니다. \\n파인 튜닝 데이터 세트 크기가 훨씬 작기 때문에 사전 훈련(pre-training)하는 것 보다 훨씬 빠르지만 여전히 상당한 컴퓨팅 성능과 메모리가 필요합니다. \\n파인 튜닝은 원본 모델의 모든 파라미터 가중치를 수정하므로 비용이 많이 들고 원본 모델과 동일한 크기의 모델을 생성하므로 스토리지 용량에도 부담이 됩니다.\\n\\n\\n### 응답:\\n'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1d876032",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "result = model.generate(\n",
    "        **tokenizer(\n",
    "            input_prompt, \n",
    "            return_tensors='pt', \n",
    "            return_token_type_ids=False\n",
    "        ).to('cuda'), \n",
    "        max_new_tokens=512,\n",
    "        early_stopping=True,\n",
    "        do_sample=True,\n",
    "        eos_token_id=2,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "533f0b0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decode_output :  아래는 작업을 설명하는 명령어와 추가 컨텍스트를 제공하는 입력이 짝을 이루는 예제입니다. 요청을 적절히 완료하는 응답을 작성하세요.\n",
      "\n",
      "### 명령어:\n",
      "다음 글을 알기 쉽게 요약해 주세요.\n",
      "\n",
      "### 입력:\n",
      "\n",
      "대규모 언어 모델(LLM; Large Language Models) 분야의 발전과 LLM이 가치 있는 인사이트를 제공하는 문제 세트 수가 계속 증가하고 있다는 소식을 들어보셨을 겁니다. \n",
      "대규모 데이터 세트와 여러 작업을 통해 훈련된 대규모 모델은 훈련되지 않은 특정 작업에도 잘 일반화됩니다. 이러한 모델을 파운데이션 모델(foundation model)이라고 하며, 스탠포드 HAI 연구소(Stanford Institute for Human-Centered Artificial Intelligence)에서 처음 대중화한 용어입니다. \n",
      "이러한 파운데이션 모델은 프롬프트 엔지니어링(prompt engineering) 기법의 도움으로 잘 활용할 수 있지만, 유스케이스가 도메인에 매우 특화되어 있거나 작업의 종류가 매우 다양하여 모델을 추가로 커스터마이징해야 하는 경우가 많습니다. \n",
      "특정 도메인이나 작업에 대한 대규모 모델의 성능을 개선하기 위한 한 가지 접근 방식은 더 작은 작업별 데이터 세트로 모델을 추가로 훈련하는 것입니다. 파인 튜닝(fine-tuning)으로 알려진 이 접근 방식은 LLM의 정확도를 성공적으로 개선하지만, 모든 모델 가중치를 수정해야 합니다. \n",
      "파인 튜닝 데이터 세트 크기가 훨씬 작기 때문에 사전 훈련(pre-training)하는 것 보다 훨씬 빠르지만 여전히 상당한 컴퓨팅 성능과 메모리가 필요합니다. \n",
      "파인 튜닝은 원본 모델의 모든 파라미터 가중치를 수정하므로 비용이 많이 들고 원본 모델과 동일한 크기의 모델을 생성하므로 스토리지 용량에도 부담이 됩니다.\n",
      "\n",
      "\n",
      "### 응답:\n",
      "이 글이 말하고자 하는 내용은 대규모 언어 모델에 대한 발전과 그들이 제공하는 인사이트입니다. 이러한 모델은 훈련되지 않은 작업에도 일반화되고, 매우 다양한 문제에 적용될 수 있습니다. 하지만 특정 도메인이나 작업에 대해 프롬프트 엔지니어링을 하려면 많은 노력이 필요하고 모델을 추가로 커스터마이징해야 하는 경우가 많습니다. 더 작은 작업에 대해 LLM 모델을 추가로 훈련하면 정확도를 높일 수 있지만, 많은 컴퓨팅 및 메모리 리소스가 필요합니다. 파인 튜닝은 원본 모델의 모든 파라미터 가중치를 수정하므로 효율적이지만 스토리지, 컴퓨팅 및 메모리 용량을 많이 사용합니다.<|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "#print(tokenizer.decode(result[0]))\n",
    "#print(result)\n",
    "\n",
    "decoded_output = tokenizer.decode(result[0])\n",
    "\n",
    "print(\"decode_output : \", decoded_output)\n",
    "# 이제 JSON 형식으로 변환\n",
    "json_data = json.dumps({\"generated_text\": decoded_output})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2b339655",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"generated_text\": \"\\\\uc544\\\\ub798\\\\ub294 \\\\uc791\\\\uc5c5\\\\uc744 \\\\uc124\\\\uba85\\\\ud558\\\\ub294 \\\\uba85\\\\ub839\\\\uc5b4\\\\uc640 \\\\ucd94\\\\uac00 \\\\ucee8\\\\ud14d\\\\uc2a4\\\\ud2b8\\\\ub97c \\\\uc81c\\\\uacf5\\\\ud558\\\\ub294 \\\\uc785\\\\ub825\\\\uc774 \\\\uc9dd\\\\uc744 \\\\uc774\\\\ub8e8\\\\ub294 \\\\uc608\\\\uc81c\\\\uc785\\\\ub2c8\\\\ub2e4. \\\\uc694\\\\uccad\\\\uc744 \\\\uc801\\\\uc808\\\\ud788 \\\\uc644\\\\ub8cc\\\\ud558\\\\ub294 \\\\uc751\\\\ub2f5\\\\uc744 \\\\uc791\\\\uc131\\\\ud558\\\\uc138\\\\uc694.\\\\n\\\\n### \\\\uba85\\\\ub839\\\\uc5b4:\\\\n\\\\ub2e4\\\\uc74c \\\\uae00\\\\uc744 \\\\uc54c\\\\uae30 \\\\uc27d\\\\uac8c \\\\uc694\\\\uc57d\\\\ud574 \\\\uc8fc\\\\uc138\\\\uc694.\\\\n\\\\n### \\\\uc785\\\\ub825:\\\\n\\\\n\\\\ub300\\\\uaddc\\\\ubaa8 \\\\uc5b8\\\\uc5b4 \\\\ubaa8\\\\ub378(LLM; Large Language Models) \\\\ubd84\\\\uc57c\\\\uc758 \\\\ubc1c\\\\uc804\\\\uacfc LLM\\\\uc774 \\\\uac00\\\\uce58 \\\\uc788\\\\ub294 \\\\uc778\\\\uc0ac\\\\uc774\\\\ud2b8\\\\ub97c \\\\uc81c\\\\uacf5\\\\ud558\\\\ub294 \\\\ubb38\\\\uc81c \\\\uc138\\\\ud2b8 \\\\uc218\\\\uac00 \\\\uacc4\\\\uc18d \\\\uc99d\\\\uac00\\\\ud558\\\\uace0 \\\\uc788\\\\ub2e4\\\\ub294 \\\\uc18c\\\\uc2dd\\\\uc744 \\\\ub4e4\\\\uc5b4\\\\ubcf4\\\\uc168\\\\uc744 \\\\uac81\\\\ub2c8\\\\ub2e4. \\\\n\\\\ub300\\\\uaddc\\\\ubaa8 \\\\ub370\\\\uc774\\\\ud130 \\\\uc138\\\\ud2b8\\\\uc640 \\\\uc5ec\\\\ub7ec \\\\uc791\\\\uc5c5\\\\uc744 \\\\ud1b5\\\\ud574 \\\\ud6c8\\\\ub828\\\\ub41c \\\\ub300\\\\uaddc\\\\ubaa8 \\\\ubaa8\\\\ub378\\\\uc740 \\\\ud6c8\\\\ub828\\\\ub418\\\\uc9c0 \\\\uc54a\\\\uc740 \\\\ud2b9\\\\uc815 \\\\uc791\\\\uc5c5\\\\uc5d0\\\\ub3c4 \\\\uc798 \\\\uc77c\\\\ubc18\\\\ud654\\\\ub429\\\\ub2c8\\\\ub2e4. \\\\uc774\\\\ub7ec\\\\ud55c \\\\ubaa8\\\\ub378\\\\uc744 \\\\ud30c\\\\uc6b4\\\\ub370\\\\uc774\\\\uc158 \\\\ubaa8\\\\ub378(foundation model)\\\\uc774\\\\ub77c\\\\uace0 \\\\ud558\\\\uba70, \\\\uc2a4\\\\ud0e0\\\\ud3ec\\\\ub4dc HAI \\\\uc5f0\\\\uad6c\\\\uc18c(Stanford Institute for Human-Centered Artificial Intelligence)\\\\uc5d0\\\\uc11c \\\\ucc98\\\\uc74c \\\\ub300\\\\uc911\\\\ud654\\\\ud55c \\\\uc6a9\\\\uc5b4\\\\uc785\\\\ub2c8\\\\ub2e4. \\\\n\\\\uc774\\\\ub7ec\\\\ud55c \\\\ud30c\\\\uc6b4\\\\ub370\\\\uc774\\\\uc158 \\\\ubaa8\\\\ub378\\\\uc740 \\\\ud504\\\\ub86c\\\\ud504\\\\ud2b8 \\\\uc5d4\\\\uc9c0\\\\ub2c8\\\\uc5b4\\\\ub9c1(prompt engineering) \\\\uae30\\\\ubc95\\\\uc758 \\\\ub3c4\\\\uc6c0\\\\uc73c\\\\ub85c \\\\uc798 \\\\ud65c\\\\uc6a9\\\\ud560 \\\\uc218 \\\\uc788\\\\uc9c0\\\\ub9cc, \\\\uc720\\\\uc2a4\\\\ucf00\\\\uc774\\\\uc2a4\\\\uac00 \\\\ub3c4\\\\uba54\\\\uc778\\\\uc5d0 \\\\ub9e4\\\\uc6b0 \\\\ud2b9\\\\ud654\\\\ub418\\\\uc5b4 \\\\uc788\\\\uac70\\\\ub098 \\\\uc791\\\\uc5c5\\\\uc758 \\\\uc885\\\\ub958\\\\uac00 \\\\ub9e4\\\\uc6b0 \\\\ub2e4\\\\uc591\\\\ud558\\\\uc5ec \\\\ubaa8\\\\ub378\\\\uc744 \\\\ucd94\\\\uac00\\\\ub85c \\\\ucee4\\\\uc2a4\\\\ud130\\\\ub9c8\\\\uc774\\\\uc9d5\\\\ud574\\\\uc57c \\\\ud558\\\\ub294 \\\\uacbd\\\\uc6b0\\\\uac00 \\\\ub9ce\\\\uc2b5\\\\ub2c8\\\\ub2e4. \\\\n\\\\ud2b9\\\\uc815 \\\\ub3c4\\\\uba54\\\\uc778\\\\uc774\\\\ub098 \\\\uc791\\\\uc5c5\\\\uc5d0 \\\\ub300\\\\ud55c \\\\ub300\\\\uaddc\\\\ubaa8 \\\\ubaa8\\\\ub378\\\\uc758 \\\\uc131\\\\ub2a5\\\\uc744 \\\\uac1c\\\\uc120\\\\ud558\\\\uae30 \\\\uc704\\\\ud55c \\\\ud55c \\\\uac00\\\\uc9c0 \\\\uc811\\\\uadfc \\\\ubc29\\\\uc2dd\\\\uc740 \\\\ub354 \\\\uc791\\\\uc740 \\\\uc791\\\\uc5c5\\\\ubcc4 \\\\ub370\\\\uc774\\\\ud130 \\\\uc138\\\\ud2b8\\\\ub85c \\\\ubaa8\\\\ub378\\\\uc744 \\\\ucd94\\\\uac00\\\\ub85c \\\\ud6c8\\\\ub828\\\\ud558\\\\ub294 \\\\uac83\\\\uc785\\\\ub2c8\\\\ub2e4. \\\\ud30c\\\\uc778 \\\\ud29c\\\\ub2dd(fine-tuning)\\\\uc73c\\\\ub85c \\\\uc54c\\\\ub824\\\\uc9c4 \\\\uc774 \\\\uc811\\\\uadfc \\\\ubc29\\\\uc2dd\\\\uc740 LLM\\\\uc758 \\\\uc815\\\\ud655\\\\ub3c4\\\\ub97c \\\\uc131\\\\uacf5\\\\uc801\\\\uc73c\\\\ub85c \\\\uac1c\\\\uc120\\\\ud558\\\\uc9c0\\\\ub9cc, \\\\ubaa8\\\\ub4e0 \\\\ubaa8\\\\ub378 \\\\uac00\\\\uc911\\\\uce58\\\\ub97c \\\\uc218\\\\uc815\\\\ud574\\\\uc57c \\\\ud569\\\\ub2c8\\\\ub2e4. \\\\n\\\\ud30c\\\\uc778 \\\\ud29c\\\\ub2dd \\\\ub370\\\\uc774\\\\ud130 \\\\uc138\\\\ud2b8 \\\\ud06c\\\\uae30\\\\uac00 \\\\ud6e8\\\\uc52c \\\\uc791\\\\uae30 \\\\ub54c\\\\ubb38\\\\uc5d0 \\\\uc0ac\\\\uc804 \\\\ud6c8\\\\ub828(pre-training)\\\\ud558\\\\ub294 \\\\uac83 \\\\ubcf4\\\\ub2e4 \\\\ud6e8\\\\uc52c \\\\ube60\\\\ub974\\\\uc9c0\\\\ub9cc \\\\uc5ec\\\\uc804\\\\ud788 \\\\uc0c1\\\\ub2f9\\\\ud55c \\\\ucef4\\\\ud4e8\\\\ud305 \\\\uc131\\\\ub2a5\\\\uacfc \\\\uba54\\\\ubaa8\\\\ub9ac\\\\uac00 \\\\ud544\\\\uc694\\\\ud569\\\\ub2c8\\\\ub2e4. \\\\n\\\\ud30c\\\\uc778 \\\\ud29c\\\\ub2dd\\\\uc740 \\\\uc6d0\\\\ubcf8 \\\\ubaa8\\\\ub378\\\\uc758 \\\\ubaa8\\\\ub4e0 \\\\ud30c\\\\ub77c\\\\ubbf8\\\\ud130 \\\\uac00\\\\uc911\\\\uce58\\\\ub97c \\\\uc218\\\\uc815\\\\ud558\\\\ubbc0\\\\ub85c \\\\ube44\\\\uc6a9\\\\uc774 \\\\ub9ce\\\\uc774 \\\\ub4e4\\\\uace0 \\\\uc6d0\\\\ubcf8 \\\\ubaa8\\\\ub378\\\\uacfc \\\\ub3d9\\\\uc77c\\\\ud55c \\\\ud06c\\\\uae30\\\\uc758 \\\\ubaa8\\\\ub378\\\\uc744 \\\\uc0dd\\\\uc131\\\\ud558\\\\ubbc0\\\\ub85c \\\\uc2a4\\\\ud1a0\\\\ub9ac\\\\uc9c0 \\\\uc6a9\\\\ub7c9\\\\uc5d0\\\\ub3c4 \\\\ubd80\\\\ub2f4\\\\uc774 \\\\ub429\\\\ub2c8\\\\ub2e4.\\\\n\\\\n\\\\n### \\\\uc751\\\\ub2f5:\\\\n\\\\uc774 \\\\uae00\\\\uc774 \\\\ub9d0\\\\ud558\\\\uace0\\\\uc790 \\\\ud558\\\\ub294 \\\\ub0b4\\\\uc6a9\\\\uc740 \\\\ub300\\\\uaddc\\\\ubaa8 \\\\uc5b8\\\\uc5b4 \\\\ubaa8\\\\ub378\\\\uc5d0 \\\\ub300\\\\ud55c \\\\ubc1c\\\\uc804\\\\uacfc \\\\uadf8\\\\ub4e4\\\\uc774 \\\\uc81c\\\\uacf5\\\\ud558\\\\ub294 \\\\uc778\\\\uc0ac\\\\uc774\\\\ud2b8\\\\uc785\\\\ub2c8\\\\ub2e4. \\\\uc774\\\\ub7ec\\\\ud55c \\\\ubaa8\\\\ub378\\\\uc740 \\\\ud6c8\\\\ub828\\\\ub418\\\\uc9c0 \\\\uc54a\\\\uc740 \\\\uc791\\\\uc5c5\\\\uc5d0\\\\ub3c4 \\\\uc77c\\\\ubc18\\\\ud654\\\\ub418\\\\uace0, \\\\ub9e4\\\\uc6b0 \\\\ub2e4\\\\uc591\\\\ud55c \\\\ubb38\\\\uc81c\\\\uc5d0 \\\\uc801\\\\uc6a9\\\\ub420 \\\\uc218 \\\\uc788\\\\uc2b5\\\\ub2c8\\\\ub2e4. \\\\ud558\\\\uc9c0\\\\ub9cc \\\\ud2b9\\\\uc815 \\\\ub3c4\\\\uba54\\\\uc778\\\\uc774\\\\ub098 \\\\uc791\\\\uc5c5\\\\uc5d0 \\\\ub300\\\\ud574 \\\\ud504\\\\ub86c\\\\ud504\\\\ud2b8 \\\\uc5d4\\\\uc9c0\\\\ub2c8\\\\uc5b4\\\\ub9c1\\\\uc744 \\\\ud558\\\\ub824\\\\uba74 \\\\ub9ce\\\\uc740 \\\\ub178\\\\ub825\\\\uc774 \\\\ud544\\\\uc694\\\\ud558\\\\uace0 \\\\ubaa8\\\\ub378\\\\uc744 \\\\ucd94\\\\uac00\\\\ub85c \\\\ucee4\\\\uc2a4\\\\ud130\\\\ub9c8\\\\uc774\\\\uc9d5\\\\ud574\\\\uc57c \\\\ud558\\\\ub294 \\\\uacbd\\\\uc6b0\\\\uac00 \\\\ub9ce\\\\uc2b5\\\\ub2c8\\\\ub2e4. \\\\ub354 \\\\uc791\\\\uc740 \\\\uc791\\\\uc5c5\\\\uc5d0 \\\\ub300\\\\ud574 LLM \\\\ubaa8\\\\ub378\\\\uc744 \\\\ucd94\\\\uac00\\\\ub85c \\\\ud6c8\\\\ub828\\\\ud558\\\\uba74 \\\\uc815\\\\ud655\\\\ub3c4\\\\ub97c \\\\ub192\\\\uc77c \\\\uc218 \\\\uc788\\\\uc9c0\\\\ub9cc, \\\\ub9ce\\\\uc740 \\\\ucef4\\\\ud4e8\\\\ud305 \\\\ubc0f \\\\uba54\\\\ubaa8\\\\ub9ac \\\\ub9ac\\\\uc18c\\\\uc2a4\\\\uac00 \\\\ud544\\\\uc694\\\\ud569\\\\ub2c8\\\\ub2e4. \\\\ud30c\\\\uc778 \\\\ud29c\\\\ub2dd\\\\uc740 \\\\uc6d0\\\\ubcf8 \\\\ubaa8\\\\ub378\\\\uc758 \\\\ubaa8\\\\ub4e0 \\\\ud30c\\\\ub77c\\\\ubbf8\\\\ud130 \\\\uac00\\\\uc911\\\\uce58\\\\ub97c \\\\uc218\\\\uc815\\\\ud558\\\\ubbc0\\\\ub85c \\\\ud6a8\\\\uc728\\\\uc801\\\\uc774\\\\uc9c0\\\\ub9cc \\\\uc2a4\\\\ud1a0\\\\ub9ac\\\\uc9c0, \\\\ucef4\\\\ud4e8\\\\ud305 \\\\ubc0f \\\\uba54\\\\ubaa8\\\\ub9ac \\\\uc6a9\\\\ub7c9\\\\uc744 \\\\ub9ce\\\\uc774 \\\\uc0ac\\\\uc6a9\\\\ud569\\\\ub2c8\\\\ub2e4.<|endoftext|>\"}'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "eb00a374",
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_text = parse_response(json_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "873b20ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'아래는 작업을 설명하는 명령어와 추가 컨텍스트를 제공하는 입력이 짝을 이루는 예제입니다. 요청을 적절히 완료하는 응답을 작성하세요.\\n\\n### 명령어:\\n다음 글을 알기 쉽게 요약해 주세요.\\n\\n### 입력:\\n\\n대규모 언어 모델(LLM; Large Language Models) 분야의 발전과 LLM이 가치 있는 인사이트를 제공하는 문제 세트 수가 계속 증가하고 있다는 소식을 들어보셨을 겁니다. \\n대규모 데이터 세트와 여러 작업을 통해 훈련된 대규모 모델은 훈련되지 않은 특정 작업에도 잘 일반화됩니다. 이러한 모델을 파운데이션 모델(foundation model)이라고 하며, 스탠포드 HAI 연구소(Stanford Institute for Human-Centered Artificial Intelligence)에서 처음 대중화한 용어입니다. \\n이러한 파운데이션 모델은 프롬프트 엔지니어링(prompt engineering) 기법의 도움으로 잘 활용할 수 있지만, 유스케이스가 도메인에 매우 특화되어 있거나 작업의 종류가 매우 다양하여 모델을 추가로 커스터마이징해야 하는 경우가 많습니다. \\n특정 도메인이나 작업에 대한 대규모 모델의 성능을 개선하기 위한 한 가지 접근 방식은 더 작은 작업별 데이터 세트로 모델을 추가로 훈련하는 것입니다. 파인 튜닝(fine-tuning)으로 알려진 이 접근 방식은 LLM의 정확도를 성공적으로 개선하지만, 모든 모델 가중치를 수정해야 합니다. \\n파인 튜닝 데이터 세트 크기가 훨씬 작기 때문에 사전 훈련(pre-training)하는 것 보다 훨씬 빠르지만 여전히 상당한 컴퓨팅 성능과 메모리가 필요합니다. \\n파인 튜닝은 원본 모델의 모든 파라미터 가중치를 수정하므로 비용이 많이 들고 원본 모델과 동일한 크기의 모델을 생성하므로 스토리지 용량에도 부담이 됩니다.\\n\\n\\n### 응답:\\n이 글이 말하고자 하는 내용은 대규모 언어 모델에 대한 발전과 그들이 제공하는 인사이트입니다. 이러한 모델은 훈련되지 않은 작업에도 일반화되고, 매우 다양한 문제에 적용될 수 있습니다. 하지만 특정 도메인이나 작업에 대해 프롬프트 엔지니어링을 하려면 많은 노력이 필요하고 모델을 추가로 커스터마이징해야 하는 경우가 많습니다. 더 작은 작업에 대해 LLM 모델을 추가로 훈련하면 정확도를 높일 수 있지만, 많은 컴퓨팅 및 메모리 리소스가 필요합니다. 파인 튜닝은 원본 모델의 모든 파라미터 가중치를 수정하므로 효율적이지만 스토리지, 컴퓨팅 및 메모리 용량을 많이 사용합니다.<|endoftext|>'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "90cafbd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "이 글이 말하고자 하는 내용은 대규모 언어 모델에 대한 발전과 그들이 제공하는 인사이트입니다. 이러한 모델은 훈련되지 않은 작업에도 일반화되고, 매우 다양한 문제에 적용될 수 있습니다. 하지만 특정 도메인이나 작업에 대해 프롬프트 엔지니어링을 하려면 많은 노력이 필요하고 모델을 추가로 커스터마이징해야 하는 경우가 많습니다. 더 작은 작업에 대해 LLM 모델을 추가로 훈련하면 정확도를 높일 수 있지만, 많은 컴퓨팅 및 메모리 리소스가 필요합니다. 파인 튜닝은 원본 모델의 모든 파라미터 가중치를 수정하므로 효율적이지만 스토리지, 컴퓨팅 및 메모리 용량을 많이 사용합니다.<|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "parts = generated_text.split('### 응답:')\n",
    "\n",
    "# parts의 길이가 2 이상이면, '### 응답:' 이후의 부분을 추출합니다.\n",
    "if len(parts) >= 2:\n",
    "    response_text = parts[1]\n",
    "else:\n",
    "    response_text = \"\"  # '### 응답:'이 없는 경우 빈 문자열을 반환합니다.\n",
    "\n",
    "print(response_text.strip())  # 앞뒤 공백을 제거하고 출력합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5b101d2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
