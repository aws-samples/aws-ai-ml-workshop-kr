{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7e1e3ce5-d856-4cf8-93b7-50e0acce9f15",
   "metadata": {},
   "source": [
    "# Preprocess Datasets for Korean LLM (Large Language Model) fine-tuning\n",
    "---\n",
    "\n",
    "- Alpaca 논문에서 전처리했던 방식대로 전처리 수행\n",
    "- 허깅페이스 인증 정보 설정: `huggingface-cli login`\n",
    "    - https://huggingface.co/join\n",
    "    - https://huggingface.co/settings/tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "283f4cee-20d4-4b94-930b-da48f05d03db",
   "metadata": {
    "pycharm": {
     "is_executing": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "[ERROR] 0번 모듈 노트북(0_setup.ipynb)을 먼저 실행해 주세요.\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import sys\n",
    "sys.path.append('../utils')\n",
    "sys.path.append('../templates')\n",
    "\n",
    "from common_lib import check_packages\n",
    "check_packages()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0088d0bd-fd82-4dc4-a9e4-572b96df59d5",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## 1. Download LLM from Hugging Face hub\n",
    "---\n",
    "\n",
    "### Load dataset\n",
    "허깅페이스 허브에서 다운로드하거나 json/json 포맷의 데이터 세트를 다운로드합니다. 데이터 세트 내 샘플은 (`instruction, input, output`)의 \n",
    "key-value나 (`instruction, output`)의 key-value로 구성되어야 합니다.\n",
    "\n",
    "구름 데이터셋 v2는 GPT-4-LLM, Vicuna, 그리고 Databricks의 Dolly 데이터셋을 병합한 것입니다. 이 모든 데이터셋은 DeepL을 이용하여 한국어로 번역되었습니다.\n",
    "https://huggingface.co/datasets/nlpai-lab/kullm-v2\n",
    "\n",
    "예시:\n",
    "```\n",
    "{\n",
    "    \"instruction\":\"건강을 유지하기 위한 세 가지 팁을 알려주세요.\",\n",
    "    \"input\":\"\",\n",
    "    \"output\":\"세 가지 팁은 아침식사를 꼭 챙기며, 충분한 수면을 취하고, 적극적으로 운동을 하는 것입니다.\"\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9f226ac4-c157-4ee3-abd7-b9519320e229",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8894be863da64d1a96019bb16ed5346f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/1.23k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2219294f298431e8be11230d6862090",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9de7ec4d597c4c23b7464f541979430c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/264M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6281d5c6e1d74c2593cbe97964251e1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f95d8688e3b4f20833491acda9631bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import transformers\n",
    "from datasets import load_dataset\n",
    "from local_utils.inference_lib import Prompter\n",
    "from transformers import GPTNeoXForCausalLM, GPTNeoXTokenizerFast\n",
    "\n",
    "data_path = \"nlpai-lab/kullm-v2\"\n",
    "#data_path = \"beomi/KoAlpaca-v1.1a\"\n",
    "#data_path = \"./data/ko_alpaca_data.json\"\n",
    "\n",
    "if data_path.endswith(\".json\") or data_path.endswith(\".jsonl\"):\n",
    "    data = load_dataset(\"json\", data_files=data_path)\n",
    "else:\n",
    "    data = load_dataset(data_path)\n",
    "    \n",
    "prompter = Prompter(\"kullm\")\n",
    "cutoff_len = 2048\n",
    "train_on_inputs = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7261f675",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'nlpai-lab/kullm-v2'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "94014370-6f4b-4243-8c98-534cfc5eca36",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40185da4e13d4f94b400f28b445254af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/210 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ecb51924159b4741930de2b4b8ec86bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.65M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f798e829a983476db981b6eb607c78df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/185 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'PreTrainedTokenizerFast'. \n",
      "The class this function is called from is 'GPTNeoXTokenizerFast'.\n",
      "/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/huggingface_hub/file_download.py:1194: UserWarning: `local_dir_use_symlinks` parameter is deprecated and will be ignored. The process to download files to a local folder has been updated and do not rely on symlinks anymore. You only need to pass a destination folder as`local_dir`.\n",
      "For more details, check out https://huggingface.co/docs/huggingface_hub/main/en/guides/download#download-files-to-local-folder.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7396c544c8e84a3f92f5c22536a25445",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 9 files:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e519bc6db3b7497fa7cf6961714aa6d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/714 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d74be0482e640bf90c3814442cd2b57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.65M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1503a9ab38a14387a69c0c208e625c42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/185 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db964e6d3d434b548705445ec4ea915f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin.index.json:   0%|          | 0.00/52.5k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3611dd58ffc41329b2b101452da10ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/210 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a0088c22a184954b67f220ce32b9623",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/111 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1ac8d14dc1649da9de9ba67a14f887e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model-00001-of-00003.bin:   0%|          | 0.00/10.0G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "664daa3ead994d29b872ae422e5f89bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model-00003-of-00003.bin:   0%|          | 0.00/6.01G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "882b1967522a4e3c815f1c7938dd46ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model-00002-of-00003.bin:   0%|          | 0.00/9.93G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "HF_MODEL_ID = \"nlpai-lab/kullm-polyglot-12.8b-v2\"\n",
    "\n",
    "tokenizer = GPTNeoXTokenizerFast.from_pretrained(HF_MODEL_ID)\n",
    "\n",
    "# Only download pytorch checkpoint files\n",
    "allow_patterns = [\"*.json\", \"*.pt\", \"*.bin\", \"*.txt\", \"*.model\"]\n",
    "\n",
    "# create model dir\n",
    "model_name = HF_MODEL_ID.split(\"/\")[-1].replace('.', '-')\n",
    "model_tar_dir = Path(f\"/home/ec2-user/SageMaker/models/{model_name}\")\n",
    "if not os.path.isdir(model_tar_dir):\n",
    "    os.makedirs(model_tar_dir, exist_ok=True)\n",
    "    # Download model from Hugging Face into model_dir\n",
    "    snapshot_download(\n",
    "        HF_MODEL_ID, \n",
    "        local_dir=str(model_tar_dir), \n",
    "        local_dir_use_symlinks=False,\n",
    "        allow_patterns=allow_patterns,\n",
    "        cache_dir=\"/home/ec2-user/SageMaker/\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f1dad55",
   "metadata": {},
   "source": [
    "# 입력 데이터셋 살펴보기 EDA\n",
    "\n",
    "데이터 구조\n",
    "- id: 각 데이터 포인트의 고유 식별자\n",
    "- instruction: 사용자의 질문 또는 명령\n",
    "- input: 추가 입력 (여기서는 비어 있음)\n",
    "- output: 질문 또는 명령에 대한 답변"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e2e73d5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': ['alpaca_{idx}', 'alpaca_{idx}', 'alpaca_{idx}'], 'instruction': ['3원색이란 무엇인가요?', '어려운 결정을 내려야 했던 순간을 설명해 주세요.', '원자의 구조를 설명합니다.'], 'input': ['', '', ''], 'output': ['세 가지 기본 색은 빨강, 파랑, 노랑입니다. 이 색은 다른 색을 혼합하여 만들 수 없고 다른 모든 색은 다양한 비율로 조합하여 만들 수 있기 때문에 원색이라고 부릅니다. 빛에 사용되는 첨가제 색상 시스템에서 원색은 빨강, 녹색, 파랑(RGB)입니다.', 'AI 어시스턴트인 저는 저만의 개인적인 경험이나 물리적 존재를 가지고 있지 않습니다. 하지만 알고리즘, 논리, 데이터를 기반으로 의사 결정을 내리도록 프로그래밍되어 있습니다. 상충되는 정보나 여러 가지 가능한 결과가 있을 수 있기 때문에 의사결정이 어려운 상황에 직면할 때가 있습니다. 그런 경우 저는 각 옵션의 위험과 이점을 분석하고 비교하여 원하는 결과를 얻을 가능성이 가장 높은 결론에 도달할 수 있는 능력을 활용합니다.', '원자는 모든 물질의 기본 구성 요소이며 양성자, 중성자, 전자의 세 가지 유형의 입자로 이루어져 있습니다. 원자의 구조는 전자 구름으로 둘러싸인 중앙에 핵이 있는 것으로 설명할 수 있습니다.\\n\\n원자의 핵은 양성자와 중성자로 구성됩니다. 양성자는 양전하를 띠는 입자이고 중성자는 전하를 띠지 않는 중성 입자입니다. 이 두 입자는 원자의 중심에 있으며 원자 질량의 대부분을 차지하는 원자핵에 위치합니다.\\n\\n원자핵을 둘러싸고 있는 것은 전자 구름입니다. 전자는 음전하를 띠는 입자로, 원자핵 주변에서 끊임없이 움직입니다. 전자 구름은 껍질 또는 궤도로 나뉘며, 각 껍질은 특정 수의 전자를 보유할 수 있습니다. 원자가 껍질이라고 하는 가장 바깥쪽 껍질에 있는 전자의 수에 따라 원자의 화학적 특성이 결정됩니다.\\n\\n중성 원자에서 핵의 양성자 수는 전자 구름의 전자 수와 같으므로 양전하와 음전하가 균형을 이루며 원자는 전체 전하를 갖지 않습니다. 원자 번호라고도 하는 양성자의 수는 원자가 어떤 원소인지 결정합니다.']}\n"
     ]
    }
   ],
   "source": [
    "# 데이터셋의 처음 3개 샘플을 출력\n",
    "print(data['train'][:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39425f35-ece7-4888-9366-f54283075fc8",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## 2. Tokenize\n",
    "---\n",
    "목적: 텍스트를 더 작은 부분, 즉 \"토큰(token)\"으로 분리하는 과정입니다.토큰화의 주요 목적은 텍스트 데이터를 단순화하는 것입니다. 긴 텍스트를 더 작은 단위로 나누어 알고리즘이 언어를 처리하고 이해하기 쉽게 만듭니다. 토큰은 언어를 이해하는 기반을 마련합니다. 이 토큰들을 분석함으로써, 알고리즘은 문장의 의미를 해석하고 단어가 사용된 맥락을 이해할 수 있습니다.\n",
    "단위: 보통 단어, 문장, 문단 등을 토큰으로 분리합니다. 하지만 언어와 문맥에 따라 다를 수 있습니다.\n",
    "\n",
    "### NLP에서 Tokenization\n",
    "\n",
    "#### Tokenization (토큰화)\n",
    "\n",
    "**정의**: 토큰화는 큰 문단을 문장, 단어 또는 다른 단위로 분해하는 과정입니다. 이 과정은 텍스트의 구조를 이해하는 데 필수적이며, 자연어 처리(NLP)에서 종종 첫 번째 단계 중 하나입니다.\n",
    "\n",
    "**사용 사례**: \n",
    "- 텍스트 분류\n",
    "- 감정 분석\n",
    "- 기계 번역\n",
    "\n",
    "**예시**: \n",
    "```text\n",
    "입력: \"안녕하세요, 세상! 어떻게 지내세요?\"\n",
    "토큰: [\"안녕하세요\", \",\", \"세상\", \"!\", \"어떻게\", \"지내세요\", \"?\"]\n",
    "```\n",
    "\n",
    "**장점**:\n",
    "- 단순하고 빠름\n",
    "- 추가적인 텍스트 분석의 기초가 됨. 토큰화는 필터링, 정규화, 어간 추출(stemming) 및 표제어 추출(lemmatization) 같은 후속 데이터 전처리 단계의 기초를 제공합니다.\n",
    "- 토큰화를 통해 언어의 구문론적(syntactic) 및 의미론적(semantic) 특성을 파악할 수 있습니다. 이는 품사 태깅, 개체명 인식 등의 고급 NLP 작업에 필수적입니다.\n",
    "- 토큰화는 언어 모델이 텍스트를 효과적으로 이해하고 생성할 수 있게 도와줍니다. 이는 기계 번역, 감성 분석, 문서 요약 등의 작업에서 중요합니다.\n",
    "\n",
    "**단점**:\n",
    "- 단어 사이의 문맥과 의미 관계를 잃음\n",
    "- 새롭게 생겨나는 신조어, 속어, 인터넷 용어 등은 기존의 토큰화 도구에서 제대로 인식되지 않을 수 있습니다.\n",
    "- 단어들이 독립적으로 처리될 때 그 문맥상의 의미나 뉘앙스가 무시되기 쉽습니다.\n",
    "- 관용 표현을 잘 처리하지 못할 수 있음\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d7c3c17",
   "metadata": {},
   "source": [
    "### tokenize 함수\n",
    "- 주어진 텍스트(prompt)를 토큰화하는 역할을 합니다.\n",
    "- 토큰화된 결과는 cutoff_len으로 정의된 최대 길이로 제한합니다.\n",
    "\n",
    "### generate_and_tokenize_prompt 함수 \n",
    "- 데이터를 바탕으로 프롬프트를 생성하고 토큰화하는 역할을 합니다.\n",
    "- 학습 데이터를 모델에 적합한 형태로 변환하는 데 사용합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7a492cfc-8402-4fb1-8fe3-3b27c64c5c35",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def tokenize(prompt, add_eos_token=True):\n",
    "    # there's probably a way to do this with the tokenizer settings\n",
    "    # but again, gotta move fast\n",
    "    result = tokenizer(\n",
    "        prompt,\n",
    "        truncation=True,\n",
    "        max_length=cutoff_len,\n",
    "        padding=False,\n",
    "        return_tensors=None,\n",
    "    )\n",
    "    if (\n",
    "        result[\"input_ids\"][-1] != tokenizer.eos_token_id\n",
    "        and len(result[\"input_ids\"]) < cutoff_len\n",
    "        and add_eos_token\n",
    "    ):\n",
    "        result[\"input_ids\"].append(tokenizer.eos_token_id)\n",
    "        result[\"attention_mask\"].append(1)\n",
    "\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "\n",
    "    return result\n",
    "\n",
    "def generate_and_tokenize_prompt(data_point):\n",
    "    full_prompt = prompter.generate_prompt(\n",
    "        data_point[\"instruction\"],\n",
    "        data_point.get(\"input\"),\n",
    "        data_point[\"output\"],\n",
    "    )\n",
    "    tokenized_full_prompt = tokenize(full_prompt)\n",
    "    if not train_on_inputs:\n",
    "        user_prompt = prompter.generate_prompt(data_point[\"instruction\"], data_point.get(\"input\"))\n",
    "        tokenized_user_prompt = tokenize(user_prompt, add_eos_token=add_eos_token)\n",
    "        user_prompt_len = len(tokenized_user_prompt[\"input_ids\"])\n",
    "\n",
    "        if add_eos_token:\n",
    "            user_prompt_len -= 1\n",
    "\n",
    "        tokenized_full_prompt[\"labels\"] = [-100] * user_prompt_len + tokenized_full_prompt[\"labels\"][\n",
    "            user_prompt_len:\n",
    "        ]  # could be sped up, probably\n",
    "    return tokenized_full_prompt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b870f0e1-eab7-45db-bd59-3bd5c9c19394",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3b808311ff149e494294b7e1c002996",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/152630 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of samples: 152630\n"
     ]
    }
   ],
   "source": [
    "dataset = data['train'].shuffle()#.select(range(100))\n",
    "lm_dataset = dataset.map(generate_and_tokenize_prompt)\n",
    "\n",
    "# Print total number of samples\n",
    "print(f\"Total number of samples: {len(lm_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d7458604",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>instruction</th>\n",
       "      <th>input</th>\n",
       "      <th>output</th>\n",
       "      <th>input_ids</th>\n",
       "      <th>attention_mask</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>vicuna_{idx}</td>\n",
       "      <td>이 기본 예시를 더 정확한 모델로 바꾸려면 어떻게 해야 할까요?</td>\n",
       "      <td></td>\n",
       "      <td>앞서 제공한 기본 몬테카를로 시뮬레이션 예제를 보다 정확한 모델로 전환하는 몇 가지...</td>\n",
       "      <td>[12657, 272, 2236, 276, 1251, 284, 272, 5026, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[12657, 272, 2236, 276, 1251, 284, 272, 5026, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>dolly_{idx}</td>\n",
       "      <td>지하철이란 무엇인가요?</td>\n",
       "      <td>지하 철도는 19세기 초에서 중반에 미국에 설립된 비밀 경로와 은신처 네트워크였습니...</td>\n",
       "      <td>지하 철도는 19세기 미국에서 노예들을 도와 자유를 찾게 해주는 비밀 경로와 은신처...</td>\n",
       "      <td>[12657, 272, 2236, 276, 1251, 284, 272, 5026, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[12657, 272, 2236, 276, 1251, 284, 272, 5026, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>vicuna_{idx}</td>\n",
       "      <td>eH는 갓 티어 광고의 창립자입니다.그의 유튜브 채널은 https://www.you...</td>\n",
       "      <td></td>\n",
       "      <td>에드 리크에 대한 자세한 정보를 제공해 주셔서 감사합니다. 그의 유튜브 채널을 기반...</td>\n",
       "      <td>[12657, 272, 2236, 276, 1251, 284, 272, 5026, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[12657, 272, 2236, 276, 1251, 284, 272, 5026, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>vicuna_{idx}</td>\n",
       "      <td>마지막 장면을 작성합니다. 이 장면에서 마티 맥플라이와 닥 브라운은 집에 돌아와 매...</td>\n",
       "      <td></td>\n",
       "      <td>INT. 맥플라이 하우스 - 밤마티와 닥은 소파에 앉아 맥주를 마시며 그들의 놀라운...</td>\n",
       "      <td>[12657, 272, 2236, 276, 1251, 284, 272, 5026, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[12657, 272, 2236, 276, 1251, 284, 272, 5026, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>vicuna_{idx}</td>\n",
       "      <td>이 날 결혼식에 대한 긴 시를 쓰고 영원, 영원히, 폭포, 일출, 일몰, 은혜, 연...</td>\n",
       "      <td></td>\n",
       "      <td>칠칠절인 7일 아즈푸에서 결혼식이 열릴 예정이었어요,사랑과 은총으로 맺어진 두 사람...</td>\n",
       "      <td>[12657, 272, 2236, 276, 1251, 284, 272, 5026, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[12657, 272, 2236, 276, 1251, 284, 272, 5026, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152625</th>\n",
       "      <td>vicuna_{idx}</td>\n",
       "      <td>설명과 함께 nodejs를 사용한 redisSearch</td>\n",
       "      <td></td>\n",
       "      <td>edisSearch는 널리 사용되는 인메모리 키-값 저장소인 Redis를 기반으로 ...</td>\n",
       "      <td>[12657, 272, 2236, 276, 1251, 284, 272, 5026, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[12657, 272, 2236, 276, 1251, 284, 272, 5026, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152626</th>\n",
       "      <td>alpaca_{idx}</td>\n",
       "      <td>이 텍스트를 두 개의 하위 섹션으로 나눕니다.</td>\n",
       "      <td>우리는 의료에서 금융에 이르기까지 의사 결정에 데이터가 점점 더 많이 활용되는 세상...</td>\n",
       "      <td>#### 하위 섹션 1: 의사 결정에 데이터 사용\\n우리는 의료에서 금융에 이르기까...</td>\n",
       "      <td>[12657, 272, 2236, 276, 1251, 284, 272, 5026, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[12657, 272, 2236, 276, 1251, 284, 272, 5026, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152627</th>\n",
       "      <td>vicuna_{idx}</td>\n",
       "      <td>캐나다의 기후 변화로 인한 식량 불안을 다루는 정책 개요를 작성하여 네 가지 정책 ...</td>\n",
       "      <td></td>\n",
       "      <td>정책 요약: 캐나다의 기후 변화로 인한 식량 불안정 문제 해결소개:기후 변화는 캐나...</td>\n",
       "      <td>[12657, 272, 2236, 276, 1251, 284, 272, 5026, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[12657, 272, 2236, 276, 1251, 284, 272, 5026, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152628</th>\n",
       "      <td>vicuna_{idx}</td>\n",
       "      <td>push\\_update\\_services를 여러 번 호출할 수 있습니다.</td>\n",
       "      <td></td>\n",
       "      <td>f`push_update_services()`를 여러 번 호출해야 하고 각 호출이 ...</td>\n",
       "      <td>[12657, 272, 2236, 276, 1251, 284, 272, 5026, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[12657, 272, 2236, 276, 1251, 284, 272, 5026, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152629</th>\n",
       "      <td>alpaca_{idx}</td>\n",
       "      <td>막대형 차트에 대한 두 개의 데이터 포인트 생성</td>\n",
       "      <td></td>\n",
       "      <td>다음은 막대형 차트에 대한 두 개의 데이터 포인트입니다:\\n\\n1.  **2020년...</td>\n",
       "      <td>[12657, 272, 2236, 276, 1251, 284, 272, 5026, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[12657, 272, 2236, 276, 1251, 284, 272, 5026, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>152630 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  id                                        instruction  \\\n",
       "0       vicuna_{idx}                이 기본 예시를 더 정확한 모델로 바꾸려면 어떻게 해야 할까요?   \n",
       "1        dolly_{idx}                                       지하철이란 무엇인가요?   \n",
       "2       vicuna_{idx}  eH는 갓 티어 광고의 창립자입니다.그의 유튜브 채널은 https://www.you...   \n",
       "3       vicuna_{idx}  마지막 장면을 작성합니다. 이 장면에서 마티 맥플라이와 닥 브라운은 집에 돌아와 매...   \n",
       "4       vicuna_{idx}  이 날 결혼식에 대한 긴 시를 쓰고 영원, 영원히, 폭포, 일출, 일몰, 은혜, 연...   \n",
       "...              ...                                                ...   \n",
       "152625  vicuna_{idx}                     설명과 함께 nodejs를 사용한 redisSearch   \n",
       "152626  alpaca_{idx}                          이 텍스트를 두 개의 하위 섹션으로 나눕니다.   \n",
       "152627  vicuna_{idx}  캐나다의 기후 변화로 인한 식량 불안을 다루는 정책 개요를 작성하여 네 가지 정책 ...   \n",
       "152628  vicuna_{idx}           push\\_update\\_services를 여러 번 호출할 수 있습니다.   \n",
       "152629  alpaca_{idx}                         막대형 차트에 대한 두 개의 데이터 포인트 생성   \n",
       "\n",
       "                                                    input  \\\n",
       "0                                                           \n",
       "1       지하 철도는 19세기 초에서 중반에 미국에 설립된 비밀 경로와 은신처 네트워크였습니...   \n",
       "2                                                           \n",
       "3                                                           \n",
       "4                                                           \n",
       "...                                                   ...   \n",
       "152625                                                      \n",
       "152626  우리는 의료에서 금융에 이르기까지 의사 결정에 데이터가 점점 더 많이 활용되는 세상...   \n",
       "152627                                                      \n",
       "152628                                                      \n",
       "152629                                                      \n",
       "\n",
       "                                                   output  \\\n",
       "0       앞서 제공한 기본 몬테카를로 시뮬레이션 예제를 보다 정확한 모델로 전환하는 몇 가지...   \n",
       "1       지하 철도는 19세기 미국에서 노예들을 도와 자유를 찾게 해주는 비밀 경로와 은신처...   \n",
       "2       에드 리크에 대한 자세한 정보를 제공해 주셔서 감사합니다. 그의 유튜브 채널을 기반...   \n",
       "3       INT. 맥플라이 하우스 - 밤마티와 닥은 소파에 앉아 맥주를 마시며 그들의 놀라운...   \n",
       "4       칠칠절인 7일 아즈푸에서 결혼식이 열릴 예정이었어요,사랑과 은총으로 맺어진 두 사람...   \n",
       "...                                                   ...   \n",
       "152625  edisSearch는 널리 사용되는 인메모리 키-값 저장소인 Redis를 기반으로 ...   \n",
       "152626  #### 하위 섹션 1: 의사 결정에 데이터 사용\\n우리는 의료에서 금융에 이르기까...   \n",
       "152627  정책 요약: 캐나다의 기후 변화로 인한 식량 불안정 문제 해결소개:기후 변화는 캐나...   \n",
       "152628  f`push_update_services()`를 여러 번 호출해야 하고 각 호출이 ...   \n",
       "152629  다음은 막대형 차트에 대한 두 개의 데이터 포인트입니다:\\n\\n1.  **2020년...   \n",
       "\n",
       "                                                input_ids  \\\n",
       "0       [12657, 272, 2236, 276, 1251, 284, 272, 5026, ...   \n",
       "1       [12657, 272, 2236, 276, 1251, 284, 272, 5026, ...   \n",
       "2       [12657, 272, 2236, 276, 1251, 284, 272, 5026, ...   \n",
       "3       [12657, 272, 2236, 276, 1251, 284, 272, 5026, ...   \n",
       "4       [12657, 272, 2236, 276, 1251, 284, 272, 5026, ...   \n",
       "...                                                   ...   \n",
       "152625  [12657, 272, 2236, 276, 1251, 284, 272, 5026, ...   \n",
       "152626  [12657, 272, 2236, 276, 1251, 284, 272, 5026, ...   \n",
       "152627  [12657, 272, 2236, 276, 1251, 284, 272, 5026, ...   \n",
       "152628  [12657, 272, 2236, 276, 1251, 284, 272, 5026, ...   \n",
       "152629  [12657, 272, 2236, 276, 1251, 284, 272, 5026, ...   \n",
       "\n",
       "                                           attention_mask  \\\n",
       "0       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "1       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "2       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "3       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "4       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "...                                                   ...   \n",
       "152625  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "152626  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "152627  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "152628  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "152629  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "\n",
       "                                                   labels  \n",
       "0       [12657, 272, 2236, 276, 1251, 284, 272, 5026, ...  \n",
       "1       [12657, 272, 2236, 276, 1251, 284, 272, 5026, ...  \n",
       "2       [12657, 272, 2236, 276, 1251, 284, 272, 5026, ...  \n",
       "3       [12657, 272, 2236, 276, 1251, 284, 272, 5026, ...  \n",
       "4       [12657, 272, 2236, 276, 1251, 284, 272, 5026, ...  \n",
       "...                                                   ...  \n",
       "152625  [12657, 272, 2236, 276, 1251, 284, 272, 5026, ...  \n",
       "152626  [12657, 272, 2236, 276, 1251, 284, 272, 5026, ...  \n",
       "152627  [12657, 272, 2236, 276, 1251, 284, 272, 5026, ...  \n",
       "152628  [12657, 272, 2236, 276, 1251, 284, 272, 5026, ...  \n",
       "152629  [12657, 272, 2236, 276, 1251, 284, 272, 5026, ...  \n",
       "\n",
       "[152630 rows x 7 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_lm = lm_dataset.to_pandas()\n",
    "df_lm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "98c86083",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>instruction</th>\n",
       "      <th>input</th>\n",
       "      <th>output</th>\n",
       "      <th>input_ids</th>\n",
       "      <th>attention_mask</th>\n",
       "      <th>labels</th>\n",
       "      <th>input_ids_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>vicuna_{idx}</td>\n",
       "      <td>이 기본 예시를 더 정확한 모델로 바꾸려면 어떻게 해야 할까요?</td>\n",
       "      <td></td>\n",
       "      <td>앞서 제공한 기본 몬테카를로 시뮬레이션 예제를 보다 정확한 모델로 전환하는 몇 가지...</td>\n",
       "      <td>[12657, 272, 2236, 276, 1251, 284, 272, 5026, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[12657, 272, 2236, 276, 1251, 284, 272, 5026, ...</td>\n",
       "      <td>567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>dolly_{idx}</td>\n",
       "      <td>지하철이란 무엇인가요?</td>\n",
       "      <td>지하 철도는 19세기 초에서 중반에 미국에 설립된 비밀 경로와 은신처 네트워크였습니...</td>\n",
       "      <td>지하 철도는 19세기 미국에서 노예들을 도와 자유를 찾게 해주는 비밀 경로와 은신처...</td>\n",
       "      <td>[12657, 272, 2236, 276, 1251, 284, 272, 5026, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[12657, 272, 2236, 276, 1251, 284, 272, 5026, ...</td>\n",
       "      <td>482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>vicuna_{idx}</td>\n",
       "      <td>eH는 갓 티어 광고의 창립자입니다.그의 유튜브 채널은 https://www.you...</td>\n",
       "      <td></td>\n",
       "      <td>에드 리크에 대한 자세한 정보를 제공해 주셔서 감사합니다. 그의 유튜브 채널을 기반...</td>\n",
       "      <td>[12657, 272, 2236, 276, 1251, 284, 272, 5026, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[12657, 272, 2236, 276, 1251, 284, 272, 5026, ...</td>\n",
       "      <td>1011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>vicuna_{idx}</td>\n",
       "      <td>마지막 장면을 작성합니다. 이 장면에서 마티 맥플라이와 닥 브라운은 집에 돌아와 매...</td>\n",
       "      <td></td>\n",
       "      <td>INT. 맥플라이 하우스 - 밤마티와 닥은 소파에 앉아 맥주를 마시며 그들의 놀라운...</td>\n",
       "      <td>[12657, 272, 2236, 276, 1251, 284, 272, 5026, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[12657, 272, 2236, 276, 1251, 284, 272, 5026, ...</td>\n",
       "      <td>580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>vicuna_{idx}</td>\n",
       "      <td>이 날 결혼식에 대한 긴 시를 쓰고 영원, 영원히, 폭포, 일출, 일몰, 은혜, 연...</td>\n",
       "      <td></td>\n",
       "      <td>칠칠절인 7일 아즈푸에서 결혼식이 열릴 예정이었어요,사랑과 은총으로 맺어진 두 사람...</td>\n",
       "      <td>[12657, 272, 2236, 276, 1251, 284, 272, 5026, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[12657, 272, 2236, 276, 1251, 284, 272, 5026, ...</td>\n",
       "      <td>489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152625</th>\n",
       "      <td>vicuna_{idx}</td>\n",
       "      <td>설명과 함께 nodejs를 사용한 redisSearch</td>\n",
       "      <td></td>\n",
       "      <td>edisSearch는 널리 사용되는 인메모리 키-값 저장소인 Redis를 기반으로 ...</td>\n",
       "      <td>[12657, 272, 2236, 276, 1251, 284, 272, 5026, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[12657, 272, 2236, 276, 1251, 284, 272, 5026, ...</td>\n",
       "      <td>1399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152626</th>\n",
       "      <td>alpaca_{idx}</td>\n",
       "      <td>이 텍스트를 두 개의 하위 섹션으로 나눕니다.</td>\n",
       "      <td>우리는 의료에서 금융에 이르기까지 의사 결정에 데이터가 점점 더 많이 활용되는 세상...</td>\n",
       "      <td>#### 하위 섹션 1: 의사 결정에 데이터 사용\\n우리는 의료에서 금융에 이르기까...</td>\n",
       "      <td>[12657, 272, 2236, 276, 1251, 284, 272, 5026, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[12657, 272, 2236, 276, 1251, 284, 272, 5026, ...</td>\n",
       "      <td>242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152627</th>\n",
       "      <td>vicuna_{idx}</td>\n",
       "      <td>캐나다의 기후 변화로 인한 식량 불안을 다루는 정책 개요를 작성하여 네 가지 정책 ...</td>\n",
       "      <td></td>\n",
       "      <td>정책 요약: 캐나다의 기후 변화로 인한 식량 불안정 문제 해결소개:기후 변화는 캐나...</td>\n",
       "      <td>[12657, 272, 2236, 276, 1251, 284, 272, 5026, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[12657, 272, 2236, 276, 1251, 284, 272, 5026, ...</td>\n",
       "      <td>901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152628</th>\n",
       "      <td>vicuna_{idx}</td>\n",
       "      <td>push\\_update\\_services를 여러 번 호출할 수 있습니다.</td>\n",
       "      <td></td>\n",
       "      <td>f`push_update_services()`를 여러 번 호출해야 하고 각 호출이 ...</td>\n",
       "      <td>[12657, 272, 2236, 276, 1251, 284, 272, 5026, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[12657, 272, 2236, 276, 1251, 284, 272, 5026, ...</td>\n",
       "      <td>451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152629</th>\n",
       "      <td>alpaca_{idx}</td>\n",
       "      <td>막대형 차트에 대한 두 개의 데이터 포인트 생성</td>\n",
       "      <td></td>\n",
       "      <td>다음은 막대형 차트에 대한 두 개의 데이터 포인트입니다:\\n\\n1.  **2020년...</td>\n",
       "      <td>[12657, 272, 2236, 276, 1251, 284, 272, 5026, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[12657, 272, 2236, 276, 1251, 284, 272, 5026, ...</td>\n",
       "      <td>248</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>152630 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  id                                        instruction  \\\n",
       "0       vicuna_{idx}                이 기본 예시를 더 정확한 모델로 바꾸려면 어떻게 해야 할까요?   \n",
       "1        dolly_{idx}                                       지하철이란 무엇인가요?   \n",
       "2       vicuna_{idx}  eH는 갓 티어 광고의 창립자입니다.그의 유튜브 채널은 https://www.you...   \n",
       "3       vicuna_{idx}  마지막 장면을 작성합니다. 이 장면에서 마티 맥플라이와 닥 브라운은 집에 돌아와 매...   \n",
       "4       vicuna_{idx}  이 날 결혼식에 대한 긴 시를 쓰고 영원, 영원히, 폭포, 일출, 일몰, 은혜, 연...   \n",
       "...              ...                                                ...   \n",
       "152625  vicuna_{idx}                     설명과 함께 nodejs를 사용한 redisSearch   \n",
       "152626  alpaca_{idx}                          이 텍스트를 두 개의 하위 섹션으로 나눕니다.   \n",
       "152627  vicuna_{idx}  캐나다의 기후 변화로 인한 식량 불안을 다루는 정책 개요를 작성하여 네 가지 정책 ...   \n",
       "152628  vicuna_{idx}           push\\_update\\_services를 여러 번 호출할 수 있습니다.   \n",
       "152629  alpaca_{idx}                         막대형 차트에 대한 두 개의 데이터 포인트 생성   \n",
       "\n",
       "                                                    input  \\\n",
       "0                                                           \n",
       "1       지하 철도는 19세기 초에서 중반에 미국에 설립된 비밀 경로와 은신처 네트워크였습니...   \n",
       "2                                                           \n",
       "3                                                           \n",
       "4                                                           \n",
       "...                                                   ...   \n",
       "152625                                                      \n",
       "152626  우리는 의료에서 금융에 이르기까지 의사 결정에 데이터가 점점 더 많이 활용되는 세상...   \n",
       "152627                                                      \n",
       "152628                                                      \n",
       "152629                                                      \n",
       "\n",
       "                                                   output  \\\n",
       "0       앞서 제공한 기본 몬테카를로 시뮬레이션 예제를 보다 정확한 모델로 전환하는 몇 가지...   \n",
       "1       지하 철도는 19세기 미국에서 노예들을 도와 자유를 찾게 해주는 비밀 경로와 은신처...   \n",
       "2       에드 리크에 대한 자세한 정보를 제공해 주셔서 감사합니다. 그의 유튜브 채널을 기반...   \n",
       "3       INT. 맥플라이 하우스 - 밤마티와 닥은 소파에 앉아 맥주를 마시며 그들의 놀라운...   \n",
       "4       칠칠절인 7일 아즈푸에서 결혼식이 열릴 예정이었어요,사랑과 은총으로 맺어진 두 사람...   \n",
       "...                                                   ...   \n",
       "152625  edisSearch는 널리 사용되는 인메모리 키-값 저장소인 Redis를 기반으로 ...   \n",
       "152626  #### 하위 섹션 1: 의사 결정에 데이터 사용\\n우리는 의료에서 금융에 이르기까...   \n",
       "152627  정책 요약: 캐나다의 기후 변화로 인한 식량 불안정 문제 해결소개:기후 변화는 캐나...   \n",
       "152628  f`push_update_services()`를 여러 번 호출해야 하고 각 호출이 ...   \n",
       "152629  다음은 막대형 차트에 대한 두 개의 데이터 포인트입니다:\\n\\n1.  **2020년...   \n",
       "\n",
       "                                                input_ids  \\\n",
       "0       [12657, 272, 2236, 276, 1251, 284, 272, 5026, ...   \n",
       "1       [12657, 272, 2236, 276, 1251, 284, 272, 5026, ...   \n",
       "2       [12657, 272, 2236, 276, 1251, 284, 272, 5026, ...   \n",
       "3       [12657, 272, 2236, 276, 1251, 284, 272, 5026, ...   \n",
       "4       [12657, 272, 2236, 276, 1251, 284, 272, 5026, ...   \n",
       "...                                                   ...   \n",
       "152625  [12657, 272, 2236, 276, 1251, 284, 272, 5026, ...   \n",
       "152626  [12657, 272, 2236, 276, 1251, 284, 272, 5026, ...   \n",
       "152627  [12657, 272, 2236, 276, 1251, 284, 272, 5026, ...   \n",
       "152628  [12657, 272, 2236, 276, 1251, 284, 272, 5026, ...   \n",
       "152629  [12657, 272, 2236, 276, 1251, 284, 272, 5026, ...   \n",
       "\n",
       "                                           attention_mask  \\\n",
       "0       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "1       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "2       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "3       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "4       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "...                                                   ...   \n",
       "152625  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "152626  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "152627  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "152628  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "152629  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "\n",
       "                                                   labels  input_ids_length  \n",
       "0       [12657, 272, 2236, 276, 1251, 284, 272, 5026, ...               567  \n",
       "1       [12657, 272, 2236, 276, 1251, 284, 272, 5026, ...               482  \n",
       "2       [12657, 272, 2236, 276, 1251, 284, 272, 5026, ...              1011  \n",
       "3       [12657, 272, 2236, 276, 1251, 284, 272, 5026, ...               580  \n",
       "4       [12657, 272, 2236, 276, 1251, 284, 272, 5026, ...               489  \n",
       "...                                                   ...               ...  \n",
       "152625  [12657, 272, 2236, 276, 1251, 284, 272, 5026, ...              1399  \n",
       "152626  [12657, 272, 2236, 276, 1251, 284, 272, 5026, ...               242  \n",
       "152627  [12657, 272, 2236, 276, 1251, 284, 272, 5026, ...               901  \n",
       "152628  [12657, 272, 2236, 276, 1251, 284, 272, 5026, ...               451  \n",
       "152629  [12657, 272, 2236, 276, 1251, 284, 272, 5026, ...               248  \n",
       "\n",
       "[152630 rows x 8 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 'input_ids' 칼럼의 각 벡터의 길이를 새로운 칼럼 'input_ids_length'에 저장\n",
    "df_lm['input_ids_length'] = df_lm['input_ids'].apply(len)\n",
    "\n",
    "# 새로운 칼럼을 확인\n",
    "df_lm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "906d77dd",
   "metadata": {},
   "source": [
    "#### 토크나이징 벡터의 최대 길이는 설정한 cutoff_len로 고정됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "589d4518",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The maximum length of 'input_ids' is 2048\n"
     ]
    }
   ],
   "source": [
    "# 'input_ids_length' 칼럼의 최대값을 확인\n",
    "max_value = df_lm['input_ids_length'].max()\n",
    "\n",
    "print(f\"The maximum length of 'input_ids' is {max_value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bdb5742-ded5-4914-8c64-ad13e06dd548",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## 4. Save dataset to S3\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e1328ad7-c912-4da5-a905-2859e2276cfc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/ec2-user/SageMaker/.xdg/config/sagemaker/config.yaml\n",
      "SageMaker role arn: arn:aws:iam::057716757052:role/dt2gsmoon\n",
      "SageMaker bucket: sagemaker-us-east-1-057716757052\n",
      "SageMaker session region: us-east-1\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "sess = sagemaker.Session()\n",
    "region = boto3.Session().region_name\n",
    "# sagemaker session bucket -> used for uploading data, models and logs\n",
    "# sagemaker will automatically create this bucket if it not exists\n",
    "bucket = None\n",
    "if bucket is None and sess is not None:\n",
    "    # set to default bucket if a bucket name is not given\n",
    "    bucket = sess.default_bucket()\n",
    "\n",
    "try:\n",
    "    role = sagemaker.get_execution_role()\n",
    "except ValueError:\n",
    "    iam = boto3.client('iam')\n",
    "    role = iam.get_role(RoleName='sagemaker_execution_role')['Role']['Arn']\n",
    "\n",
    "sess = sagemaker.Session(default_bucket=bucket)\n",
    "\n",
    "print(f\"SageMaker role arn: {role}\")\n",
    "print(f\"SageMaker bucket: {sess.default_bucket()}\")\n",
    "print(f\"SageMaker session region: {sess.boto_region_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1620deb3-6020-4a56-838e-6bf56dd50e9f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S3 data path: \n",
      " s3://sagemaker-us-east-1-057716757052/ko-llms/peft/64903/kullm-polyglot-12-8b-v2/dataset/alpaca-train-all\n",
      "S3 dataset_prefix_50_samples data path: \n",
      " s3://sagemaker-us-east-1-057716757052/ko-llms/peft/64903/kullm-polyglot-12-8b-v2/dataset/alpaca-train-50-samples\n",
      "S3 dataset_prefix_10000_samples data path: \n",
      " s3://sagemaker-us-east-1-057716757052/ko-llms/peft/64903/kullm-polyglot-12-8b-v2/dataset/alpaca-train-1000-samples\n",
      "S3 pretrained model path: \n",
      " s3://sagemaker-us-east-1-057716757052/ko-llms/peft/64903/huggingface-models/kullm-polyglot-12-8b-v2/\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "#  서로 다른 S3 위치에 저장하기 위해 S3 경로의 Prefix 생성\n",
    "random_integer = random.randint(1, 100000)  # 1과 10 사이의 정수 난수 생성\n",
    "\n",
    "bucket_prefix = f'ko-llms/peft/{random_integer}'\n",
    "dataset_prefix = 'alpaca-train'\n",
    "dataset_prefix_50_samples = 'alpaca-train-50-samples'\n",
    "dataset_prefix_10000_samples = 'alpaca-train-1000-samples'\n",
    "dataset_prefix_all = 'alpaca-train-all'\n",
    "\n",
    "s3_data_path = f\"s3://{bucket}/{bucket_prefix}/{model_name}/dataset/{dataset_prefix_all}\"\n",
    "s3_data_path_50_samples = f\"s3://{bucket}/{bucket_prefix}/{model_name}/dataset/{dataset_prefix_50_samples}\"\n",
    "s3_data_path_10000_samples = f\"s3://{bucket}/{bucket_prefix}/{model_name}/dataset/{dataset_prefix_10000_samples}\"\n",
    "\n",
    "s3_pretrained_model_path = f\"s3://{bucket}/{bucket_prefix}/huggingface-models/{model_name}/\"\n",
    "print(f\"S3 data path: \\n {s3_data_path}\")\n",
    "print(f\"S3 dataset_prefix_50_samples data path: \\n {s3_data_path_50_samples}\")\n",
    "print(f\"S3 dataset_prefix_10000_samples data path: \\n {s3_data_path_10000_samples}\")\n",
    "print(f\"S3 pretrained model path: \\n {s3_pretrained_model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a73063c8",
   "metadata": {},
   "source": [
    "### 로컬학습을 위한 데이터셋 저장\n",
    "- dataset_prefix : 디버깅을 위한 처음 50개의 샘플(num_debug_samples)을 선택하여 dataset_prefix에 지정된 경로에 저장합니다. \n",
    "- dataset_prefix_all : 152630개를 모두 학습에 사용합니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "068c198f-c0a5-40f4-85fe-74c286903cb4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05c47e2b67be40149f2b52246a334db2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/50 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/fsspec/registry.py:272: UserWarning: Your installed version of s3fs is very old and known to cause\n",
      "severe performance issues, see also https://github.com/dask/dask/issues/10276\n",
      "\n",
      "To fix, you should specify a lower version bound on s3fs, or\n",
      "update the current installation.\n",
      "\n",
      "  warnings.warn(s3_msg)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "401a815ec15d415cad589c5639c9dcfe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/3 shards):   0%|          | 0/152630 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9dfdb2fbf5a4d6f8b6d7e3295238983",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/50 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aff7c4d31073449b846fbd0b36b4f05c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/10000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "517b89b803ca46a5b2e0497154a9ddf3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/3 shards):   0%|          | 0/152630 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples for debugging: 50\n"
     ]
    }
   ],
   "source": [
    "num_debug_samples = 50\n",
    "num_short_train_samples = 10000\n",
    "\n",
    "# save to local\n",
    "lm_dataset.select(range(num_debug_samples)).save_to_disk(dataset_prefix)\n",
    "# save to s3\n",
    "lm_dataset.save_to_disk(s3_data_path)\n",
    "lm_dataset.select(range(num_debug_samples)).save_to_disk(s3_data_path_50_samples)\n",
    "lm_dataset.select(range(num_short_train_samples)).save_to_disk(s3_data_path_10000_samples)\n",
    "\n",
    "lm_dataset.save_to_disk(dataset_prefix_all)\n",
    "print(f\"Number of samples for debugging: {num_debug_samples}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8dd9b5ce-de81-4218-963f-7c9fa086a5ab",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'bucket_prefix' (str)\n",
      "Stored 'dataset_prefix_50_samples' (str)\n",
      "Stored 's3_data_path' (str)\n",
      "Stored 's3_data_path_50_samples' (str)\n",
      "Stored 's3_data_path_10000_samples' (str)\n",
      "Stored 'dataset_prefix' (str)\n"
     ]
    }
   ],
   "source": [
    "%store bucket_prefix dataset_prefix_50_samples s3_data_path s3_data_path_50_samples s3_data_path_10000_samples\n",
    "%store dataset_prefix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80f5dbbd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba4002a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccbf3c81",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_p310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
