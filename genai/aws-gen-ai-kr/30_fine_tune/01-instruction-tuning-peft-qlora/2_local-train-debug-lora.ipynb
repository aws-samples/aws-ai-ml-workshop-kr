{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "55cfa3d1-2187-4176-8c23-19bdf0925364",
   "metadata": {},
   "source": [
    "# Korean LLM (Large Language Model) fine-tuning on Local environment (Debugging)\n",
    "---\n",
    "\n",
    "- 허깅페이스 인증 정보 설정: `huggingface-cli login`\n",
    "    - https://huggingface.co/join\n",
    "    - https://huggingface.co/settings/tokens\n",
    "    \n",
    "\n",
    "## Overview \n",
    "\n",
    "본격적으로 SageMaker 훈련 인스턴스로 훈련을 수행하기 전에 SageMaker Notebook / SageMaker Studio / SageMaker Studio Lab 에서 샘플 데이터로 디버깅을 수행합니다.\n",
    "물론 온프레미스 환경에서 디버깅을 수행할 수 있다면, 기존 환경과 동일하게 디버깅을 수행하면 됩니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d8e8020b-9964-4198-aed5-0efca878bf5a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import sys\n",
    "import os\n",
    "import torch\n",
    "import transformers\n",
    "from datasets import load_dataset, load_from_disk\n",
    "from transformers import GPTNeoXForCausalLM, GPTNeoXTokenizerFast\n",
    "\n",
    "sys.path.append('./utils')\n",
    "sys.path.append('./templates')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "96b7bd04-d647-45cc-a340-84505df99f67",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%store -r bucket_prefix dataset_prefix s3_data_path dataset_prefix_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e2745978-138f-4b70-8693-64859f4704ea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    dataset_prefix\n",
    "except NameError:\n",
    "    print(\"++++++++++++++++++++++++++++++++++++++++++++++++++++++++\")\n",
    "    print(\"[ERROR] 1번 모듈 노트북을 다시 실행해 주세요.\")\n",
    "    print(\"++++++++++++++++++++++++++++++++++++++++++++++++++++++++\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d44b3088-f0b9-4897-a556-933475a9880a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "lm_dataset = load_from_disk(dataset_prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "70a0cd87",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lm_dataset = load_from_disk(dataset_prefix_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dfe7d512",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'attention_mask', 'labels'],\n",
       "    num_rows: 50\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5497012c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'chunk-train'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_prefix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "624f77db-7a07-4439-98c0-2e19cc72e0cb",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## 1. Load Model\n",
    "---\n",
    "한정된 GPU 메모리 안에 LLM 모델을 로드하는 것은 매우 어렵습니다. 예컨대 20B의 모델을 로드하려면 fp32 기준으로 80GB 이상의 메모리가 필요하고 fp16 기준으로도 40GB 이상의 GPU 메모리가 필요하며, 파인 튜닝을 수행하는 경우는 이보다 더욱 많은 GPU 메모리가 필요합니다. 이런 경우 4비트 양자화와 LoRA를 사용하면 범용적으로 사용하고 있는 16GB 및 24GB GPU 메모리로도 파인 튜닝이 가능합니다. 현 기준으로는 4비트 양자화를 지원하는 QLoRA 기법이 가장 널리 사용되고 있으며 bitsandbytes를 사용하여 QLoRA를 쉽게 적용할 수 있습니다. QLoRA는 양자화된 파라미터의 분포 범위를 정규 분포 내로 억제하여 정밀도의 저하를 방지하는 4비트 NormalFloat 양자화 양자화를 적용하는 정수에 대해서도 양자화를 적용하는 이중 양자화, 그리고 optimizer state 등의 데이터를 CPU 메모리에 저장하는 페이징 기법을 적용하여 GPU 메모리 사용량을 억제합니다. QLoRA에 대한 자세한 내용은 논문 (https://arxiv.org/pdf/2305.14314.pdf) 을 참조하기 바랍니다.\n",
    "\n",
    "### Create a bitsandbytes configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "965584cc-3956-4711-8eb4-4fbdaa41f397",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import BitsAndBytesConfig\n",
    "quant_4bit = True\n",
    "quant_8bit = False\n",
    "\n",
    "if quant_4bit:\n",
    "    nf4_config = BitsAndBytesConfig(\n",
    "       load_in_4bit=True,\n",
    "       bnb_4bit_quant_type=\"nf4\",\n",
    "       bnb_4bit_use_double_quant=True,\n",
    "       bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "else:\n",
    "    nf4_config = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f50e68b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9da734ff-5488-4ead-9004-9b5a049b0070",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "HF_MODEL_ID = \"nlpai-lab/kullm-polyglot-12.8b-v2\"\n",
    "\n",
    "# create model dir\n",
    "model_name = HF_MODEL_ID.split(\"/\")[-1].replace('.', '-')\n",
    "model_tar_dir = Path(f\"/home/ec2-user/SageMaker/models/{model_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7b565d1a-5969-4bd3-8233-cce8ad1e245e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'PreTrainedTokenizerFast'. \n",
      "The class this function is called from is 'GPTNeoXTokenizerFast'.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "363ea0fd653b4a6f8cf238d773f684c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "device_map = \"auto\"\n",
    "\n",
    "tokenizer = GPTNeoXTokenizerFast.from_pretrained(HF_MODEL_ID)\n",
    "\n",
    "model = GPTNeoXForCausalLM.from_pretrained(\n",
    "    model_tar_dir,\n",
    "    load_in_8bit=True if quant_8bit else False,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=device_map,\n",
    "    #cache_dir=cache_dir,\n",
    "    quantization_config=nf4_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3b37ebe-c475-47ee-993f-464c0d7201d0",
   "metadata": {},
   "source": [
    "### Create LoRA config\n",
    "LoRA 설정에 대한 자세한 내용은 아래를 참조해 주세요.\n",
    "- https://huggingface.co/docs/peft/conceptual_guides/lora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d6d0fcf8-67a0-44cc-9b89-c6f26a8dcc2d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from peft import (\n",
    "    LoraConfig,\n",
    "    get_peft_model,\n",
    "    get_peft_model_state_dict,\n",
    "    prepare_model_for_kbit_training,\n",
    "    set_peft_model_state_dict,\n",
    ")\n",
    "\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "lora_r  = 8\n",
    "lora_alpha = 32\n",
    "lora_dropout = 0.05\n",
    "lora_target_modules = [\"query_key_value\", \"xxx\"]\n",
    "    \n",
    "config = LoraConfig(\n",
    "    r=lora_r,\n",
    "    lora_alpha=lora_alpha,\n",
    "    target_modules=lora_target_modules,\n",
    "    lora_dropout=lora_dropout,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "model = get_peft_model(model, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "35312b9f-98d5-410b-b120-4156c5cce185",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 6,553,600 || all params: 12,900,157,440 || trainable%: 0.05080248074863806\n"
     ]
    }
   ],
   "source": [
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f488f765-058c-434b-8ed1-b64a490cf96c",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## 2. Training\n",
    "---\n",
    "### Setting Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "415f898f-760f-4ffa-ba5c-3e93c0dee47b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_data = lm_dataset\n",
    "val_data = None\n",
    "num_epochs = 3\n",
    "batch_size = 2\n",
    "\n",
    "learning_rate = 3e-5\n",
    "gradient_accumulation_steps = 2\n",
    "val_set_size = 0\n",
    "output_dir = 'output'\n",
    "world_size = 1\n",
    "ddp = world_size != 1\n",
    "group_by_length = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "55083ad0-a671-4634-9912-e036b18f29d9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_args = transformers.TrainingArguments(\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "    warmup_steps=100,\n",
    "    num_train_epochs=num_epochs,\n",
    "    learning_rate=learning_rate,\n",
    "    bf16=True,\n",
    "    logging_steps=2,\n",
    "    optim=\"paged_adamw_8bit\",\n",
    "    evaluation_strategy=\"steps\" if val_set_size > 0 else \"no\",\n",
    "    save_strategy=\"steps\",\n",
    "    eval_steps=200 if val_set_size > 0 else None,\n",
    "    save_steps=10,\n",
    "    output_dir=output_dir,\n",
    "    load_best_model_at_end=True if val_set_size > 0 else False,\n",
    "    ddp_find_unused_parameters=False if ddp else None,\n",
    "    report_to=\"none\",\n",
    "    group_by_length=group_by_length,\n",
    ")\n",
    "\n",
    "trainer = transformers.Trainer(\n",
    "    model=model,\n",
    "    train_dataset=train_data,\n",
    "    eval_dataset=val_data,\n",
    "    args=train_args,\n",
    "    data_collator=transformers.DataCollatorForSeq2Seq(\n",
    "        tokenizer, pad_to_multiple_of=8, return_tensors=\"pt\", padding=True\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6208dfc0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): GPTNeoXForCausalLM(\n",
       "      (gpt_neox): GPTNeoXModel(\n",
       "        (embed_in): Embedding(30080, 5120)\n",
       "        (emb_dropout): Dropout(p=0.0, inplace=False)\n",
       "        (layers): ModuleList(\n",
       "          (0-39): 40 x GPTNeoXLayer(\n",
       "            (input_layernorm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
       "            (post_attention_layernorm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
       "            (post_attention_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (post_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (attention): GPTNeoXAttention(\n",
       "              (rotary_emb): GPTNeoXRotaryEmbedding()\n",
       "              (query_key_value): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=5120, out_features=15360, bias=True)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=5120, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=15360, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (dense): Linear4bit(in_features=5120, out_features=5120, bias=True)\n",
       "              (attention_dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (mlp): GPTNeoXMLP(\n",
       "              (dense_h_to_4h): Linear4bit(in_features=5120, out_features=20480, bias=True)\n",
       "              (dense_4h_to_h): Linear4bit(in_features=20480, out_features=5120, bias=True)\n",
       "              (act): GELUActivation()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (embed_out): Linear(in_features=5120, out_features=30080, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66470982-1dad-4e3d-9e29-a736fab51cf0",
   "metadata": {},
   "source": [
    "### Start Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2a70473b-78dd-4ede-9b81-4980e9404c3f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a GPTNeoXTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='36' max='36' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [36/36 09:47, Epoch 2/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.970600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.837500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.886500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>1.978900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.923500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>1.842900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>1.934400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>1.957300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>1.807000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.895300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>1.961800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>1.897600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>1.890900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>1.920700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>1.868100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>1.872200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>1.945600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>1.869000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model.config.use_cache = False\n",
    "\n",
    "# old_state_dict = model.state_dict\n",
    "# model.state_dict = (lambda self, *_, **__: get_peft_model_state_dict(self, old_state_dict())).__get__(\n",
    "#     model, type(model)\n",
    "# )\n",
    "\n",
    "if torch.__version__ >= \"2\" and sys.platform != \"win32\":\n",
    "    model = torch.compile(model)\n",
    "\n",
    "train_result = trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "841d0a1a-d4f6-4650-a2f2-af5375a126a5",
   "metadata": {},
   "source": [
    "### Check metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "31e72538-ca9e-4fe0-9927-61c4e24d6f7a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** train metrics *****\n",
      "  epoch                    =       2.88\n",
      "  total_flos               = 15753753GF\n",
      "  train_loss               =     1.9033\n",
      "  train_runtime            = 0:10:06.24\n",
      "  train_samples_per_second =      0.247\n",
      "  train_steps_per_second   =      0.059\n"
     ]
    }
   ],
   "source": [
    "metrics = train_result.metrics\n",
    "trainer.log_metrics(\"train\", metrics)\n",
    "#trainer.save_metrics(\"train\", metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c8155ff-b6b8-493a-96bf-321f605f9fad",
   "metadata": {},
   "source": [
    "### Save fine-tuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "52e04694-47c4-45b1-a034-09812e21dc5d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainer.model.save_pretrained(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "92919402",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('output/tokenizer_config.json',\n",
       " 'output/special_tokens_map.json',\n",
       " 'output/tokenizer.json')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.save_pretrained(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8dc4970b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'output_dir' (str)\n"
     ]
    }
   ],
   "source": [
    "%store output_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1a93c053-0543-4da7-9e6a-7b0f95e160d3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Free memory for merging weights\n",
    "#del model\n",
    "#del trainer\n",
    "#torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e56fad97",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "model.config.use_cache = True  # silence the warnings. Please re-enable for inference!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d8157c05",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/transformers/generation/utils.py:1355: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/transformers/generation/utils.py:1636: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cpu, whereas the model is on cuda. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cuda') before running `.generate()`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[    6,     6,     6,  2438,    29,  5565,   462,   272, 26343,   270,\n",
       "          6976,   489,    17,   503,   293,  7735,   272,   388,   296,  3133]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.generate(**tokenizer(\"### 질문: 안녕 나는 빡빡이 아저씨야\", return_tensors='pt', return_token_type_ids=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d8f90e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen(x):\n",
    "    gened = model.generate(\n",
    "        **tokenizer(\n",
    "            f\"### 질문: {x}\\n\\n### 답변:\", \n",
    "            return_tensors='pt', \n",
    "            return_token_type_ids=False\n",
    "        ), \n",
    "        max_new_tokens=256,\n",
    "        early_stopping=True,\n",
    "        do_sample=True,\n",
    "        eos_token_id=2,\n",
    "    )\n",
    "    print(tokenizer.decode(gened[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ced53ddf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:430: UserWarning: `num_beams` is set to 1. However, `early_stopping` is set to `True` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `early_stopping`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### 질문: 프로그래밍을 잘 하기 위한 세 가지 방법은?\n",
      "\n",
      "### 답변:\n",
      "세 가지 방법의 예에는 다음이 포함됩니다:1. 알고리즘을 구체적으로 작성하기: 문제를 식별하고, 필요한 것을 정확히 계산하고, 실행 가능한 구현을 작성하여 알고리즘을 구체적이고 이해하기 쉽게 만듭니다.2. 컴퓨터와 더 잘 작업하기: 컴퓨터가 내가 생각하는 것을 이해할 수 있도록 컴퓨터의 언어로 나의 생각을 설명합니다. 이는 코드, 텍스트 설명 또는 기타 구두 표현을 통해 이루어질 수 있습니다.3. 반복하기: 알고리즘을 처음부터 다시 작성합니다. 문제에 대해 새로운 아이디어를 더하고, 알고리즘을 수정하고, 구현을 테스트하여 작업에 대한 이해를 보장합니다.<|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "gen('프로그래밍을 잘 하기 위한 세 가지 방법은?')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47689a08",
   "metadata": {},
   "source": [
    "### 2_local-infer-debug-lora 과정을 위해 allocated CUDA memory를 Release합니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2f333a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Free memory for merging weights\n",
    "del model\n",
    "del trainer\n",
    "torch.cuda.empty_cache()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
