import base64
import streamlit as st  # ëª¨ë“  streamlit ëª…ë ¹ì€ "st" aliasë¡œ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
import bedrock as glib  # ë¡œì»¬ ë¼ì´ë¸ŒëŸ¬ë¦¬ ìŠ¤í¬ë¦½íŠ¸ì— ëŒ€í•œ ì°¸ì¡°
from langchain.callbacks import StreamlitCallbackHandler

##################### Functions ########################
def parse_image(image_base64):
    st.image(base64.b64decode(image_base64))

def parse_table(text_as_html):
    st.markdown(text_as_html, unsafe_allow_html=True)

# 'Separately' ì˜µì…˜ ì„ íƒ ì‹œ ë‚˜ì˜¤ëŠ” ì¤‘ê°„ Contextë¥¼ íƒ­ í˜•íƒœë¡œ ë³´ì—¬ì£¼ëŠ” UI
def show_context_with_tab(contexts):
    tab_category = ["Semantic", "Keyword", "Without Reranker", "Similar Docs"]
    tab_contents = {
        tab_category[0]: [],
        tab_category[1]: [],
        tab_category[2]: [],
        tab_category[3]: []
    }
    for i, contexts_by_doctype in enumerate(contexts):
        tab_contents[tab_category[i]].append(contexts_by_doctype)
    tabs = st.tabs(tab_category)
    for i, tab in enumerate(tabs):
        category = tab_category[i]
        with tab:
            st.header(category)
            for contexts_by_doctype in tab_contents[category]:
                for context in contexts_by_doctype:
                    st.markdown('##### `ì •í™•ë„`: {}'.format(context["score"]))
                    for line in context["lines"]:
                        st.write(line)
                    ### TODO: context["meta"] ì—ì„œ ì´ë¯¸ì§€/í…Œì´ë¸” ë½‘ê¸° (orig_elements í˜¹ì€ image_base64)
                    ### TODO: parent_docs ì„ íƒ ì‹œ ë°œìƒí•˜ëŠ” ì˜¤ë¥˜ fix
                    
# 'Separately' ì˜µì…˜ ì„ íƒ ì‹œ ë‚˜ì˜¤ëŠ” ì¤‘ê°„ Contextë¥¼ expander í˜•íƒœë¡œ ë³´ì—¬ì£¼ëŠ” UI -- í˜„ì¬ ë¯¸ì‚¬ìš©
def show_context_with_expander(contexts):
    context_no = 0
    for context in contexts:
        context_no += 1
        st.markdown("### {}".format(context_no))
        # Contexts ë‚´ìš© ì¶œë ¥
        page_content = context.page_content
        st.markdown(page_content)
                    
        # Image, Table ì´ ìˆì„ ê²½ìš° íŒŒì‹±í•´ ì¶œë ¥
        metadata = context.metadata
        category = "None"
        if "category" in context.metadata:
            category = metadata["category"]
            if category == "Table":
                parse_table(metadata["text_as_html"])
            elif category == "Image":
                parse_image(metadata["image_base64"])
            else: 
                pass
        st.markdown(''' - - - ''')

# 'All at once' ì˜µì…˜ ì„ íƒ ì‹œ 4ê°œì˜ ì»¬ëŸ¼ìœ¼ë¡œ ë‚˜ëˆ„ì–´ ê²°ê³¼ í‘œì‹œí•˜ëŠ” UI
# TODO: HyDE, RagFusion ì¶”ê°€ ë…¼ì˜ í•„ìš”
def show_answer_with_multi_columns(answers): 
    col1, col2, col3, col4 = st.columns(4)
    with col1:
        st.markdown('''### `Lexical search` ''')
        st.write(answers[0])
    with col2:
        st.markdown('''### `Semantic search` ''')
        st.write(answers[1])
    with col3:
        st.markdown('''### + `Reranker` ''')
        st.write(answers[2])
    with col4:
        st.markdown('''### + `Parent_docs` ''') 
        st.write(answers[3])

####################### Application ###############################
st.set_page_config(layout="wide")
st.title("AWS Q&A Bot with Advanced RAG!")  # page ì œëª©

st.markdown('''- This chatbot is implemented using Amazon Bedrock Claude v3 Sonnet.''')
st.markdown('''- Integrated advanced RAG technology: **Hybrid Search, ReRanker, and Parent Document, HyDE, Rag Fusion** techniques.''')
st.markdown('''- The original data is stored in Amazon OpenSearch, and the embedding model utilizes Amazon Titan.''')
st.markdown('''
            - You can find the source code in 
            [this Github](https://github.com/aws-samples/aws-ai-ml-workshop-kr/tree/master/genai/aws-gen-ai-kr/20_applications/02_qa_chatbot/04_web_ui)
            ''')
# Store the initial value of widgets in session state
if "showing_option" not in st.session_state:
    st.session_state.showing_option = "Separately"
if "search_mode" not in st.session_state:
    st.session_state.search_mode = "Hybrid search"
if "hyde_or_ragfusion" not in st.session_state:
    st.session_state.hyde_or_ragfusion = "None"
disabled = st.session_state.showing_option=="All at once"

with st.sidebar: # Sidebar ëª¨ë¸ ì˜µì…˜
    with st.container(border=True):
        st.radio(
            "Choose UI between 2 options:",
            ["Separately", "All at once"],
            captions = ["ì•„ë˜ì—ì„œ ì„¤ì •í•œ íŒŒë¼ë¯¸í„° ì¡°í•©ìœ¼ë¡œ í•˜ë‚˜ì˜ ê²€ìƒ‰ ê²°ê³¼ê°€ ë„ì¶œë©ë‹ˆë‹¤.", "ì—¬ëŸ¬ ì˜µì…˜ë“¤ì„ í•œ í™”ë©´ì—ì„œ í•œêº¼ë²ˆì— ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤."],
            key="showing_option",
        )
    st.markdown('''### Set parameters for your Bot ğŸ‘‡''')

    with st.container(border=True):
        search_mode = st.radio(
            "Choose a search mode:",
            ["Lexical search", "Semantic search", "Hybrid search"],
            captions = [
                "í‚¤ì›Œë“œì˜ ì¼ì¹˜ ì—¬ë¶€ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤.",
                "í‚¤ì›Œë“œì˜ ì¼ì¹˜ ì—¬ë¶€ë³´ë‹¤ëŠ” ë¬¸ë§¥ì˜ ì˜ë¯¸ì  ìœ ì‚¬ë„ì— ê¸°ë°˜í•´ ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤.", 
                "ì•„ë˜ì˜ Alpha ê°’ì„ ì¡°ì •í•˜ì—¬ Lexical/Semantic searchì˜ ë¹„ìœ¨ì„ ì¡°ì •í•©ë‹ˆë‹¤."
                ],
            key="search_mode",
            disabled=disabled
            )
        alpha = st.slider('Alpha value for Hybrid search â¬‡ï¸', 0.0, 0.51, 1.0, 
                          disabled=st.session_state.search_mode != "Hybrid search",
                          help="""Alpha=0.0 ì´ë©´ Lexical search,   \nAlpha=1.0 ì´ë©´ Semantic search ì…ë‹ˆë‹¤."""
                          )
        if search_mode == "Lexical search":
            alpha = 0.0
        elif search_mode == "Semantic search":
            alpha = 1.0
    
    col1, col2 = st.columns(2)
    with col1:
        reranker = st.toggle("Reranker", 
                             help="""ì´ˆê¸° ê²€ìƒ‰ ê²°ê³¼ë¥¼ ì¬í‰ê°€í•˜ì—¬ ìˆœìœ„ë¥¼ ì¬ì¡°ì •í•˜ëŠ” ëª¨ë¸ì…ë‹ˆë‹¤.   
                             ë¬¸ë§¥ ì •ë³´ì™€ ì§ˆì˜ ê´€ë ¨ì„±ì„ ê³ ë ¤í•˜ì—¬ ì í•©í•œ ê²°ê³¼ë¥¼ ìƒìœ„ì— ì˜¬ë¦½ë‹ˆë‹¤.""",
                             disabled=disabled)
    with col2:
        parent = st.toggle("Parent Docs", 
                           help="""ë‹µë³€ ìƒì„± ëª¨ë¸ì´ ì§ˆì˜ì— ëŒ€í•œ ë‹µë³€ì„ ìƒì„±í•  ë•Œ ì°¸ì¡°í•œ ì •ë³´ì˜ ì¶œì²˜ë¥¼ í‘œì‹œí•˜ëŠ” ì˜µì…˜ì…ë‹ˆë‹¤.""", 
                           disabled=disabled)

    with st.container(border=True):
        hyde_or_ragfusion = st.radio(
            "Choose a RAG option:",
            ["None", "HyDE", "RAG-Fusion"],
            captions = [
                "", 
                "ë¬¸ì„œì™€ ì§ˆì˜ ê°„ì˜ ì˜ë¯¸ì  ìœ ì‚¬ë„ë¥¼ ì¸¡ì •í•˜ê¸° ìœ„í•œ ì„ë² ë”© ê¸°ë²•ì…ë‹ˆë‹¤. í•˜ì´í¼ë³¼ë¦­ ê³µê°„ì—ì„œ ê±°ë¦¬ë¥¼ ê³„ì‚°í•˜ì—¬ ìœ ì‚¬ë„ë¥¼ ì¸¡ì •í•©ë‹ˆë‹¤.", 
                "ê²€ìƒ‰ê³¼ ìƒì„±ì„ ê²°í•©í•œ ëª¨ë¸ë¡œ, ê²€ìƒ‰ ëª¨ë“ˆì´ ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ê³  ìƒì„± ëª¨ë“ˆì´ ì´ë¥¼ ì°¸ì¡°í•˜ì—¬ ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤. ë‘ ëª¨ë“ˆì˜ ì¶œë ¥ì„ ìœµí•©í•˜ì—¬ ìµœì¢… ë‹µë³€ì„ ë„ì¶œí•©ë‹ˆë‹¤."
                ],
            key="hyde_or_ragfusion",
            disabled=disabled
            ) 
        hyde = hyde_or_ragfusion == "HyDE"
        ragfusion = hyde_or_ragfusion == "RAG-Fusion"

###### 1) 'Separately' ì˜µì…˜ ì„ íƒí•œ ê²½ìš° ######
if st.session_state.showing_option == "Separately":
    if "messages" not in st.session_state:
        st.session_state["messages"] = [
            {"role": "assistant", "content": "How can I help you?"}
        ]
    # ì§€ë‚œ ë‹µë³€ ì¶œë ¥
    for msg in st.session_state.messages:
        # ì§€ë‚œ ë‹µë³€ì— ëŒ€í•œ ì»¨í…ìŠ¤íŠ¸ ì¶œë ¥
        if msg["role"] == "assistant_context": 
            with st.chat_message("assistant"):
                with st.expander("Context í™•ì¸í•˜ê¸° â¬‡ï¸"):
                    show_context_with_tab(contexts=msg["content"])
        elif msg["role"] == "assistant_column":
            # 'Separately' ì˜µì…˜ì¼ ê²½ìš° multi column ìœ¼ë¡œ ë³´ì—¬ì£¼ì§€ ì•Šê³  ì²« ë²ˆì§¸ ë‹µë³€ë§Œ ì¶œë ¥
            st.chat_message(msg["role"]).write(msg["content"][0]) 
        else:
            st.chat_message(msg["role"]).write(msg["content"])
    
    # ìœ ì €ê°€ ì“´ chatì„ queryë¼ëŠ” ë³€ìˆ˜ì— ë‹´ìŒ
    query = st.chat_input("Search documentation")
    if query:
        # Sessionì— ë©”ì„¸ì§€ ì €ì¥
        st.session_state.messages.append({"role": "user", "content": query})
        
        # UIì— ì¶œë ¥
        st.chat_message("user").write(query)
        
        # Streamlit callback handlerë¡œ bedrock streaming ë°›ì•„ì˜¤ëŠ” ì»¨í…Œì´ë„ˆ ì„¤ì •
        st_cb = StreamlitCallbackHandler(
            st.chat_message("assistant"), 
            collapse_completed_thoughts=True
            )
        # bedrock.pyì˜ invoke í•¨ìˆ˜ ì‚¬ìš©
        response = glib.invoke(
            query=query, 
            streaming_callback=st_cb, 
            parent=parent, 
            reranker=reranker,
            hyde = hyde,
            ragfusion = ragfusion,
            alpha = alpha
        )
        # response ë¡œ ë©”ì„¸ì§€, ë§í¬, ë ˆí¼ëŸ°ìŠ¤(source_documents) ë°›ì•„ì˜¤ê²Œ ì„¤ì •ëœ ê²ƒì„ ë³€ìˆ˜ë¡œ ì €ì¥
        answer = response[0]
        contexts = response[1] 

        # UI ì¶œë ¥
        st.chat_message("assistant").write(answer)
        
        with st.chat_message("assistant"): 
            with st.expander("ì •í™•ë„ ë³„ ë‹µë³€ ë³´ê¸° â¬‡ï¸"):
                show_context_with_tab(contexts)

        # Session ë©”ì„¸ì§€ ì €ì¥
        st.session_state.messages.append({"role": "assistant", "content": answer})
        st.session_state.messages.append({"role": "assistant_context", "content": contexts})
        
        # Thinkingì„ completeë¡œ ìˆ˜ë™ìœ¼ë¡œ ë°”ê¾¸ì–´ ì¤Œ
        st_cb._complete_current_thought()

###### 2) 'All at once' ì˜µì…˜ ì„ íƒí•œ ê²½ìš° ######
else:
    if "messages" not in st.session_state:
        st.session_state["messages"] = [
            {"role": "assistant", "content": "How can I help you?"}
        ]
    # ì§€ë‚œ ë‹µë³€ ì¶œë ¥
    for msg in st.session_state.messages:
        if msg["role"] == "assistant_column":
            answers = msg["content"]
            show_answer_with_multi_columns(answers)
        elif msg["role"] == "assistant_context": 
            pass # 'All at once' ì˜µì…˜ ì„ íƒ ì‹œì—ëŠ” context ë¡œê·¸ë¥¼ ì¶œë ¥í•˜ì§€ ì•ŠìŒ
        else:
            st.chat_message(msg["role"]).write(msg["content"])
    
    # ìœ ì €ê°€ ì“´ chatì„ queryë¼ëŠ” ë³€ìˆ˜ì— ë‹´ìŒ
    query = st.chat_input("Search documentation")
    if query:
        # Sessionì— ë©”ì„¸ì§€ ì €ì¥
        st.session_state.messages.append({"role": "user", "content": query})
        
        # UIì— ì¶œë ¥
        st.chat_message("user").write(query)

        col1, col2, col3, col4 = st.columns(4)
        with col1:
            st.markdown('''### `Lexical search` ''')
            st.markdown(":green[: Alpha ê°’ì´ 0.0]ìœ¼ë¡œ, í‚¤ì›Œë“œì˜ ì •í™•í•œ ì¼ì¹˜ ì—¬ë¶€ë¥¼ íŒë‹¨í•˜ëŠ” Lexical search ê²°ê³¼ì…ë‹ˆë‹¤.")
        with col2:
            st.markdown('''### `Semantic search` ''')
            st.markdown(":green[: Alpha ê°’ì´ 1.0]ìœ¼ë¡œ, í‚¤ì›Œë“œ ì¼ì¹˜ ì—¬ë¶€ë³´ë‹¤ëŠ” ë¬¸ë§¥ì˜ ì˜ë¯¸ì  ìœ ì‚¬ë„ì— ê¸°ë°˜í•œ Semantic search ê²°ê³¼ì…ë‹ˆë‹¤.")
        with col3:
            st.markdown('''### + `Reranker` ''')
            st.markdown(""": ì´ˆê¸° ê²€ìƒ‰ ê²°ê³¼ë¥¼ ì¬í‰ê°€í•˜ì—¬ ìˆœìœ„ë¥¼ ì¬ì¡°ì •í•˜ëŠ” ëª¨ë¸ì…ë‹ˆë‹¤. ë¬¸ë§¥ ì •ë³´ì™€ ì§ˆì˜ ê´€ë ¨ì„±ì„ ê³ ë ¤í•˜ì—¬ ì í•©í•œ ê²°ê³¼ë¥¼ ìƒìœ„ì— ì˜¬ë¦½ë‹ˆë‹¤.
                        :green[Alpha ê°’ì€ ì™¼ìª½ ì‚¬ì´ë“œë°”ì—ì„œ ì„¤ì •í•˜ì‹  ê°’]ìœ¼ë¡œ ì ìš©ë©ë‹ˆë‹¤.""")
        with col4:
            st.markdown('''### + `Parent Docs` ''')
            st.markdown(""": ì§ˆì˜ì— ëŒ€í•œ ë‹µë³€ì„ ìƒì„±í•  ë•Œ ì°¸ì¡°í•˜ëŠ” ë¬¸ì„œ ì§‘í•©ì…ë‹ˆë‹¤. ë‹µë³€ ìƒì„± ëª¨ë¸ì´ ì°¸ì¡°í•  ìˆ˜ ìˆëŠ” ê´€ë ¨ ì •ë³´ì˜ ì¶œì²˜ê°€ ë©ë‹ˆë‹¤.
                        :green[Alpha ê°’ì€ ì™¼ìª½ ì‚¬ì´ë“œë°”ì—ì„œ ì„¤ì •í•˜ì‹  ê°’]ìœ¼ë¡œ ì ìš©ë©ë‹ˆë‹¤.""")
        
        with col1:
            # Streamlit callback handlerë¡œ bedrock streaming ë°›ì•„ì˜¤ëŠ” ì»¨í…Œì´ë„ˆ ì„¤ì •
            st_cb = StreamlitCallbackHandler(
                st.chat_message("assistant"), 
                collapse_completed_thoughts=True
                )
            answer1 = glib.invoke(
                query=query, 
                streaming_callback=st_cb, 
                parent=False, 
                reranker=False,
                hyde = False,
                ragfusion = False,
                alpha = 0 # Lexical search
                )[0]
            st.write(answer1)
            st_cb._complete_current_thought() # Thinkingì„ completeë¡œ ìˆ˜ë™ìœ¼ë¡œ ë°”ê¾¸ì–´ ì¤Œ
        with col2:
            st_cb = StreamlitCallbackHandler(
                st.chat_message("assistant"), 
                collapse_completed_thoughts=True
                )
            answer2 = glib.invoke(
                query=query, 
                streaming_callback=st_cb, 
                parent=False, 
                reranker=False,
                hyde = False,
                ragfusion = False,
                alpha = 1.0 # Semantic search
                )[0]
            st.write(answer2)
            st_cb._complete_current_thought() 
        with col3:
            st_cb = StreamlitCallbackHandler(
                st.chat_message("assistant"), 
                collapse_completed_thoughts=True
                )
            answer3 = glib.invoke(
                query=query, 
                streaming_callback=st_cb, 
                parent=False, 
                reranker=True, # Add Reranker option
                hyde = False,
                ragfusion = False,
                alpha = alpha # Hybrid search
                )[0]
            st.write(answer3)
            st_cb._complete_current_thought() 
        with col4:
            st_cb = StreamlitCallbackHandler(
                st.chat_message("assistant"), 
                collapse_completed_thoughts=True
            )
            answer4 = glib.invoke(
                query=query, 
                streaming_callback=st_cb, 
                parent=True, # Add Parent_docs option
                reranker=True, # Add Reranker option
                hyde = False,
                ragfusion = False,
                alpha = alpha # Hybrid search
                )[0]
            st.write(answer4)
            st_cb._complete_current_thought()

        # Session ë©”ì„¸ì§€ ì €ì¥
        answers = [answer1, answer2, answer3, answer4]
        st.session_state.messages.append({"role": "assistant_column", "content": answers})
