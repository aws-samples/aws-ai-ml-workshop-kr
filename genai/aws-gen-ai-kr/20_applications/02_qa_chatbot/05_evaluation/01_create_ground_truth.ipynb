{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "039b6554-6bfc-42ea-b383-fa5490781b55",
   "metadata": {},
   "source": [
    "# Ground Truth Generator\n",
    "- For retriever\n",
    "- For generator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6bf72bf-9a8b-47ca-9420-645d84a5235e",
   "metadata": {},
   "source": [
    "## Setting\n",
    " - Auto Reload\n",
    " - path for utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84024e4b-8ea9-43ab-929d-da56c96b4564",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fb55d8f-5651-4baa-b1a6-721df8ef7cf9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys, os\n",
    "module_path = \"../../..\"\n",
    "sys.path.append(os.path.abspath(module_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "064ed28a-72ba-45df-8e38-3784d80ab73d",
   "metadata": {},
   "source": [
    "## 1. Bedrock Client 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "918ddfff-4854-48f1-9069-e425f5f0afe0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import boto3\n",
    "from pprint import pprint\n",
    "from termcolor import colored\n",
    "from utils import bedrock, print_ww\n",
    "from utils.bedrock import bedrock_info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afcb445b-9ada-494e-b356-87d6d25f553f",
   "metadata": {},
   "source": [
    "### ---- ⚠️ Un-comment and edit the below lines as needed for your AWS setup ⚠️ ----\n",
    "- os.environ[\"AWS_DEFAULT_REGION\"] = \"<REGION_NAME>\"  # E.g. \"us-east-1\"\n",
    "- os.environ[\"AWS_PROFILE\"] = \"<YOUR_PROFILE>\"\n",
    "- os.environ[\"BEDROCK_ASSUME_ROLE\"] = \"<YOUR_ROLE_ARN>\"  # E.g. \"arn:aws:...\"\n",
    "- os.environ[\"BEDROCK_ENDPOINT_URL\"] = \"<YOUR_ENDPOINT_URL>\"  # E.g. \"https://...\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ebae5f65-9517-4eeb-9874-50d3775c474e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create new client\n",
      "  Using region: None\n",
      "  Using profile: None\n",
      "boto3 Bedrock client successfully created!\n",
      "bedrock-runtime(https://bedrock-runtime.us-east-1.amazonaws.com)\n",
      "\u001b[32m\n",
      "== FM lists ==\u001b[0m\n",
      "{'Claude-Instant-V1': 'anthropic.claude-instant-v1',\n",
      " 'Claude-V1': 'anthropic.claude-v1',\n",
      " 'Claude-V2': 'anthropic.claude-v2',\n",
      " 'Claude-V2-1': 'anthropic.claude-v2:1',\n",
      " 'Cohere-Embeddings-En': 'cohere.embed-english-v3',\n",
      " 'Cohere-Embeddings-Multilingual': 'cohere.embed-multilingual-v3',\n",
      " 'Command': 'cohere.command-text-v14',\n",
      " 'Command-Light': 'cohere.command-light-text-v14',\n",
      " 'Jurassic-2-Mid': 'ai21.j2-mid-v1',\n",
      " 'Jurassic-2-Ultra': 'ai21.j2-ultra-v1',\n",
      " 'Llama2-13b-Chat': 'meta.llama2-13b-chat-v1',\n",
      " 'Titan-Embeddings-G1': 'amazon.titan-embed-text-v1',\n",
      " 'Titan-Text-G1': 'amazon.titan-text-express-v1',\n",
      " 'Titan-Text-G1-Light': 'amazon.titan-text-lite-v1'}\n"
     ]
    }
   ],
   "source": [
    "boto3_bedrock = bedrock.get_bedrock_client(\n",
    "    assumed_role=os.environ.get(\"BEDROCK_ASSUME_ROLE\", None),\n",
    "    endpoint_url=os.environ.get(\"BEDROCK_ENDPOINT_URL\", None),\n",
    "    region=os.environ.get(\"AWS_DEFAULT_REGION\", None),\n",
    ")\n",
    "\n",
    "aws_region = os.environ.get(\"AWS_DEFAULT_REGION\", None)\n",
    "print (colored(\"\\n== FM lists ==\", \"green\"))\n",
    "pprint (bedrock_info.get_list_fm_models())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23156de5-c854-45d1-b964-9e10d9976935",
   "metadata": {},
   "source": [
    "## 2.LLM 로딩 (Claude-v2.1 for retriever, Jurassic for reasoning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f2f05efc-4be9-4d82-836b-5e9a9469f332",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.llms.bedrock import Bedrock\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "58dca13d-e6ab-4642-a0ef-1ba26c88ab2e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "llm_claude = Bedrock(\n",
    "    model_id=bedrock_info.get_model_id(model_name=\"Claude-V2-1\"),\n",
    "    client=boto3_bedrock,\n",
    "    model_kwargs={\n",
    "        \"max_tokens_to_sample\": 512\n",
    "    },\n",
    "    streaming=False,\n",
    "    callbacks=[StreamingStdOutCallbackHandler()]\n",
    ")\n",
    "\n",
    "llm_jurassic = Bedrock(\n",
    "    model_id=bedrock_info.get_model_id(model_name=\"Jurassic-2-Ultra\"),\n",
    "    client=boto3_bedrock,\n",
    "    model_kwargs={\n",
    "        #\"max_tokens\": 512,\n",
    "        \"maxTokens\": 512\n",
    "    },\n",
    "    streaming=False,\n",
    "    callbacks=[StreamingStdOutCallbackHandler()]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80ff1f1c-e49c-4bee-8810-401ecd27afb4",
   "metadata": {},
   "source": [
    "## 2. OpenSearch 정의\n",
    "### 선수 조건\n",
    "- 01_preprocess_docs/02_load_docs_opensearch.ipynb를 통해서 OpenSearch Index 가 생성이 되어 있어야 합니다.\n",
    "#### [중요] 아래에 aws parameter store 에 아래 인증정보가 먼저 입력되어 있어야 합니다.\n",
    "- 01_preprocess_docs/01_parameter_store_example.ipynb 참고"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "003b4187-4ee5-43a8-bd91-a17ad2d94b2c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from utils.proc_docs import get_parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1e0c2dc2-c36f-4b89-8a6f-88ecb77057d5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "aws_region = \"us-east-1\"\n",
    "ssm = boto3.client(\"ssm\", aws_region)\n",
    "\n",
    "opensearch_domain_endpoint = get_parameter(\n",
    "    boto3_client = ssm,\n",
    "    parameter_name = 'knox_opensearch_domain_endpoint',\n",
    ")\n",
    "\n",
    "opensearch_user_id = get_parameter(\n",
    "    boto3_client = ssm,\n",
    "    parameter_name = 'knox_opensearch_userid',\n",
    ")\n",
    "\n",
    "opensearch_user_password = get_parameter(\n",
    "    boto3_client = ssm,\n",
    "    parameter_name = 'knox_opensearch_password',\n",
    ")\n",
    "http_auth = (opensearch_user_id, opensearch_user_password) # Master username, Master password"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ff9abd1-acd6-41df-b416-62c92d329a2b",
   "metadata": {},
   "source": [
    "### Index 이름 셋팅\n",
    "- 이전 노트북 01_preprocess_docs/02_load_docs_opensearch.ipynb를 통해서 생성된 OpenSearch Index name 입력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "904ed70e-34dd-45ab-8e08-eb41ad60e6c2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "index_name = \"v17-genai-poc-knox-kor-eval-parent-doc-retriever\"\n",
    "#index_name = \"v16-genai-poc-knox-eval-parent-doc-retriever\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58befff4-cf96-49c4-8ced-438b560defe9",
   "metadata": {
    "tags": []
   },
   "source": [
    "### OpenSearch Client 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0b0e141d-ad7c-41da-9f9b-f1ffb111989d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from utils.opensearch import opensearch_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "661eb022-6db5-41c6-9231-b2c73933a7ae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "os_client = opensearch_utils.create_aws_opensearch_client(\n",
    "    aws_region,\n",
    "    opensearch_domain_endpoint,\n",
    "    http_auth\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4498ad54-21a8-42fc-b3d5-e2cc621bb4e0",
   "metadata": {},
   "source": [
    "## 3. Ground Truth Generator 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2600d723-0a3c-45d9-ab03-6c7c87c4280c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from termcolor import colored\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b2ff3cf2-3d8e-4ce4-9854-c25966a90c9e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "retriever_prompt_template = \"\"\"\n",
    "\\n\\nHuman: Here is the context information, inside <context></context> XML tags.\n",
    "\n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "\n",
    "Given the context information and not prior knowledge.\n",
    "generate only questions based on the below query.\n",
    "\n",
    "You are a Professor. Your task is to setup \\\n",
    "{num_questions_per_chunk} questions for an upcoming \\\n",
    "quiz/examination. The questions should be diverse in nature \\\n",
    "across the document. The questions should not contain options, start with \"-\"\n",
    "Restrict the questions to the context information provided.\n",
    "Write in Korean.\n",
    "\n",
    "\\n\\nAssistant:\"\"\"\n",
    "\n",
    "PROMPT_RETRIEVER = PromptTemplate(\n",
    "    template=retriever_prompt_template, input_variables=[\"context\", \"num_questions_per_chunk\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c97e8559-c5a8-4b4d-9a7e-fa512fdd3fda",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "generation_prompt_template = \"\"\"\n",
    "Here is the context, inside <context></context> XML tags.\n",
    "\n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "\n",
    "Only using the context as above, answer the following question with the rules as below:\n",
    "    - Don't insert XML tag such as <context> and </context> when answering.\n",
    "    - Write as much as you can\n",
    "    - Be courteous and polite\n",
    "    - Only answer the question if you can find the answer in the context with certainty.\n",
    "    - Skip the preamble\n",
    "    - Use three sentences maximum and keep the answer concise.\n",
    "    - If the answer is not in the context, just say \"Could not find answer in given contexts.\"\n",
    "    - Answer in Korean.\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Answer:\"\"\"\n",
    "\n",
    "PROMPT_GENERATION = PromptTemplate(\n",
    "    template=generation_prompt_template, input_variables=[\"context\", \"question\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3372c64e-7f2b-42d3-ace3-2066f95a2aff",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def GTGenerator(os_client, llm_retriever, llm_generation, prompt_retriever, \\\n",
    "                prompt_generation,  docs_per_request, parent_document=False, num_questions_per_chunk=2):\n",
    "\n",
    "    is_done = False\n",
    "    offset = 0\n",
    "    count = 0\n",
    "    limit = docs_per_request = 5\n",
    "    fetched_count = 0\n",
    "    loop_count = 0\n",
    "\n",
    "    if parent_document:\n",
    "        query = {\n",
    "            \"query\": {\n",
    "                    \"bool\" : {\n",
    "                        \"must\" : {\n",
    "                            \"match_all\": {}\n",
    "                        },\n",
    "                        \"filter\": [\n",
    "                            {'term': {'metadata.family_tree': 'child'}}\n",
    "                        ]\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "    else:\n",
    "        query = {\"query\": {\"match_all\": {}}}\n",
    "\n",
    "    llm_chain_retriever = LLMChain(llm=llm_retriever, prompt=prompt_retriever)\n",
    "    llm_chain_generation = LLMChain(llm=llm_generation, prompt=prompt_generation)\n",
    "    gt = [] # [question, 정답 id, 정답 text]\n",
    "\n",
    "    while not is_done:\n",
    "        try:\n",
    "            fetched_count += 1\n",
    "            fetched_docs = os_client.search(\n",
    "                index=index_name,\n",
    "                body=query,\n",
    "                size=limit,\n",
    "                from_=offset\n",
    "            )\n",
    "            fetched_count = 0\n",
    "        except Exception as e:\n",
    "            if fetched_count == 3:\n",
    "                print(\"Terminating script as connection is timeout more than 3 times.\")\n",
    "                break\n",
    "            print (\"{} Couldn't get records trying again for limit:{} and offset:{}\".format(e, limit, offset))\n",
    "            continue\n",
    "\n",
    "        fetched_docs = fetched_docs[\"hits\"][\"hits\"]\n",
    "        loop_count += 1\n",
    "\n",
    "        for index, doc in enumerate(fetched_docs):\n",
    "            # Process the doc here.\n",
    "            doc_id = doc[\"_id\"]\n",
    "            doc_text = doc[\"_source\"][\"text\"]\n",
    "            #print (colored(f'DOC ID: {doc_id}', \"green\"))\n",
    "            #print (colored(f'TEXT: {doc_text}', \"blue\"))\n",
    "\n",
    "            questions = llm_chain_retriever.predict(context=doc_text, num_questions_per_chunk=str(num_questions_per_chunk))\n",
    "            #print (questions)\n",
    "            questions = questions.split(\"\\n\\n-\")\n",
    "            if len(questions) <= num_questions_per_chunk + 1:\n",
    "\n",
    "                if len(questions) == num_questions_per_chunk:\n",
    "                    questions = list(map(lambda x:x.strip(), questions))\n",
    "                else:\n",
    "                    questions = list(map(lambda x:x.strip(), questions[1:]))\n",
    "                for q in questions:\n",
    "                    answer = llm_chain_generation.predict(question=q, context=doc_text)\n",
    "                    answer = answer.strip()\n",
    "                    #answer = answer[1:-1].strip()\n",
    "                    #print (colored(f'question: {q}', \"green\"))\n",
    "                    #print (colored(f'answer: {answer}', \"blue\"))\n",
    "                    gt.append([q, answer, doc_id, doc_text])\n",
    "            else:\n",
    "                print (\"err\")\n",
    "                print (questions)\n",
    "\n",
    "            #print (\"==\")\n",
    "        #break\n",
    "        #if loop_count == 10: break\n",
    "        offset += docs_per_request\n",
    "        if len(fetched_docs) < docs_per_request:\n",
    "            print(\"This is last batch.\")\n",
    "            is_done = True\n",
    "\n",
    "        print(\"batch {} completed\".format(count))\n",
    "    return gt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "12dab143-d8e9-4108-9089-00312f9afd6d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 0 completed\n",
      "batch 0 completed\n",
      "batch 0 completed\n",
      "batch 0 completed\n",
      "batch 0 completed\n",
      "batch 0 completed\n",
      "batch 0 completed\n",
      "batch 0 completed\n",
      "batch 0 completed\n",
      "batch 0 completed\n",
      "batch 0 completed\n",
      "batch 0 completed\n",
      "batch 0 completed\n",
      "batch 0 completed\n",
      "batch 0 completed\n",
      "batch 0 completed\n",
      "batch 0 completed\n",
      "batch 0 completed\n",
      "batch 0 completed\n",
      "batch 0 completed\n",
      "batch 0 completed\n",
      "batch 0 completed\n",
      "batch 0 completed\n",
      "batch 0 completed\n",
      "batch 0 completed\n",
      "batch 0 completed\n",
      "batch 0 completed\n",
      "batch 0 completed\n",
      "batch 0 completed\n",
      "batch 0 completed\n",
      "This is last batch.\n",
      "batch 0 completed\n",
      "CPU times: user 1.39 s, sys: 35.4 ms, total: 1.42 s\n",
      "Wall time: 26min 51s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "gt = GTGenerator(\n",
    "    os_client=os_client,\n",
    "    llm_retriever=llm_claude,\n",
    "    llm_generation=llm_claude, #llm_jurassic,\n",
    "    prompt_retriever=PROMPT_RETRIEVER,\n",
    "    prompt_generation=PROMPT_GENERATION,\n",
    "    docs_per_request=5,\n",
    "    parent_document=True,\n",
    "    num_questions_per_chunk=1\n",
    ")\n",
    "\n",
    "eval_dataset_retriever = pd.DataFrame(gt, columns=[\"question\", \"answer\", \"doc_id\", \"doc\"])\n",
    "eval_dataset_retriever.to_csv(\"eval_dataset_v17.csv\", index=False)\n",
    "#eval_dataset_retriever.to_pickle(\"eval_dataset.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b678b83f-f731-401e-879b-8aee9e9aa145",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58a171a5-2945-45e0-8eff-d54f264e362b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fd34ed3-9878-4a40-8530-a777d5bb94d3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "4b8e647a79df62bf31906a725b05de775d285962ac600487339d38c51a5c07b1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
