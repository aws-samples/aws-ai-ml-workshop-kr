{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2662eb8f-ba8b-4c34-8ab9-b312ecfcff91",
   "metadata": {},
   "source": [
    "# Korean LLM (Large Language Model) Serving on SageMaker with AWS Large Model Container DLC\n",
    "---\n",
    "* KULLM-Polyglot-12.8B-v2 모델로써, 130억개의 파라미터를 가진 LLM 모델이며, 최소 G5.12xlarge GPU 4개를 사용하는 모델입니다. \n",
    "* (이벤트엔진계정에서는 배포할수없습니다.)\n",
    "* 한국어 LLM 모델 SageMaker 서빙 핸즈온 (허깅페이스 허브에서 모델을 그대로 배포)\n",
    "- LLM GitHub: https://github.com/nlpai-lab/KULLM\n",
    "- Hugging Face model hub: https://huggingface.co/nlpai-lab/kullm-polyglot-12.8b-v2\n",
    "- [AWS Blog: Deploy large models on Amazon SageMaker using DJLServing and DeepSpeed model parallel inference](https://aws.amazon.com/ko/blogs/machine-learning/deploy-large-models-on-amazon-sagemaker-using-djlserving-and-deepspeed-model-parallel-inference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5a056d19-3339-4778-b73d-f5fe14d50ae0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import sys\n",
    "sys.path.append('./utils')\n",
    "sys.path.append('./templates')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "45ca4169-bc8f-4eeb-b897-b48b15f1c6eb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sagemaker, boto3, jinja2\n",
    "role = sagemaker.get_execution_role()  # execution role for the endpoint\n",
    "sess = sagemaker.session.Session()  # sagemaker session for interacting with different AWS APIs\n",
    "bucket = sess.default_bucket()  # bucket to house artifacts\n",
    "model_bucket = sess.default_bucket()  # bucket to house artifacts\n",
    "\n",
    "region = sess._region_name  # region name of the current SageMaker Studio environment\n",
    "account_id = sess.account_id()  # account_id of the current SageMaker Studio environment\n",
    "\n",
    "s3_client = boto3.client(\"s3\")  # client to intreract with S3 API\n",
    "sm_client = boto3.client(\"sagemaker\")  # client to intreract with SageMaker\n",
    "smr_client = boto3.client(\"sagemaker-runtime\")  # client to intreract with SageMaker Endpoints\n",
    "jinja_env = jinja2.Environment()  # jinja environment to generate model configuration templates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85236aea-d0e6-4b5f-89bf-2d185881740c",
   "metadata": {
    "tags": []
   },
   "source": [
    "<br>\n",
    "\n",
    "## 1. Download LLM model and upload it to S3\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8a104756-a389-403c-bb3a-b512ed6b948d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import snapshot_download\n",
    "from pathlib import Path\n",
    "\n",
    "model_id = \"nlpai-lab/kullm-polyglot-12.8b-v2\"\n",
    "model_prefix = model_id.split('/')[-1].replace('.', '-')\n",
    "\n",
    "s3_code_prefix = f\"ko-llm/{model_prefix}/code\"  # folder within bucket where code artifact will go\n",
    "s3_model_prefix = f\"ko-llm/{model_prefix}/model\"  # folder where model checkpoint will go"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99fca204-ff77-49c5-bc76-b5383c8ded4e",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## 2. Model Serving Scripts\n",
    "---\n",
    "### Create `serving.properties`\n",
    "\n",
    "이 설정 파일은 어떤 추론 최적화 라이브러리를 사용할지, 어떤 설정을 사용할지 DJL Serving에 알려주는 설정 파일입니다. 필요에 따라 적절한 구성을 설정할 수 있습니다.\n",
    "\n",
    "모델이 레이어에 따라 분할되는 파이프라인 병렬화(Pipeline Parallelism)를 사용하는 허깅페이스 Accelerate와 달리, DeepSpeed는 각 레이어(텐서)가 여러 디바이스에 걸쳐 샤딩되는 텐서 병렬화(Tensor Parallelism)를 사용합니다. 파이프라인 병렬 처리 접근 방식에서는 데이터가 각 GPU 장치를 통해 순차적으로 흐르지만, 텐서 병렬 처리는 데이터가 모든 GPU 장치로 전송되어 각 GPU에서 부분적인 결과가 계산됩니다. 그런 다음 All-Gather 연산을 통해 부분 결과를 수집하여 최종 결과를 계산합니다. 따라서, 텐서 병렬화가 일반적으로 더 높은 GPU 활용률과 더 나은 성능을 제공합니다.\n",
    "\n",
    "- `option.s3url` - 모델 파일의 위치를 지정합니다. 또는`option.model_id` 옵션을 대신 사용하여 허깅페이스 허브에서 모델을 지정할 수 있습니다(예: EleutherAI/gpt-j-6B). 그러면 허브에서 모델이 자동으로 다운로드됩니다. s3url 접근 방식은 자체 환경 내에서 모델 아티팩트를 호스팅할 수 있고 DJL 추론 컨테이너 내에서 최적화된 접근 방식을 활용하여 S3에서 호스팅 인스턴스로 모델을 전송함으로써 더 빠른 모델 배포가 가능합니다.\n",
    "\n",
    "`serving.properties`의 일반적인 설정법과 자세한 내용은 https://docs.aws.amazon.com/sagemaker/latest/dg/large-model-inference-configuration.html 를 참조하세요.\n",
    "\n",
    "<img src=\"./images/TensorShard.png\" width=\"800\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e960108-f6bf-4b8d-9ea6-78b6ca3c6915",
   "metadata": {
    "tags": [],
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "src_path = f\"src/{model_prefix}\"\n",
    "!rm -rf {src_path}\n",
    "os.makedirs(src_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5beab7e1-205c-47b1-b27a-0120614caec2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing src/kullm-polyglot-12-8b-v2/serving.properties\n"
     ]
    }
   ],
   "source": [
    "%%writefile {src_path}/serving.properties\n",
    "\n",
    "engine=DeepSpeed\n",
    "\n",
    "# passing extra options to model.py or built-in handler\n",
    "job_queue_size=100\n",
    "batch_size=1\n",
    "max_batch_delay=1\n",
    "max_idle_time=60\n",
    "\n",
    "# Built-in entrypoint\n",
    "#option.entryPoint=djl_python.deepspeed\n",
    "\n",
    "# Hugging Face model id\n",
    "#option.model_id={{model_id}}\n",
    "\n",
    "# defines custom environment variables\n",
    "#env=SERVING_NUMBER_OF_NETTY_THREADS=2\n",
    "\n",
    "# Allows to load DeepSpeed workers in parallel\n",
    "option.parallel_loading=true\n",
    "\n",
    "# specify tensor parallel degree (number of partitions)\n",
    "option.tensor_parallel_degree=4\n",
    "\n",
    "# specify per model timeout\n",
    "option.model_loading_timeout=600\n",
    "#option.predict_timeout=240\n",
    "\n",
    "# mark the model as failure after python process crashing 10 times\n",
    "retry_threshold=0\n",
    "\n",
    "option.task=text-generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76217c91-0a17-4895-a400-e8bd9889f00d",
   "metadata": {},
   "source": [
    "### Create model.py with custom inference code\n",
    "빌트인 추론 코드로 no-code로 배포할 수도 있지만, 커스텀 추론 코드를 작성하는 것도 가능합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ff9c01fd-d6cc-4dfc-9dfe-8a63ebd8cf60",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing src/kullm-polyglot-12-8b-v2/model.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {src_path}/model.py\n",
    "from djl_python import Input, Output\n",
    "import os\n",
    "import deepspeed\n",
    "import torch\n",
    "import logging\n",
    "from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer\n",
    "from transformers import GPTNeoXLayer\n",
    "\n",
    "predictor = None\n",
    "\n",
    "def get_model(properties):\n",
    "    \n",
    "    tp_degree = properties[\"tensor_parallel_degree\"]\n",
    "    # model_location = properties[\"model_dir\"]\n",
    "    # if \"model_id\" in properties:\n",
    "    #     model_location = properties[\"model_id\"]\n",
    "    model_location = \"nlpai-lab/kullm-polyglot-12.8b-v2\"      \n",
    "    task = properties[\"task\"]\n",
    "    \n",
    "    logging.info(f\"Loading model in {model_location}\")    \n",
    "    local_rank = int(os.getenv(\"LOCAL_RANK\", \"0\"))\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_location)\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_location,\n",
    "        torch_dtype=torch.float16,\n",
    "        low_cpu_mem_usage=True,\n",
    "    )\n",
    "    \n",
    "    model.requires_grad_(False)\n",
    "    model.eval()\n",
    "    \n",
    "    ds_config = {\n",
    "        \"tensor_parallel\": {\"tp_size\": tp_degree},\n",
    "        \"dtype\": model.dtype,\n",
    "        \"injection_policy\": {\n",
    "            GPTNeoXLayer:('attention.dense', 'mlp.dense_4h_to_h')\n",
    "        }\n",
    "    }\n",
    "    logging.info(f\"Starting DeepSpeed init with TP={tp_degree}\")        \n",
    "    model = deepspeed.init_inference(model, ds_config)  \n",
    "    \n",
    "    generator = pipeline(\n",
    "        task=task, model=model, tokenizer=tokenizer, device=local_rank\n",
    "    )\n",
    "    # https://huggingface.co/docs/hub/models-tasks\n",
    "    return generator\n",
    "    \n",
    "def handle(inputs: Input) -> None:\n",
    "    \"\"\"\n",
    "    inputs: Contains the configurations from serving.properties\n",
    "    \"\"\"    \n",
    "    global predictor\n",
    "    if not predictor:\n",
    "        predictor = get_model(inputs.get_properties())\n",
    "\n",
    "    if inputs.is_empty():\n",
    "        # Model server makes an empty call to warmup the model on startup\n",
    "        logging.info(\"is_empty\")\n",
    "        return None\n",
    "\n",
    "    data = inputs.get_as_json() #inputs.get_as_string()\n",
    "    logging.info(\"data:\", data)\n",
    "    \n",
    "    input_prompt, params = data[\"inputs\"], data[\"parameters\"]\n",
    "    result = predictor(input_prompt, **params)\n",
    "    logging.info(\"result:\", result)\n",
    "\n",
    "    return Output().add_as_json(result) #Output().add(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a063b634-3a4f-49ec-8779-fdda2818326e",
   "metadata": {},
   "source": [
    "### Create the Tarball and then upload to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "90fa1063-839a-46be-9ca4-f6b9c14efbc7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./\n",
      "./serving.properties\n",
      "./model.py\n"
     ]
    }
   ],
   "source": [
    "!rm -rf model.tar.gz\n",
    "!tar czvf model.tar.gz -C {src_path} ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1d60fd8a-8a0e-497c-8628-98eb26772cd9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S3 Code or Model tar ball uploaded to --- > s3://sagemaker-us-east-1-143656149352/ko-llm/kullm-polyglot-12-8b-v2/code/model.tar.gz\n"
     ]
    }
   ],
   "source": [
    "s3_code_artifact = sess.upload_data(\"model.tar.gz\", bucket, s3_code_prefix)\n",
    "print(f\"S3 Code or Model tar ball uploaded to --- > {s3_code_artifact}\")\n",
    "!rm -rf model.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57ea2358-3ba1-4ff3-9ca4-df772b59770d",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## 3. Serve LLM Model on SageMaker\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7820b1c3-7854-433d-bbb6-03193abefa22",
   "metadata": {},
   "source": [
    "### Create SageMaker Model\n",
    "\n",
    "SageMaker 엔드포인트 생성 매개변수 VolumeSizeInGB를 지정할 때 마운트되는 Amazon EBS(Amazon Elastic Block Store) 볼륨에 /tmp를 매핑하기 때문에 컨테이너는 인스턴스의 `/tmp` 공간에 모델을 다운로드합니다. 이때 s5cmd (https://github.com/peak/s5cmd) 를 활용하므로 대용량 모델을 빠르게 다운로드할 수 있습니다.\n",
    "볼륨 인스턴스와 함께 미리 빌드되어 제공되는 p4dn과 같은 인스턴스의 경우 컨테이너의 `/tmp`를 계속 활용할 수 있습니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "598b6ded-ba9c-4f25-b862-090546607b98",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kullm-polyglot-12-8b-v2-2023-07-23-14-04-08-969\n",
      "Created Model: arn:aws:sagemaker:us-east-1:143656149352:model/kullm-polyglot-12-8b-v2-2023-07-23-14-04-08-969\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.utils import name_from_base\n",
    "from sagemaker import image_uris\n",
    "\n",
    "img_uri = image_uris.retrieve(framework=\"djl-deepspeed\", region=region, version=\"0.23.0\")\n",
    "model_name = name_from_base(f\"{model_prefix}\")\n",
    "print(model_name)\n",
    "\n",
    "model_response = sm_client.create_model(\n",
    "    ModelName=model_name,\n",
    "    ExecutionRoleArn=role,\n",
    "    PrimaryContainer={\"Image\": img_uri, \"ModelDataUrl\": s3_code_artifact},\n",
    ")\n",
    "model_arn = model_response[\"ModelArn\"]\n",
    "print(f\"Created Model: {model_arn}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a96783b7-9e6a-4bed-8ff9-c779d9e628e4",
   "metadata": {},
   "source": [
    "### Create SageMaker Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0d2f670c-57f4-4092-af29-b1416829e9dd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created Endpoint: arn:aws:sagemaker:us-east-1:143656149352:endpoint/kullm-polyglot-12-8b-v2-2023-07-23-14-04-08-969-endpoint\n"
     ]
    }
   ],
   "source": [
    "endpoint_config_name = f\"{model_name}-config\"\n",
    "endpoint_name = f\"{model_name}-endpoint\"\n",
    "variant_name = \"variant1\"\n",
    "instance_type = \"ml.g5.12xlarge\"\n",
    "initial_instance_count = 1\n",
    "\n",
    "prod_variants = [\n",
    "    {\n",
    "        \"VariantName\": \"variant1\",\n",
    "        \"ModelName\": model_name,\n",
    "        \"InstanceType\": instance_type,\n",
    "        \"InitialInstanceCount\": initial_instance_count,\n",
    "        # \"ModelDataDownloadTimeoutInSeconds\": 2400,\n",
    "        \"ContainerStartupHealthCheckTimeoutInSeconds\": 1600,\n",
    "    }\n",
    "]\n",
    "\n",
    "endpoint_config_response = sm_client.create_endpoint_config(\n",
    "    EndpointConfigName=endpoint_config_name,\n",
    "    ProductionVariants=prod_variants\n",
    ")\n",
    "\n",
    "endpoint_response = sm_client.create_endpoint(\n",
    "    EndpointName=endpoint_name, EndpointConfigName=endpoint_config_name\n",
    ")\n",
    "print(f\"Created Endpoint: {endpoint_response['EndpointArn']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bf78f45-b06e-431c-9048-3ade776cac07",
   "metadata": {},
   "source": [
    "엔드포인트가 생성되는 동안 아래의 문서를 같이 확인해 보세요.\n",
    "- https://docs.aws.amazon.com/sagemaker/latest/dg/large-model-inference-dlc.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6122a3f8-78b6-42b9-b390-af8942d8e30c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<b> [SageMaker LLM Serving] <a target=\"blank\" href=\"https://console.aws.amazon.com/sagemaker/home?region=us-east-1#/endpoints/kullm-polyglot-12-8b-v2-2023-07-23-14-04-08-969-endpoint\">Check Endpoint Status</a></b>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "def make_console_link(region, endpoint_name, task='[SageMaker LLM Serving]'):\n",
    "    endpoint_link = f'<b> {task} <a target=\"blank\" href=\"https://console.aws.amazon.com/sagemaker/home?region={region}#/endpoints/{endpoint_name}\">Check Endpoint Status</a></b>'   \n",
    "    return endpoint_link\n",
    "\n",
    "endpoint_link = make_console_link(region, endpoint_name)\n",
    "display(HTML(endpoint_link))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9d43b855-4d79-4460-aecc-6128356214da",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Endpoint is  Creating\n",
      "Endpoint is  Creating\n",
      "Endpoint is  Creating\n",
      "Endpoint is  Creating\n",
      "Endpoint is  Creating\n",
      "Endpoint is  Creating\n",
      "Endpoint is  Creating\n",
      "Endpoint is  Creating\n",
      "Endpoint is  Creating\n",
      "Endpoint is  Creating\n",
      "Endpoint is  Creating\n",
      "Endpoint is  Creating\n",
      "Endpoint is  Creating\n",
      "Endpoint is  Creating\n",
      "Endpoint is  Creating\n",
      "Endpoint is  InService\n",
      "CPU times: user 307 ms, sys: 10.1 ms, total: 317 ms\n",
      "Wall time: 15min 2s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "from inference_lib import describe_endpoint, Prompter\n",
    "describe_endpoint(endpoint_name)         "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79b26c2e-b620-4df9-b712-c5aeb8e9e32a",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## 4. Inference\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd435259-7952-4a76-b4ee-3360da5dd7c5",
   "metadata": {},
   "source": [
    "엔드포인트를 호출할 때 이 텍스트를 JSON 페이로드 내에 제공해야 합니다. 이 JSON 페이로드에는 length, sampling strategy, output token sequence restrictions을 제어하는 데 도움이 되는 원하는 추론 매개변수가 포함될 수 있습니다. 허깅페이스 트랜스포머 transformers 라이브러리에는 [사용 가능한 페이로드 매개변수](https://huggingface.co/docs/transformers/main_classes/text_generation#transformers.GenerationConfig)의 전체 목록이 정의되어 있지만, 중요한 페이로드 매개변수는 다음과 같이 정의되어 있습니다:\n",
    "\n",
    "* **do_sample (`bool`)** – logits sampling 활성화\n",
    "* **max_new_tokens (`int`)** – 생성된 토큰의 최대 수\n",
    "* **best_of (`int`)** – best_of 개의 시퀀스를 생성하고 가장 높은 토큰 로그 확률이 있는 경우 반환\n",
    "* **repetition_penalty (`float`)** – 반복 패널티에 대한 파라미터, 1.0은 패널티가 없음을 의미하여 Greedy 서치와 동일, 커질수록 다양한 결과를 얻을 수 있으며, 자세한 사항은 [this paper](https://arxiv.org/pdf/1909.05858.pdf)을 참고\n",
    "* **return_full_text (`bool`)** – 생성된 텍스트에 프롬프트를 추가할지 여부\n",
    "* **seed (`int`)** – Random sampling seed\n",
    "* **stop_sequences (`List[str]`)** – `stop_sequences` 가 생성되면 토큰 생성을 중지\n",
    "* **temperature (`float`)** – logits 분포 모듈화에 사용되는 값\n",
    "* **top_k (`int`)** – 상위 K개 만큼 가장 높은 확률 어휘 토큰의 수\n",
    "* **top_p (`float`)** – 1 보다 작게 설정하게 되며, 상위부터 정렬했을 때 가능한 토큰들의 확률을 합산하여 `top_p` 이상의 가장 작은 집합을 유지\n",
    "* **truncate (`int`)** – 입력 토큰을 지정된 크기로 잘라냄\n",
    "* **typical_p (`float`)** – typical Decoding 양으로, 자세한 사항은 [Typical Decoding for Natural Language Generation](https://arxiv.org/abs/2202.00666)을 참고\n",
    "* **watermark (`bool`)** –  [A Watermark for Large Language Models](https://arxiv.org/abs/2301.10226)가 Watermarking\n",
    "* **decoder_input_details (`bool`)** – decoder input token logprobs와 ids를 반환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12185205-fc49-4716-aacd-c1017a8541a0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"do_sample\": False,\n",
    "    \"max_new_tokens\": 128,\n",
    "    \"temperature\": 0.4,\n",
    "    \"top_p\": 0.9,\n",
    "    \"return_full_text\": False,\n",
    "    \"repetition_penalty\": 1.1,\n",
    "    \"presence_penalty\": None,\n",
    "    \"eos_token_id\": 2,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "922d07d1-7bc6-457d-b18e-9bb4831e4f02",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from inference_utils import KoLLMSageMakerEndpoint\n",
    "pred = KoLLMSageMakerEndpoint(endpoint_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4c4579f-7ef7-40d5-a943-ae8dc137b8bd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "instruction = \"다음 글을 요약해 주세요.\"\n",
    "context = \"\"\"\n",
    "엔터프라이즈 환경에서 생성 AI와 대규모 언어 모델(LLM; Large Language Models)의 가장 일반적인 유스케이스 중 하나는 기업의 지식 코퍼스를 기반으로 질문에 답변하는 것입니다. Amazon Lex는 AI 기반 챗봇을 구축하기 위한 프레임워크를 제공합니다. 사전 훈련된 파운데이션 모델(FM; Foundation Models)은 다양한 주제에 대한 요약, 텍스트 생성, 질문 답변과 같은 자연어 이해(NLU; Natural Language Understanding) 작업은 잘 수행하지만, 훈련 데이터의 일부로 보지 못한 콘텐츠에 대한 질문에는 정확한(오답 없이) 답변을 제공하는 데 어려움을 겪거나 완전히 실패합니다. 또한 FM은 특정 시점의 데이터 스냅샷으로 훈련하기에 추론 시점에 새로운 데이터에 액세스할 수 있는 고유한 기능이 없기에 잠재적으로 부정확하거나 부적절한 답변을 제공할 수 있습니다.\n",
    "\n",
    "이 문제를 해결하기 위해 흔히 사용되는 접근 방식은 검색 증강 생성(RAG; Retrieval Augmented Generation)이라는 기법을 사용하는 것입니다. RAG 기반 접근 방식에서는 LLM을 사용하여 사용자 질문을 벡터 임베딩으로 변환한 다음, 엔터프라이즈 지식 코퍼스에 대한 임베딩이 미리 채워진 벡터 데이터베이스에서 이러한 임베딩에 대한 유사성 검색을 수행합니다. 소수의 유사한 문서(일반적으로 3개)가 사용자 질문과 함께 다른 LLM에 제공된 ‘프롬프트’에 컨텍스트로 추가되고, 해당 LLM은 프롬프트에 컨텍스트로 제공된 정보를 사용하여 사용자 질문에 대한 답변을 생성합니다. RAG 모델은 매개변수 메모리(parametric memory)는 사전 훈련된 seq2seq 모델이고 비매개변수 메모리(non-parametric memory)는 사전 훈련된 신경망 검색기로 액세스되는 위키백과의 고밀도 벡터 색인 모델로 2020년에 Lewis 등이 도입했습니다. RAG 기반 접근 방식의 전반적 구조를 이해하려면 Question answering using Retrieval Augmented Generation with foundation models in Amazon SageMaker JumpStart 블로그를 참조하기 바랍니다.\n",
    "\"\"\"\n",
    "payload = pred.get_payload(instruction, context, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62f94f9b-9b69-408d-bec5-68a195879b00",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "generated_text = pred.infer(payload, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d6bcdb9-b73d-468f-8606-6bf4b2f90a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint_name_text = endpoint_name\n",
    "%store endpoint_name_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "625410bd-e2e9-4d57-bb35-bddf5cf20301",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## 5. Clean Up\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b5d242e-3b9f-42e4-9ea0-c9596cae540d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44680c92-0623-46a0-9d3a-2efa262f9af6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# - Delete the end point\n",
    "sm_client.delete_endpoint(EndpointName=endpoint_name)\n",
    "# - In case the end point failed we still want to delete the model\n",
    "sm_client.delete_endpoint_config(EndpointConfigName=endpoint_config_name)\n",
    "sm_client.delete_model(ModelName=model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7feeb821-db0a-4a48-8550-b0146705b8d5",
   "metadata": {
    "tags": []
   },
   "source": [
    "<br>\n",
    "\n",
    "# References\n",
    "---\n",
    "\n",
    "- Model 정보\n",
    "    - kullm-polyglot-5.8b-v2\n",
    "        - This model is a parameter-efficient fine-tuned version of EleutherAI/polyglot-ko-5.8b on a KULLM v2\n",
    "        - https://huggingface.co/nlpai-lab/kullm-polyglot-5.8b-v2        \n",
    "    - kullm-polyglot-12.8b-v2\n",
    "        - This model is a fine-tuned version of EleutherAI/polyglot-ko-12.8b on a KULLM v2\n",
    "        - https://huggingface.co/nlpai-lab/kullm-polyglot-12.8b-v2\n",
    "    - beomi/KoAlpaca-Polyglot-12.8B\n",
    "        - This model is a fine-tuned version of EleutherAI/polyglot-ko-12.8b on a KoAlpaca Dataset v1.1b\n",
    "        - https://huggingface.co/beomi/KoAlpaca-Polyglot-12.8B\n",
    "    - EleutherAI/polyglot-ko-12.8b\n",
    "        - Polyglot-Ko-12.8B was trained for 167 billion tokens over 301,000 steps on 256 A100 GPUs with the GPT-NeoX framework. It was trained as an autoregressive language model, using cross-entropy loss to maximize the likelihood of predicting the next token.\n",
    "        - License: Apache 2.0\n",
    "        - https://huggingface.co/EleutherAI/polyglot-ko-12.8b      \n",
    "- 코드\n",
    "    - [Boto3](https://github.com/aws/amazon-sagemaker-examples/blob/main/advanced_functionality/pytorch_deploy_large_GPT_model/GPT-J-6B-model-parallel-inference-DJL.ipynb)\n",
    "    - [Python SDK](https://github.com/aws/amazon-sagemaker-examples/blob/main/inference/generativeai/deepspeed/GPT-J-6B_DJLServing_with_PySDK.ipynb)\n",
    "    - [Kor LLM on SageMaker](https://github.com/gonsoomoon-ml/Kor-LLM-On-SageMaker)\n",
    "    - [AWS Generative AI Workshop for Korean language](https://github.com/aws-samples/aws-ai-ml-workshop-kr/tree/master/genai)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
