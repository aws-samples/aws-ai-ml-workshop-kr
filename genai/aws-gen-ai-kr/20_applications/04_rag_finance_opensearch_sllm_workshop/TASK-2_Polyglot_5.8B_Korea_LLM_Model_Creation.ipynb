{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2662eb8f-ba8b-4c34-8ab9-b312ecfcff91",
   "metadata": {
    "tags": []
   },
   "source": [
    "# TASK-2 Polyglot 5.8B Korea LLM Model Creation: KULLM-Polyglot-5.8B-v2\n",
    "LLM 모델은 이벤트엔진 계정에서 G5.2xlarge 단일 GPU에서 동작하는 KULLM-Polyglot-5.8B-v2 모델을 사용합니다. EleutherAI의 polyglot-ko-5.8b를 기반으로 KULLM v2에서 파인튜닝된 효율적인 모델입니다. 이 모델은 텍스트 생성(Text Generation) 작업에 특화되어 있으며, PyTorch 프레임워크와 Transformers 라이브러리를 사용합니다.\n",
    "* 프레임워크: Transformers 4.28.1, Pytorch 2.0.0+cu117, Datasets 2.11.0, Tokenizers 0.13.3 등의 버전을 사용합니다.\n",
    "* 이 모델은 Hugging Face의 Inference API에서 온디맨드로 로드가능합니다.\n",
    "\n",
    "---\n",
    "\n",
    "## Korean LLM (Large Language Model) Serving on SageMaker with AWS Large Model Container DLC\n",
    "Deep Java Library (DJL) 서빙과 DeepSpeed의 텐서 병렬화 기술을 사용하여 대규모 사전 훈련된 NLP 모델을 여러 GPU에 걸쳐 배포하는 방식을 사용합니다.\n",
    "\n",
    "### Model: [KKULM-Polyglot-5.8B](https://huggingface.co/nlpai-lab/kullm-polyglot-5.8b-v2)\n",
    "한국어 LLM 모델 SageMaker 서빙 핸즈온 (허깅페이스 허브에서 모델을 그대로 배포)\n",
    "\n",
    "- LLM GitHub: https://github.com/nlpai-lab/KULLM\n",
    "- [AWS Blog: Deploy large models on Amazon SageMaker using DJLServing and DeepSpeed model parallel inference](https://aws.amazon.com/ko/blogs/machine-learning/deploy-large-models-on-amazon-sagemaker-using-djlserving-and-deepspeed-model-parallel-inference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5a056d19-3339-4778-b73d-f5fe14d50ae0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import sys\n",
    "sys.path.append('./utils')\n",
    "sys.path.append('./templates')\n",
    "sys.path.append('./common_code')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "45ca4169-bc8f-4eeb-b897-b48b15f1c6eb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/ec2-user/.config/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/ec2-user/.config/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/ec2-user/.config/sagemaker/config.yaml\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sagemaker, boto3, jinja2\n",
    "role = sagemaker.get_execution_role()  # execution role for the endpoint\n",
    "sess = sagemaker.session.Session()  # sagemaker session for interacting with different AWS APIs\n",
    "bucket = sess.default_bucket()  # bucket to house artifacts\n",
    "model_bucket = sess.default_bucket()  # bucket to house artifacts\n",
    "\n",
    "region = sess._region_name  # region name of the current SageMaker Studio environment\n",
    "account_id = sess.account_id()  # account_id of the current SageMaker Studio environment\n",
    "\n",
    "s3_client = boto3.client(\"s3\")  # client to intreract with S3 API\n",
    "sm_client = boto3.client(\"sagemaker\")  # client to intreract with SageMaker\n",
    "smr_client = boto3.client(\"sagemaker-runtime\")  # client to intreract with SageMaker Endpoints\n",
    "jinja_env = jinja2.Environment()  # jinja environment to generate model configuration templates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85236aea-d0e6-4b5f-89bf-2d185881740c",
   "metadata": {
    "tags": []
   },
   "source": [
    "<br>\n",
    "\n",
    "## 1. Download LLM model and upload it to S3\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8a104756-a389-403c-bb3a-b512ed6b948d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import snapshot_download\n",
    "from pathlib import Path\n",
    "\n",
    "model_id = \"nlpai-lab/kullm-polyglot-5.8b-v2\"\n",
    "model_prefix = model_id.split('/')[-1].replace('.', '-')\n",
    "\n",
    "s3_code_prefix = f\"ko-llm/{model_prefix}/code\"  # folder within bucket where code artifact will go\n",
    "s3_model_prefix = f\"ko-llm/{model_prefix}/model\"  # folder where model checkpoint will go"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99fca204-ff77-49c5-bc76-b5383c8ded4e",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## 2. Model Serving Scripts\n",
    "---\n",
    "### Create `serving.properties`\n",
    "\n",
    "이 설정 파일은 어떤 추론 최적화 라이브러리를 사용할지, 어떤 설정을 사용할지 DJL Serving에 알려주는 설정 파일입니다. 필요에 따라 적절한 구성을 설정할 수 있습니다.\n",
    "\n",
    "모델이 레이어에 따라 분할되는 파이프라인 병렬화(Pipeline Parallelism)를 사용하는 허깅페이스 Accelerate와 달리, DeepSpeed는 각 레이어(텐서)가 여러 디바이스에 걸쳐 샤딩되는 텐서 병렬화(Tensor Parallelism)를 사용합니다. 파이프라인 병렬 처리 접근 방식에서는 데이터가 각 GPU 장치를 통해 순차적으로 흐르지만, 텐서 병렬 처리는 데이터가 모든 GPU 장치로 전송되어 각 GPU에서 부분적인 결과가 계산됩니다. 그런 다음 All-Gather 연산을 통해 부분 결과를 수집하여 최종 결과를 계산합니다. 따라서, 텐서 병렬화가 일반적으로 더 높은 GPU 활용률과 더 나은 성능을 제공합니다.\n",
    "\n",
    "- `option.s3url` - 모델 파일의 위치를 지정합니다. 또는`option.model_id` 옵션을 대신 사용하여 허깅페이스 허브에서 모델을 지정할 수 있습니다(예: EleutherAI/gpt-j-6B). 그러면 허브에서 모델이 자동으로 다운로드됩니다. s3url 접근 방식은 자체 환경 내에서 모델 아티팩트를 호스팅할 수 있고 DJL 추론 컨테이너 내에서 최적화된 접근 방식을 활용하여 S3에서 호스팅 인스턴스로 모델을 전송함으로써 더 빠른 모델 배포가 가능합니다.\n",
    "\n",
    "`serving.properties`의 일반적인 설정법과 자세한 내용은 https://docs.aws.amazon.com/sagemaker/latest/dg/large-model-inference-configuration.html 를 참조하세요.\n",
    "\n",
    "<img src=\"./images/TensorShard.png\" width=\"800\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7e960108-f6bf-4b8d-9ea6-78b6ca3c6915",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "src_path = f\"src/{model_prefix}\"\n",
    "!rm -rf {src_path}\n",
    "os.makedirs(src_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5beab7e1-205c-47b1-b27a-0120614caec2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing src/kullm-polyglot-5-8b-v2/serving.properties\n"
     ]
    }
   ],
   "source": [
    "%%writefile {src_path}/serving.properties\n",
    "\n",
    "engine=DeepSpeed\n",
    "\n",
    "# passing extra options to model.py or built-in handler\n",
    "job_queue_size=100\n",
    "batch_size=1\n",
    "max_batch_delay=1\n",
    "max_idle_time=60\n",
    "\n",
    "# Built-in entrypoint\n",
    "#option.entryPoint=djl_python.deepspeed\n",
    "\n",
    "# Hugging Face model id\n",
    "option.model_id={{model_id}}\n",
    "\n",
    "# defines custom environment variables\n",
    "#env=SERVING_NUMBER_OF_NETTY_THREADS=2\n",
    "\n",
    "# Allows to load DeepSpeed workers in parallel\n",
    "option.parallel_loading=true\n",
    "\n",
    "# specify tensor parallel degree (number of partitions)\n",
    "option.tensor_parallel_degree=1\n",
    "\n",
    "# specify per model timeout\n",
    "option.model_loading_timeout=600\n",
    "#option.predict_timeout=240\n",
    "\n",
    "# mark the model as failure after python process crashing 10 times\n",
    "retry_threshold=0\n",
    "\n",
    "option.task=text-generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb8cacee-600c-460b-a0b1-7c263a26b5c7",
   "metadata": {},
   "source": [
    "### serving.properties의 S3 경로 수정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "34f03823-aa2a-48e8-b862-116917c1cc4a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     1\t\u001B[36mengine\u001B[39;49;00m=\u001B[33mDeepSpeed\u001B[39;49;00m\u001B[37m\u001B[39;49;00m\n",
      "     2\t\u001B[37m\u001B[39;49;00m\n",
      "     3\t\u001B[37m# passing extra options to model.py or built-in handler\u001B[39;49;00m\u001B[37m\u001B[39;49;00m\n",
      "     4\t\u001B[36mjob_queue_size\u001B[39;49;00m=\u001B[33m100\u001B[39;49;00m\u001B[37m\u001B[39;49;00m\n",
      "     5\t\u001B[36mbatch_size\u001B[39;49;00m=\u001B[33m1\u001B[39;49;00m\u001B[37m\u001B[39;49;00m\n",
      "     6\t\u001B[36mmax_batch_delay\u001B[39;49;00m=\u001B[33m1\u001B[39;49;00m\u001B[37m\u001B[39;49;00m\n",
      "     7\t\u001B[36mmax_idle_time\u001B[39;49;00m=\u001B[33m60\u001B[39;49;00m\u001B[37m\u001B[39;49;00m\n",
      "     8\t\u001B[37m\u001B[39;49;00m\n",
      "     9\t\u001B[37m# Built-in entrypoint\u001B[39;49;00m\u001B[37m\u001B[39;49;00m\n",
      "    10\t\u001B[37m#option.entryPoint=djl_python.deepspeed\u001B[39;49;00m\u001B[37m\u001B[39;49;00m\n",
      "    11\t\u001B[37m\u001B[39;49;00m\n",
      "    12\t\u001B[37m# Hugging Face model id\u001B[39;49;00m\u001B[37m\u001B[39;49;00m\n",
      "    13\t\u001B[36moption.model_id\u001B[39;49;00m=\u001B[33mnlpai-lab/kullm-polyglot-5.8b-v2\u001B[39;49;00m\u001B[37m\u001B[39;49;00m\n",
      "    14\t\u001B[37m\u001B[39;49;00m\n",
      "    15\t\u001B[37m# defines custom environment variables\u001B[39;49;00m\u001B[37m\u001B[39;49;00m\n",
      "    16\t\u001B[37m#env=SERVING_NUMBER_OF_NETTY_THREADS=2\u001B[39;49;00m\u001B[37m\u001B[39;49;00m\n",
      "    17\t\u001B[37m\u001B[39;49;00m\n",
      "    18\t\u001B[37m# Allows to load DeepSpeed workers in parallel\u001B[39;49;00m\u001B[37m\u001B[39;49;00m\n",
      "    19\t\u001B[36moption.parallel_loading\u001B[39;49;00m=\u001B[33mtrue\u001B[39;49;00m\u001B[37m\u001B[39;49;00m\n",
      "    20\t\u001B[37m\u001B[39;49;00m\n",
      "    21\t\u001B[37m# specify tensor parallel degree (number of partitions)\u001B[39;49;00m\u001B[37m\u001B[39;49;00m\n",
      "    22\t\u001B[36moption.tensor_parallel_degree\u001B[39;49;00m=\u001B[33m1\u001B[39;49;00m\u001B[37m\u001B[39;49;00m\n",
      "    23\t\u001B[37m\u001B[39;49;00m\n",
      "    24\t\u001B[37m# specify per model timeout\u001B[39;49;00m\u001B[37m\u001B[39;49;00m\n",
      "    25\t\u001B[36moption.model_loading_timeout\u001B[39;49;00m=\u001B[33m600\u001B[39;49;00m\u001B[37m\u001B[39;49;00m\n",
      "    26\t\u001B[37m#option.predict_timeout=240\u001B[39;49;00m\u001B[37m\u001B[39;49;00m\n",
      "    27\t\u001B[37m\u001B[39;49;00m\n",
      "    28\t\u001B[37m# mark the model as failure after python process crashing 10 times\u001B[39;49;00m\u001B[37m\u001B[39;49;00m\n",
      "    29\t\u001B[36mretry_threshold\u001B[39;49;00m=\u001B[33m0\u001B[39;49;00m\u001B[37m\u001B[39;49;00m\n",
      "    30\t\u001B[37m\u001B[39;49;00m\n",
      "    31\t\u001B[36moption.task\u001B[39;49;00m=\u001B[33mtext-generation\u001B[39;49;00m\u001B[37m\u001B[39;49;00m\n"
     ]
    }
   ],
   "source": [
    "# we plug in the appropriate model location into our `serving.properties` file based on the region in which this notebook is running\n",
    "template = jinja_env.from_string(Path(f\"{src_path}/serving.properties\").open().read())\n",
    "Path(f\"{src_path}/serving.properties\").open(\"w\").write(template.render(model_id=model_id))\n",
    "!pygmentize {src_path}/serving.properties | cat -n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76217c91-0a17-4895-a400-e8bd9889f00d",
   "metadata": {},
   "source": [
    "### Create model.py with custom inference code\n",
    "빌트인 추론 코드로 no-code로 배포할 수도 있지만, 커스텀 추론 코드를 작성하는 것도 가능합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ff9c01fd-d6cc-4dfc-9dfe-8a63ebd8cf60",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing src/kullm-polyglot-5-8b-v2/model.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {src_path}/model.py\n",
    "from djl_python import Input, Output\n",
    "import os\n",
    "import deepspeed\n",
    "import torch\n",
    "import logging\n",
    "from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer\n",
    "from transformers import GPTNeoXLayer\n",
    "\n",
    "predictor = None\n",
    "\n",
    "def get_model(properties):\n",
    "    \n",
    "    tp_degree = properties[\"tensor_parallel_degree\"]\n",
    "    model_location = properties[\"model_dir\"]\n",
    "    if \"model_id\" in properties:\n",
    "        model_location = properties[\"model_id\"]\n",
    "    task = properties[\"task\"]\n",
    "    \n",
    "    logging.info(f\"Loading model in {model_location}\")    \n",
    "    local_rank = int(os.getenv(\"LOCAL_RANK\", \"0\"))\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_location)\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_location,\n",
    "        torch_dtype=torch.float16,\n",
    "        low_cpu_mem_usage=True,\n",
    "    )\n",
    "    \n",
    "    model.requires_grad_(False)\n",
    "    model.eval()\n",
    "    \n",
    "    ds_config = {\n",
    "        \"tensor_parallel\": {\"tp_size\": tp_degree},\n",
    "        \"dtype\": model.dtype,\n",
    "        \"injection_policy\": {\n",
    "            GPTNeoXLayer:('attention.dense', 'mlp.dense_4h_to_h')\n",
    "        }\n",
    "    }\n",
    "    logging.info(f\"Starting DeepSpeed init with TP={tp_degree}\")        \n",
    "    model = deepspeed.init_inference(model, ds_config)  \n",
    "    \n",
    "    generator = pipeline(\n",
    "        task=task, model=model, tokenizer=tokenizer, device=local_rank\n",
    "    )\n",
    "    # https://huggingface.co/docs/hub/models-tasks\n",
    "    return generator\n",
    "    \n",
    "def handle(inputs: Input) -> None:\n",
    "    \"\"\"\n",
    "    inputs: Contains the configurations from serving.properties\n",
    "    \"\"\"    \n",
    "    global predictor\n",
    "    if not predictor:\n",
    "        predictor = get_model(inputs.get_properties())\n",
    "\n",
    "    if inputs.is_empty():\n",
    "        # Model server makes an empty call to warmup the model on startup\n",
    "        logging.info(\"is_empty\")\n",
    "        return None\n",
    "\n",
    "    data = inputs.get_as_json() #inputs.get_as_string()\n",
    "    logging.info(\"data:\", data)\n",
    "    \n",
    "    input_prompt, params = data[\"inputs\"], data[\"parameters\"]\n",
    "    result = predictor(input_prompt, **params)\n",
    "    logging.info(\"result:\", result)\n",
    "\n",
    "    return Output().add_as_json(result) #Output().add(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a063b634-3a4f-49ec-8779-fdda2818326e",
   "metadata": {},
   "source": [
    "### Create the Tarball and then upload to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "90fa1063-839a-46be-9ca4-f6b9c14efbc7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./\n",
      "./serving.properties\n",
      "./model.py\n"
     ]
    }
   ],
   "source": [
    "!rm -rf model.tar.gz\n",
    "!tar czvf model.tar.gz -C {src_path} ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1d60fd8a-8a0e-497c-8628-98eb26772cd9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S3 Code or Model tar ball uploaded to --- > s3://sagemaker-us-east-1-143656149352/ko-llm/kullm-polyglot-5-8b-v2/code/model.tar.gz\n"
     ]
    }
   ],
   "source": [
    "s3_code_artifact = sess.upload_data(\"model.tar.gz\", bucket, s3_code_prefix)\n",
    "print(f\"S3 Code or Model tar ball uploaded to --- > {s3_code_artifact}\")\n",
    "!rm -rf model.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57ea2358-3ba1-4ff3-9ca4-df772b59770d",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## 3. Serve LLM Model on SageMaker\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7820b1c3-7854-433d-bbb6-03193abefa22",
   "metadata": {},
   "source": [
    "### Create SageMaker Model\n",
    "\n",
    "SageMaker 엔드포인트 생성 매개변수 VolumeSizeInGB를 지정할 때 마운트되는 Amazon EBS(Amazon Elastic Block Store) 볼륨에 /tmp를 매핑하기 때문에 컨테이너는 인스턴스의 `/tmp` 공간에 모델을 다운로드합니다. 이때 s5cmd (https://github.com/peak/s5cmd) 를 활용하므로 대용량 모델을 빠르게 다운로드할 수 있습니다.\n",
    "볼륨 인스턴스와 함께 미리 빌드되어 제공되는 p4dn과 같은 인스턴스의 경우 컨테이너의 `/tmp`를 계속 활용할 수 있습니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "598b6ded-ba9c-4f25-b862-090546607b98",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kullm-polyglot-5-8b-v2-2023-09-19-01-48-58-468\n",
      "Created Model: arn:aws:sagemaker:us-east-1:143656149352:model/kullm-polyglot-5-8b-v2-2023-09-19-01-48-58-468\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.utils import name_from_base\n",
    "from sagemaker import image_uris\n",
    "\n",
    "img_uri = image_uris.retrieve(framework=\"djl-deepspeed\", region=region, version=\"0.23.0\")\n",
    "model_name = name_from_base(f\"{model_prefix}\")\n",
    "print(model_name)\n",
    "\n",
    "model_response = sm_client.create_model(\n",
    "    ModelName=model_name,\n",
    "    ExecutionRoleArn=role,\n",
    "    PrimaryContainer={\"Image\": img_uri, \"ModelDataUrl\": s3_code_artifact},\n",
    ")\n",
    "model_arn = model_response[\"ModelArn\"]\n",
    "print(f\"Created Model: {model_arn}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a96783b7-9e6a-4bed-8ff9-c779d9e628e4",
   "metadata": {},
   "source": [
    "### Create SageMaker 한국어 LLM Endpoint \n",
    "* G5.2xlarge GPU 1대를 사용합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0d2f670c-57f4-4092-af29-b1416829e9dd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created Endpoint: arn:aws:sagemaker:us-east-1:143656149352:endpoint/kullm-polyglot-5-8b-v2-2023-09-19-01-48-58-468-endpoint\n"
     ]
    }
   ],
   "source": [
    "endpoint_config_name = f\"{model_name}-config\"\n",
    "endpoint_name = f\"{model_name}-endpoint\"\n",
    "variant_name = \"variant1\"\n",
    "instance_type = \"ml.g5.2xlarge\"\n",
    "initial_instance_count = 1\n",
    "\n",
    "prod_variants = [\n",
    "    {\n",
    "        \"VariantName\": variant_name,\n",
    "        \"ModelName\": model_name,\n",
    "        \"InstanceType\": instance_type,\n",
    "        \"InitialInstanceCount\": initial_instance_count,\n",
    "        # \"ModelDataDownloadTimeoutInSeconds\": 2400,\n",
    "        \"ContainerStartupHealthCheckTimeoutInSeconds\": 1500, # at least 12min\n",
    "    }\n",
    "]\n",
    "\n",
    "endpoint_config_response = sm_client.create_endpoint_config(\n",
    "    EndpointConfigName=endpoint_config_name,\n",
    "    ProductionVariants=prod_variants\n",
    ")\n",
    "\n",
    "endpoint_response = sm_client.create_endpoint(\n",
    "    EndpointName=endpoint_name, EndpointConfigName=endpoint_config_name\n",
    ")\n",
    "print(f\"Created Endpoint: {endpoint_response['EndpointArn']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bf78f45-b06e-431c-9048-3ade776cac07",
   "metadata": {},
   "source": [
    "엔드포인트가 생성되는 동안 아래의 문서를 같이 확인해 보세요.\n",
    "- https://docs.aws.amazon.com/sagemaker/latest/dg/large-model-inference-dlc.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6122a3f8-78b6-42b9-b390-af8942d8e30c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/ec2-user/.config/sagemaker/config.yaml\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<b> [SageMaker LLM Serving] <a target=\"blank\" href=\"https://console.aws.amazon.com/sagemaker/home?region=us-east-1#/endpoints/kullm-polyglot-5-8b-v2-2023-09-19-01-48-58-468-endpoint\">Check Endpoint Status</a></b>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "import time\n",
    "def make_console_link(region, endpoint_name, task='[SageMaker LLM Serving]'):\n",
    "    endpoint_link = f'<b> {task} <a target=\"blank\" href=\"https://console.aws.amazon.com/sagemaker/home?region={region}#/endpoints/{endpoint_name}\">Check Endpoint Status</a></b>'   \n",
    "    return endpoint_link\n",
    "\n",
    "sess = sagemaker.session.Session()  # sagemaker session for interacting with different AWS APIs\n",
    "region = sess._region_name  # region name of the current SageMaker Studio environment\n",
    "\n",
    "endpoint_link = make_console_link(region, endpoint_name)\n",
    "display(HTML(endpoint_link))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9d43b855-4d79-4460-aecc-6128356214da",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Endpoint is  Creating\n",
      "Endpoint is  Creating\n",
      "Endpoint is  Creating\n",
      "Endpoint is  Creating\n",
      "Endpoint is  Creating\n",
      "Endpoint is  Creating\n",
      "Endpoint is  Creating\n",
      "Endpoint is  Creating\n",
      "Endpoint is  Creating\n",
      "Endpoint is  Creating\n",
      "Endpoint is  Creating\n",
      "Endpoint is  InService\n",
      "CPU times: user 223 ms, sys: 12.4 ms, total: 235 ms\n",
      "Wall time: 11min 2s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "from inference_lib import describe_endpoint\n",
    "describe_endpoint(endpoint_name)         "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79b26c2e-b620-4df9-b712-c5aeb8e9e32a",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## 4. Inference\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd435259-7952-4a76-b4ee-3360da5dd7c5",
   "metadata": {},
   "source": [
    "엔드포인트를 호출할 때 이 텍스트를 JSON 페이로드 내에 제공해야 합니다. 이 JSON 페이로드에는 length, sampling strategy, output token sequence restrictions을 제어하는 데 도움이 되는 원하는 추론 매개변수가 포함될 수 있습니다. 허깅페이스 트랜스포머 transformers 라이브러리에는 [사용 가능한 페이로드 매개변수](https://huggingface.co/docs/transformers/main_classes/text_generation#transformers.GenerationConfig)의 전체 목록이 정의되어 있지만, 중요한 페이로드 매개변수는 다음과 같이 정의되어 있습니다:\n",
    "\n",
    "* **do_sample (`bool`)** – logits sampling 활성화\n",
    "* **max_new_tokens (`int`)** – 생성된 토큰의 최대 수\n",
    "* **best_of (`int`)** – best_of 개의 시퀀스를 생성하고 가장 높은 토큰 로그 확률이 있는 경우 반환\n",
    "* **repetition_penalty (`float`)** – 반복 패널티에 대한 파라미터, 1.0은 패널티가 없음을 의미하여 Greedy 서치와 동일, 커질수록 다양한 결과를 얻을 수 있으며, 자세한 사항은 [this paper](https://arxiv.org/pdf/1909.05858.pdf)을 참고\n",
    "* **return_full_text (`bool`)** – 생성된 텍스트에 프롬프트를 추가할지 여부\n",
    "* **seed (`int`)** – Random sampling seed\n",
    "* **stop_sequences (`List[str]`)** – `stop_sequences` 가 생성되면 토큰 생성을 중지\n",
    "* **temperature (`float`)** – logits 분포 모듈화에 사용되는 값\n",
    "* **top_k (`int`)** – 상위 K개 만큼 가장 높은 확률 어휘 토큰의 수\n",
    "* **top_p (`float`)** – 1 보다 작게 설정하게 되며, 상위부터 정렬했을 때 가능한 토큰들의 확률을 합산하여 `top_p` 이상의 가장 작은 집합을 유지\n",
    "* **truncate (`int`)** – 입력 토큰을 지정된 크기로 잘라냄\n",
    "* **typical_p (`float`)** – typical Decoding 양으로, 자세한 사항은 [Typical Decoding for Natural Language Generation](https://arxiv.org/abs/2202.00666)을 참고\n",
    "* **watermark (`bool`)** –  [A Watermark for Large Language Models](https://arxiv.org/abs/2301.10226)가 Watermarking\n",
    "* **decoder_input_details (`bool`)** – decoder input token logprobs와 ids를 반환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "12185205-fc49-4716-aacd-c1017a8541a0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"do_sample\": False,\n",
    "    \"max_new_tokens\": 256,\n",
    "    \"temperature\": 0.4,\n",
    "    \"top_p\": 0.9,\n",
    "    \"return_full_text\": False,\n",
    "    \"repetition_penalty\": 1.1,\n",
    "    \"presence_penalty\": None,\n",
    "    \"eos_token_id\": 2,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "922d07d1-7bc6-457d-b18e-9bb4831e4f02",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from inference_utils import KoLLMSageMakerEndpoint\n",
    "pred = KoLLMSageMakerEndpoint(endpoint_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "43fc367e-91a0-4c93-b570-83389d59275a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "instruction = \"다음 글을 요약해 주세요.\"\n",
    "context = \"\"\"\n",
    "엔터프라이즈 환경에서 생성 AI와 대규모 언어 모델(LLM; Large Language Models)의 가장 일반적인 유스케이스 중 하나는 기업의 지식 코퍼스를 기반으로 질문에 답변하는 것입니다. Amazon Lex는 AI 기반 챗봇을 구축하기 위한 프레임워크를 제공합니다. 사전 훈련된 파운데이션 모델(FM; Foundation Models)은 다양한 주제에 대한 요약, 텍스트 생성, 질문 답변과 같은 자연어 이해(NLU; Natural Language Understanding) 작업은 잘 수행하지만, 훈련 데이터의 일부로 보지 못한 콘텐츠에 대한 질문에는 정확한(오답 없이) 답변을 제공하는 데 어려움을 겪거나 완전히 실패합니다. 또한 FM은 특정 시점의 데이터 스냅샷으로 훈련하기에 추론 시점에 새로운 데이터에 액세스할 수 있는 고유한 기능이 없기에 잠재적으로 부정확하거나 부적절한 답변을 제공할 수 있습니다.\n",
    "\n",
    "이 문제를 해결하기 위해 흔히 사용되는 접근 방식은 검색 증강 생성(RAG; Retrieval Augmented Generation)이라는 기법을 사용하는 것입니다. RAG 기반 접근 방식에서는 LLM을 사용하여 사용자 질문을 벡터 임베딩으로 변환한 다음, 엔터프라이즈 지식 코퍼스에 대한 임베딩이 미리 채워진 벡터 데이터베이스에서 이러한 임베딩에 대한 유사성 검색을 수행합니다. 소수의 유사한 문서(일반적으로 3개)가 사용자 질문과 함께 다른 LLM에 제공된 ‘프롬프트’에 컨텍스트로 추가되고, 해당 LLM은 프롬프트에 컨텍스트로 제공된 정보를 사용하여 사용자 질문에 대한 답변을 생성합니다. RAG 모델은 매개변수 메모리(parametric memory)는 사전 훈련된 seq2seq 모델이고 비매개변수 메모리(non-parametric memory)는 사전 훈련된 신경망 검색기로 액세스되는 위키백과의 고밀도 벡터 색인 모델로 2020년에 Lewis 등이 도입했습니다. RAG 기반 접근 방식의 전반적 구조를 이해하려면 Question answering using Retrieval Augmented Generation with foundation models in Amazon SageMaker JumpStart 블로그를 참조하기 바랍니다.\n",
    "\"\"\"\n",
    "payload = pred.get_payload(instruction, context, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3e953cb0-e019-43c7-bb49-44a91cad2c9e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'inputs': '아래는 작업을 설명하는 명령어와 추가 컨텍스트를 제공하는 입력이 짝을 이루는 예제입니다. 요청을 적절히 완료하는 응답을 작성하세요.\\n\\n### 명령어:\\n다음 글을 요약해 주세요.\\n\\n### 입력:\\n\\n엔터프라이즈 환경에서 생성 AI와 대규모 언어 모델(LLM; Large Language Models)의 가장 일반적인 유스케이스 중 하나는 기업의 지식 코퍼스를 기반으로 질문에 답변하는 것입니다. Amazon Lex는 AI 기반 챗봇을 구축하기 위한 프레임워크를 제공합니다. 사전 훈련된 파운데이션 모델(FM; Foundation Models)은 다양한 주제에 대한 요약, 텍스트 생성, 질문 답변과 같은 자연어 이해(NLU; Natural Language Understanding) 작업은 잘 수행하지만, 훈련 데이터의 일부로 보지 못한 콘텐츠에 대한 질문에는 정확한(오답 없이) 답변을 제공하는 데 어려움을 겪거나 완전히 실패합니다. 또한 FM은 특정 시점의 데이터 스냅샷으로 훈련하기에 추론 시점에 새로운 데이터에 액세스할 수 있는 고유한 기능이 없기에 잠재적으로 부정확하거나 부적절한 답변을 제공할 수 있습니다.\\n\\n이 문제를 해결하기 위해 흔히 사용되는 접근 방식은 검색 증강 생성(RAG; Retrieval Augmented Generation)이라는 기법을 사용하는 것입니다. RAG 기반 접근 방식에서는 LLM을 사용하여 사용자 질문을 벡터 임베딩으로 변환한 다음, 엔터프라이즈 지식 코퍼스에 대한 임베딩이 미리 채워진 벡터 데이터베이스에서 이러한 임베딩에 대한 유사성 검색을 수행합니다. 소수의 유사한 문서(일반적으로 3개)가 사용자 질문과 함께 다른 LLM에 제공된 ‘프롬프트’에 컨텍스트로 추가되고, 해당 LLM은 프롬프트에 컨텍스트로 제공된 정보를 사용하여 사용자 질문에 대한 답변을 생성합니다. RAG 모델은 매개변수 메모리(parametric memory)는 사전 훈련된 seq2seq 모델이고 비매개변수 메모리(non-parametric memory)는 사전 훈련된 신경망 검색기로 액세스되는 위키백과의 고밀도 벡터 색인 모델로 2020년에 Lewis 등이 도입했습니다. RAG 기반 접근 방식의 전반적 구조를 이해하려면 Question answering using Retrieval Augmented Generation with foundation models in Amazon SageMaker JumpStart 블로그를 참조하기 바랍니다.\\n\\n\\n### 응답:\\n',\n",
       " 'parameters': {'do_sample': False,\n",
       "  'max_new_tokens': 256,\n",
       "  'temperature': 0.4,\n",
       "  'top_p': 0.9,\n",
       "  'return_full_text': False,\n",
       "  'repetition_penalty': 1.1,\n",
       "  'presence_penalty': None,\n",
       "  'eos_token_id': 2}}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "payload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8c7c0f23-9b8c-45de-99bc-2ef798377d34",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Response: 이 글에서는 기업의 지식 코퍼스를 기반으로 질문에 답변하는 것과 관련된 문제를 해결하기 위한 방법으로 검색 증강 '\n",
      " '생성(RAG)이라는 기법을 소개하고 있습니다. 이 기법은 사전 훈련된 모델을 사용하여 사용자 질문을 벡터 임베딩으로 변환한 다음, 사전 '\n",
      " '훈련된 모델을 사용하여 사용자 질문에 대한 답변을 생성합니다. 이 접근 방식은 매개변수 메모리는 사전 훈련된 모델이고 비매개변수 메모리는 '\n",
      " '사전 훈련된 신경망 검색기로 액세스되는 위키백과의 고밀도 벡터 색인 모델을 사용합니다.')\n",
      "CPU times: user 17.9 ms, sys: 203 µs, total: 18.2 ms\n",
      "Wall time: 7.73 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "generated_text = pred.infer(payload, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1f6daf3-083f-4786-b5de-e8a1514b2399",
   "metadata": {},
   "source": [
    "### %store는 주피터 노트북에서 사용되는 매직 커맨드입니다. 이 커맨드를 사용하면 한 노트북에서 정의한 파이썬 변수를 IPython 데이터베이스에 저장하여 다른 주피터 노트북에서도 접근할 수 있게 됩니다. %store endpoint_name을 사용하면 endpoint_name 변수의 값을 저장하여 다른 노트북에서도 사용할 수 있습니다.\n",
    "* 다른 노트북에서 이 저장된 변수를 다음과 같이 불러올 수 있습니다:\n",
    "\n",
    "```%store -r endpoint_name```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6d6bcdb9-b73d-468f-8606-6bf4b2f90a56",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'endpoint_name_text' (str)\n"
     ]
    }
   ],
   "source": [
    "endpoint_name_text = endpoint_name\n",
    "%store endpoint_name_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "625410bd-e2e9-4d57-bb35-bddf5cf20301",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## 5. Clean Up\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d90b2056-5575-4a42-9dc9-0a986c2372db",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "44680c92-0623-46a0-9d3a-2efa262f9af6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ResponseMetadata': {'RequestId': 'f27b5830-17d4-49c8-a311-cbd4a972e455',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': 'f27b5830-17d4-49c8-a311-cbd4a972e455',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '0',\n",
       "   'date': 'Wed, 23 Aug 2023 15:46:59 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# - Delete the end point\n",
    "sm_client.delete_endpoint(EndpointName=endpoint_name)\n",
    "# - In case the end point failed we still want to delete the model\n",
    "sm_client.delete_endpoint_config(EndpointConfigName=endpoint_config_name)\n",
    "sm_client.delete_model(ModelName=model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7feeb821-db0a-4a48-8550-b0146705b8d5",
   "metadata": {
    "tags": []
   },
   "source": [
    "<br>\n",
    "\n",
    "# References\n",
    "---\n",
    "\n",
    "- Model 정보\n",
    "    - kullm-polyglot-5.8b-v2\n",
    "        - This model is a parameter-efficient fine-tuned version of EleutherAI/polyglot-ko-5.8b on a KULLM v2\n",
    "        - https://huggingface.co/nlpai-lab/kullm-polyglot-5.8b-v2        \n",
    "    - kullm-polyglot-12.8b-v2\n",
    "        - This model is a fine-tuned version of EleutherAI/polyglot-ko-12.8b on a KULLM v2\n",
    "        - https://huggingface.co/nlpai-lab/kullm-polyglot-12.8b-v2\n",
    "    - beomi/KoAlpaca-Polyglot-12.8B\n",
    "        - This model is a fine-tuned version of EleutherAI/polyglot-ko-12.8b on a KoAlpaca Dataset v1.1b\n",
    "        - https://huggingface.co/beomi/KoAlpaca-Polyglot-12.8B\n",
    "    - EleutherAI/polyglot-ko-12.8b\n",
    "        - Polyglot-Ko-12.8B was trained for 167 billion tokens over 301,000 steps on 256 A100 GPUs with the GPT-NeoX framework. It was trained as an autoregressive language model, using cross-entropy loss to maximize the likelihood of predicting the next token.\n",
    "        - License: Apache 2.0\n",
    "        - https://huggingface.co/EleutherAI/polyglot-ko-12.8b      \n",
    "- 코드\n",
    "    - [Boto3](https://github.com/aws/amazon-sagemaker-examples/blob/main/advanced_functionality/pytorch_deploy_large_GPT_model/GPT-J-6B-model-parallel-inference-DJL.ipynb)\n",
    "    - [Python SDK](https://github.com/aws/amazon-sagemaker-examples/blob/main/inference/generativeai/deepspeed/GPT-J-6B_DJLServing_with_PySDK.ipynb)\n",
    "    - [Kor LLM on SageMaker](https://github.com/gonsoomoon-ml/Kor-LLM-On-SageMaker)\n",
    "    - [AWS Generative AI Workshop for Korean language](https://github.com/aws-samples/aws-ai-ml-workshop-kr/tree/master/genai)"
   ]
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   }
  ],
  "instance_type": "ml.m5.large",
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
