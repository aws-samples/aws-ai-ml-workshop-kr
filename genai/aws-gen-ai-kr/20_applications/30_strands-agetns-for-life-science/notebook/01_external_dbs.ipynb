{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MCPë¥¼ í†µí•œ External Database í™œìš©\n",
    "\n",
    "ì´ ë…¸íŠ¸ë¶ì—ì„œëŠ” Strands Agentsì™€ MCP (Model Context Protocol)ë¥¼ ì‚¬ìš©í•˜ì—¬ ì™¸ë¶€ ë°ì´í„°ë² ì´ìŠ¤ì¸ Arxiv, ChEMBL, PubMed, ClinicalTrials.govë¥¼ ì—°ë™í•˜ëŠ” ë°©ë²•ì„ ì‹¤ìŠµí•©ë‹ˆë‹¤.\n",
    "\n",
    "## í•™ìŠµ ëª©í‘œ\n",
    "- MCPë¥¼ í†µí•´ ì™¸ë¶€ ë°ì´í„°ë² ì´ìŠ¤ë¥¼ Agentì˜ ë„êµ¬ë¡œ í™œìš©í•˜ëŠ” ë°©ë²• ì´í•´\n",
    "- Agent-as-tool íŒ¨í„´ì„ ì‚¬ìš©í•œ ë‹¤ì¤‘ MCP ì„œë²„ í†µí•©\n",
    "- ì‹¤ì œ ì—°êµ¬ ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ ìƒì„± ì‹¤ìŠµ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. í™˜ê²½ ì„¤ì •\n",
    "\n",
    "í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ì™€ ì˜ì¡´ì„±ì„ ì„¤ì¹˜í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# í•„ìš”í•œ íŒ¨í‚¤ì§€ ì„¤ì¹˜\n",
    "%pip install strands-agents strands-agents-tools mcp boto3 arxiv chembl-webresource-client python-dateutil pubmedmcp pytrials --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸\n",
    "import sys\n",
    "import logging\n",
    "import asyncio\n",
    "from typing import List, Dict, Any\n",
    "from datetime import datetime\n",
    "\n",
    "# AWS SDK\n",
    "import boto3\n",
    "from botocore.config import Config\n",
    "\n",
    "# Strands Agents\n",
    "from strands import Agent, tool\n",
    "from strands.models import BedrockModel\n",
    "from strands.tools.mcp import MCPClient\n",
    "\n",
    "# MCP\n",
    "from mcp import stdio_client, StdioServerParameters\n",
    "\n",
    "# ë¡œê¹… ì„¤ì •\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(\"external_dbs_demo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. MCP ì„œë²„ êµ¬ì„±\n",
    "\n",
    "ê° ì™¸ë¶€ ë°ì´í„°ë² ì´ìŠ¤ì— ëŒ€í•œ MCP ì„œë²„ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.\n",
    "ì´ ë„¤ ê°€ì§€ì˜ MCP ì„œë²„ë¥¼ êµ¬ì„±í•˜ë©°, ê° ì„œë²„ì˜ ì—­í• ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.\n",
    "\n",
    "### 1) ArXiv MCP ì„œë²„ (mcp_server_arxiv.py)\n",
    "ArXivì—ì„œ í•™ìˆ  ë…¼ë¬¸ì„ ê²€ìƒ‰í•˜ê³  ë‹¤ìš´ë¡œë“œí•˜ëŠ” ê¸°ëŠ¥ì„ ì œê³µí•©ë‹ˆë‹¤:\n",
    "- search_papers(): í‚¤ì›Œë“œ, ì¹´í…Œê³ ë¦¬, ë‚ ì§œ ë²”ìœ„ë¡œ ë…¼ë¬¸ ê²€ìƒ‰\n",
    "- download_paper(): íŠ¹ì • ë…¼ë¬¸ PDF ë‹¤ìš´ë¡œë“œ\n",
    "- read_paper(): ë…¼ë¬¸ ë©”íƒ€ë°ì´í„° ë° ì´ˆë¡ ì½ê¸°\n",
    "- list_papers(): íŠ¹ì • ì¹´í…Œê³ ë¦¬ì˜ ìµœì‹  ë…¼ë¬¸ ëª©ë¡\n",
    "\n",
    "### 2) ChEMBL MCP ì„œë²„ (mcp_server_chembl.py)\n",
    "ChEMBL ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ í™”í•™ í™”í•©ë¬¼ ì •ë³´ë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤:\n",
    "\n",
    "- compount_activity(): íŠ¹ì • í™”í•©ë¬¼ì˜ ìƒë¬¼í•™ì  í™œì„± ë°ì´í„° (IC50 ê°’ í¬í•¨)\n",
    "- target_activity(): íŠ¹ì • íƒ€ê²Ÿì— ëŒ€í•œ í™œì„± ë°ì´í„° (ì¸ê°„ ëŒ€ìƒ)\n",
    "\n",
    "    ì°¸ê³ : ê° ê²€ìƒ‰ì€ ìµœëŒ€ 100ê°œì˜ í™œì„± ë°ì´í„°ë¡œ ì œí•œë©ë‹ˆë‹¤.\n",
    "\n",
    "### 3) PubMed MCP ì„œë²„ (mcp_server_pubmed.py)\n",
    "PubMedì—ì„œ ìƒì˜í•™ ë¬¸í—Œì„ ê²€ìƒ‰í•©ë‹ˆë‹¤:\n",
    "\n",
    "- pubmed_search(): ì¼ë°˜ í‚¤ì›Œë“œ ê²€ìƒ‰\n",
    "- pubmed_get_article(): íŠ¹ì • PMIDì˜ ìƒì„¸ ì •ë³´ ì¡°íšŒ\n",
    "- pubmed_search_by_protein(): ë‹¨ë°±ì§ˆ ê´€ë ¨ ë…¼ë¬¸ ê²€ìƒ‰\n",
    "- pubmed_search_by_disease(): ì§ˆë³‘ ê´€ë ¨ ë…¼ë¬¸ ê²€ìƒ‰\n",
    "- pubmed_search_by_drug(): ì•½ë¬¼ ê´€ë ¨ ë…¼ë¬¸ ê²€ìƒ‰\n",
    "\n",
    "    ì°¸ê³ : ëª¨ë“  ê²€ìƒ‰ ê²°ê³¼ì—ëŠ” PMID, ì œëª©, ì €ì, ì´ˆë¡, ì €ë„ ì •ë³´ê°€ í¬í•¨ë©ë‹ˆë‹¤.\n",
    "\n",
    "### 4) ClinicalTrials MCP ì„œë²„ (mcp_server_clinicaltrial.py)\n",
    "ClinicalTrials.govì—ì„œ ì„ìƒì‹œí—˜ ì •ë³´ë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤:\n",
    "\n",
    "- search_clinical_trials_and_save_studies_to_csv(): ì„ìƒì‹œí—˜ ê²€ìƒ‰ ë° CSV ì €ì¥\n",
    "- get_full_study_details(): íŠ¹ì • NCT IDì˜ ìƒì„¸ ì •ë³´\n",
    "- get_studies_by_keyword(): í‚¤ì›Œë“œë³„ ì„ìƒì‹œí—˜ ê²€ìƒ‰\n",
    "- get_full_studies_and_save(): ì „ì²´ ì—°êµ¬ ë°ì´í„°ë¥¼ CSVë¡œ ì €ì¥\n",
    "- load_csv_data(): ì €ì¥ëœ CSV íŒŒì¼ ë¡œë“œ\n",
    "- list_saved_csv_files(): ì‚¬ìš© ê°€ëŠ¥í•œ CSV íŒŒì¼ ëª©ë¡\n",
    "\n",
    "    ì°¸ê³ : ì¶œë ¥ì€ ìµœëŒ€ 20,000ìë¡œ ì œí•œë˜ë©°, ê²°ê³¼ê°€ í´ ê²½ìš° CSVë¡œ ì €ì¥ë©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ArXiv MCP ì„œë²„ë¥¼ êµ¬ì„±í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile mcp_server_arxiv.py\n",
    "# Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.\n",
    "# SPDX-License-Identifier: MIT-0\n",
    "\n",
    "import logging\n",
    "import sys\n",
    "from datetime import datetime, timezone\n",
    "from typing import Any, Dict, List\n",
    "\n",
    "import arxiv\n",
    "from dateutil import parser\n",
    "from mcp.server.fastmcp import FastMCP\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(filename)s:%(lineno)d | %(message)s',\n",
    "    handlers=[logging.StreamHandler(sys.stderr)]\n",
    ")\n",
    "logger = logging.getLogger(\"arxiv_mcp\")\n",
    "\n",
    "MAX_RESULTS = 10\n",
    "\n",
    "try:\n",
    "    mcp = FastMCP(name=\"arxiv_tools\")\n",
    "    logger.info(\"arXiv MCP server initialized successfully\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error: {str(e)}\")\n",
    "\n",
    "def _is_within_date_range(\n",
    "    date: datetime, start: datetime | None, end: datetime | None\n",
    ") -> bool:\n",
    "    \"\"\"Check if a date falls within the specified range.\"\"\"\n",
    "    if start and not start.tzinfo:\n",
    "        start = start.replace(tzinfo=timezone.utc)\n",
    "    if end and not end.tzinfo:\n",
    "        end = end.replace(tzinfo=timezone.utc)\n",
    "\n",
    "    if start and date < start:\n",
    "        return False\n",
    "    if end and date > end:\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "def _process_paper(paper: arxiv.Result) -> Dict[str, Any]:\n",
    "    \"\"\"Process paper information with resource URI.\"\"\"\n",
    "    return {\n",
    "        \"id\": paper.get_short_id(),\n",
    "        \"title\": paper.title,\n",
    "        \"authors\": [author.name for author in paper.authors],\n",
    "        \"abstract\": paper.summary,\n",
    "        \"categories\": paper.categories,\n",
    "        \"published\": paper.published.isoformat(),\n",
    "        \"url\": paper.pdf_url,\n",
    "        \"resource_uri\": f\"arxiv://{paper.get_short_id()}\",\n",
    "    }\n",
    "\n",
    "@mcp.tool()\n",
    "async def search_papers(\n",
    "    query: str, \n",
    "    max_results: int = 10, \n",
    "    date_from: str = None, \n",
    "    date_to: str = None, \n",
    "    categories: List[str] = None\n",
    ") -> List[Dict[str, Any]]:\n",
    "    \"\"\"Search for papers on arXiv with advanced filtering.\"\"\"\n",
    "    try:\n",
    "        client = arxiv.Client()\n",
    "        max_results = min(int(max_results), MAX_RESULTS)\n",
    "\n",
    "        # Build search query with category filtering\n",
    "        if categories:\n",
    "            category_filter = \" OR \".join(f\"cat:{cat}\" for cat in categories)\n",
    "            query = f\"({query}) AND ({category_filter})\"\n",
    "\n",
    "        search = arxiv.Search(\n",
    "            query=query,\n",
    "            max_results=max_results,\n",
    "            sort_by=arxiv.SortCriterion.SubmittedDate,\n",
    "        )\n",
    "\n",
    "        # Process results with date filtering\n",
    "        results = []\n",
    "        try:\n",
    "            date_from_obj = (\n",
    "                parser.parse(date_from).replace(tzinfo=timezone.utc)\n",
    "                if date_from\n",
    "                else None\n",
    "            )\n",
    "            date_to_obj = (\n",
    "                parser.parse(date_to).replace(tzinfo=timezone.utc) if date_to else None\n",
    "            )\n",
    "        except (ValueError, TypeError) as e:\n",
    "            return [{\"error\": f\"Invalid date format - {str(e)}\"}]\n",
    "\n",
    "        for paper in client.results(search):\n",
    "            if _is_within_date_range(paper.published, date_from_obj, date_to_obj):\n",
    "                results.append(_process_paper(paper))\n",
    "\n",
    "            if len(results) >= max_results:\n",
    "                break\n",
    "\n",
    "        return results\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Search error: {str(e)}\")\n",
    "        return [{\"error\": f\"Search failed: {str(e)}\"}]\n",
    "\n",
    "@mcp.tool()\n",
    "async def download_paper(paper_id: str) -> Dict[str, Any]:\n",
    "    \"\"\"Download a paper from arXiv.\"\"\"\n",
    "    try:\n",
    "        client = arxiv.Client()\n",
    "        search = arxiv.Search(id_list=[paper_id])\n",
    "        \n",
    "        for paper in client.results(search):\n",
    "            return {\n",
    "                \"id\": paper.get_short_id(),\n",
    "                \"title\": paper.title,\n",
    "                \"url\": paper.pdf_url,\n",
    "                \"download_status\": \"success\",\n",
    "                \"resource_uri\": f\"arxiv://{paper.get_short_id()}\"\n",
    "            }\n",
    "        \n",
    "        return {\"error\": f\"Paper with ID {paper_id} not found\"}\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Download error: {str(e)}\")\n",
    "        return {\"error\": f\"Download failed: {str(e)}\"}\n",
    "\n",
    "@mcp.tool()\n",
    "async def read_paper(paper_id: str) -> Dict[str, Any]:\n",
    "    \"\"\"Read the content of an arXiv paper.\"\"\"\n",
    "    try:\n",
    "        client = arxiv.Client()\n",
    "        search = arxiv.Search(id_list=[paper_id])\n",
    "\n",
    "        for paper in client.results(search):\n",
    "            return {\n",
    "                \"id\": paper.get_short_id(),\n",
    "                \"title\": paper.title,\n",
    "                \"authors\": [author.name for author in paper.authors],\n",
    "                \"abstract\": paper.summary,\n",
    "                \"categories\": paper.categories,\n",
    "                \"published\": paper.published.isoformat(),\n",
    "                \"content_type\": \"text\",\n",
    "                \"content\": paper.summary,\n",
    "            }\n",
    "\n",
    "        return {\"error\": f\"Paper with ID {paper_id} not found\"}\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Read error: {str(e)}\")\n",
    "        return {\"error\": f\"Read failed: {str(e)}\"}\n",
    "\n",
    "@mcp.tool()\n",
    "async def list_papers(\n",
    "    category: str = None, max_results: int = 10\n",
    ") -> List[Dict[str, Any]]:\n",
    "    \"\"\"Get a list of the latest papers in a specific category.\"\"\"\n",
    "    try:\n",
    "        client = arxiv.Client()\n",
    "        max_results = min(int(max_results), MAX_RESULTS)\n",
    "\n",
    "        query = f\"cat:{category}\" if category else \"\"\n",
    "        search = arxiv.Search(\n",
    "            query=query,\n",
    "            max_results=max_results,\n",
    "            sort_by=arxiv.SortCriterion.SubmittedDate,\n",
    "        )\n",
    "\n",
    "        results = []\n",
    "        for paper in client.results(search):\n",
    "            results.append(_process_paper(paper))\n",
    "            if len(results) >= max_results:\n",
    "                break\n",
    "\n",
    "        return results\n",
    "    except Exception as e:\n",
    "        logger.error(f\"List error: {str(e)}\")\n",
    "        return [{\"error\": f\"List failed: {str(e)}\"}]\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    mcp.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CheMBL MCP ì„œë²„ë¥¼ êµ¬ì„±í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile mcp_server_chembl.py\n",
    "# Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.\n",
    "# SPDX-License-Identifier: MIT-0\n",
    "\n",
    "import logging\n",
    "import sys\n",
    "from typing import Any, Dict, List\n",
    "\n",
    "from chembl_webresource_client.new_client import new_client\n",
    "from mcp.server.fastmcp import FastMCP\n",
    "\n",
    "MAXIMUM_ACTIVITY = 100\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(filename)s:%(lineno)d | %(message)s',\n",
    "    handlers=[logging.StreamHandler(sys.stderr)]\n",
    ")\n",
    "logger = logging.getLogger(\"chembl_mcp\")\n",
    "\n",
    "try:\n",
    "    mcp = FastMCP(name=\"chembl_tools\")\n",
    "    logger.info(\"ChEMBL MCP server initialized successfully\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error: {str(e)}\")\n",
    "\n",
    "@mcp.tool()\n",
    "async def compount_activity(compound_name: str) -> List[Dict[str, Any]]:\n",
    "    \"\"\"Get activity data for the specified compound\"\"\"\n",
    "    client = new_client\n",
    "    molecule_id = client.molecule.filter(pref_name__iexact=compound_name).only('molecule_chembl_id')[0]\n",
    "    activity = list(client.activity.filter(molecule_chembl_id=molecule_id['molecule_chembl_id']).filter(standard_type=\"IC50\").only(['pchembl_value', 'assay_description', 'canonical_smiles']))\n",
    "    if len(activity) > MAXIMUM_ACTIVITY:\n",
    "        activity = activity[:MAXIMUM_ACTIVITY]\n",
    "    return activity\n",
    "\n",
    "@mcp.tool()\n",
    "async def target_activity(target_name: str) -> List[Dict[str, Any]]:\n",
    "    \"\"\"Get activity data for the specified target\"\"\"\n",
    "    client = new_client\n",
    "    target_id = client.target.filter(target_synonym__icontains=target_name, organism='Homo sapiens').only('target_chembl_id')[0]\n",
    "    activity = list(client.activity.filter(target_chembl_id=target_id['target_chembl_id']).filter(standard_type=\"IC50\").only(['pchembl_value', 'assay_description', 'canonical_smiles']))\n",
    "    if len(activity) > MAXIMUM_ACTIVITY:\n",
    "        activity = activity[:MAXIMUM_ACTIVITY]\n",
    "    return activity\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    mcp.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PubMed MCP ì„œë²„ë¥¼ êµ¬ì„±í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile mcp_server_pubmed.py\n",
    "# Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.\n",
    "# SPDX-License-Identifier: MIT-0\n",
    "\n",
    "import logging\n",
    "import sys\n",
    "from typing import Any, Dict, List, Optional\n",
    "\n",
    "import httpx\n",
    "from defusedxml import ElementTree as ET\n",
    "from mcp.server.fastmcp import FastMCP\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(filename)s:%(lineno)d | %(message)s',\n",
    "    handlers=[logging.StreamHandler(sys.stderr)]\n",
    ")\n",
    "logger = logging.getLogger(\"pubmed_mcp\")\n",
    "\n",
    "try:\n",
    "    mcp = FastMCP(name=\"pubmed_tools\")\n",
    "    logger.info(\"PubMed MCP server initialized successfully\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error: {str(e)}\")\n",
    "\n",
    "# Helper functions for PubMed API\n",
    "def search_pubmed(query: str, max_results: int = 10) -> List[Dict[str, Any]]:\n",
    "    \"\"\"Search PubMed for articles matching the query\"\"\"\n",
    "    base_url = \"https://eutils.ncbi.nlm.nih.gov/entrez/eutils\"\n",
    "    \n",
    "    # Search for IDs\n",
    "    search_url = f\"{base_url}/esearch.fcgi\"\n",
    "    search_params = {\n",
    "        \"db\": \"pubmed\",\n",
    "        \"term\": query,\n",
    "        \"retmax\": max_results,\n",
    "        \"retmode\": \"json\",\n",
    "        \"sort\": \"relevance\",\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        search_response = httpx.get(search_url, params=search_params)\n",
    "        search_response.raise_for_status()\n",
    "        search_data = search_response.json()\n",
    "        \n",
    "        # Extract IDs\n",
    "        id_list = search_data[\"esearchresult\"][\"idlist\"]\n",
    "        if not id_list:\n",
    "            return []\n",
    "        \n",
    "        # Fetch article details\n",
    "        fetch_url = f\"{base_url}/efetch.fcgi\"\n",
    "        fetch_params = {\n",
    "            \"db\": \"pubmed\",\n",
    "            \"id\": \",\".join(id_list),\n",
    "            \"retmode\": \"xml\"\n",
    "        }\n",
    "        \n",
    "        fetch_response = httpx.get(fetch_url, params=fetch_params)\n",
    "        fetch_response.raise_for_status()\n",
    "        \n",
    "        # Parse XML response\n",
    "        root = ET.fromstring(fetch_response.text)\n",
    "        articles = []\n",
    "        \n",
    "        for article_element in root.findall(\".//PubmedArticle\"):\n",
    "            try:\n",
    "                article = {}\n",
    "                \n",
    "                # Extract PMID\n",
    "                pmid = article_element.find(\".//PMID\")\n",
    "                if pmid is not None:\n",
    "                    article[\"id\"] = pmid.text\n",
    "                \n",
    "                # Extract title\n",
    "                title = article_element.find(\".//ArticleTitle\")\n",
    "                if title is not None:\n",
    "                    article[\"title\"] = title.text\n",
    "                \n",
    "                # Extract abstract\n",
    "                abstract_parts = article_element.findall(\".//AbstractText\")\n",
    "                if abstract_parts:\n",
    "                    abstract = \" \".join([part.text for part in abstract_parts if part.text])\n",
    "                    article[\"abstract\"] = abstract\n",
    "                \n",
    "                # Extract authors\n",
    "                author_elements = article_element.findall(\".//Author\")\n",
    "                if author_elements:\n",
    "                    authors = []\n",
    "                    for author in author_elements:\n",
    "                        last_name = author.find(\"LastName\")\n",
    "                        fore_name = author.find(\"ForeName\")\n",
    "                        if last_name is not None and fore_name is not None:\n",
    "                            authors.append(f\"{fore_name.text} {last_name.text}\")\n",
    "                        elif last_name is not None:\n",
    "                            authors.append(last_name.text)\n",
    "                    article[\"authors\"] = \", \".join(authors)\n",
    "                \n",
    "                # Extract journal info\n",
    "                journal = article_element.find(\".//Journal/Title\")\n",
    "                if journal is not None:\n",
    "                    article[\"journal\"] = journal.text\n",
    "                \n",
    "                # Extract publication year\n",
    "                pub_date = article_element.find(\".//PubDate/Year\")\n",
    "                if pub_date is not None:\n",
    "                    article[\"year\"] = pub_date.text\n",
    "                \n",
    "                articles.append(article)\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error parsing article: {e}\")\n",
    "                continue\n",
    "        \n",
    "        return articles\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error searching PubMed: {e}\")\n",
    "        return []\n",
    "\n",
    "def get_pubmed_article_details(pmid: str) -> Optional[Dict[str, Any]]:\n",
    "    \"\"\"Get detailed information about a specific PubMed article\"\"\"\n",
    "    base_url = \"https://eutils.ncbi.nlm.nih.gov/entrez/eutils\"\n",
    "    fetch_url = f\"{base_url}/efetch.fcgi\"\n",
    "    \n",
    "    fetch_params = {\"db\": \"pubmed\", \"id\": pmid, \"retmode\": \"xml\"}\n",
    "    \n",
    "    try:\n",
    "        fetch_response = httpx.get(fetch_url, params=fetch_params)\n",
    "        fetch_response.raise_for_status()\n",
    "        \n",
    "        # Parse XML response\n",
    "        root = ET.fromstring(fetch_response.text)\n",
    "        article_element = root.find(\".//PubmedArticle\")\n",
    "        \n",
    "        if article_element is None:\n",
    "            return None\n",
    "        \n",
    "        article = {\"id\": pmid, \"references\": []}\n",
    "        \n",
    "        # Extract title\n",
    "        title = article_element.find(\".//ArticleTitle\")\n",
    "        if title is not None:\n",
    "            article[\"title\"] = title.text\n",
    "        \n",
    "        # Extract abstract\n",
    "        abstract_parts = article_element.findall(\".//AbstractText\")\n",
    "        if abstract_parts:\n",
    "            abstract = \" \".join([part.text for part in abstract_parts if part.text])\n",
    "            article[\"abstract\"] = abstract\n",
    "        \n",
    "        # Extract authors\n",
    "        author_elements = article_element.findall(\".//Author\")\n",
    "        if author_elements:\n",
    "            authors = []\n",
    "            for author in author_elements:\n",
    "                last_name = author.find(\"LastName\")\n",
    "                fore_name = author.find(\"ForeName\")\n",
    "                if last_name is not None and fore_name is not None:\n",
    "                    authors.append(f\"{fore_name.text} {last_name.text}\")\n",
    "                elif last_name is not None:\n",
    "                    authors.append(last_name.text)\n",
    "            article[\"authors\"] = \", \".join(authors)\n",
    "        \n",
    "        # Extract journal info\n",
    "        journal = article_element.find(\".//Journal/Title\")\n",
    "        if journal is not None:\n",
    "            article[\"journal\"] = journal.text\n",
    "        \n",
    "        # Extract publication year\n",
    "        pub_date = article_element.find(\".//PubDate/Year\")\n",
    "        if pub_date is not None:\n",
    "            article[\"year\"] = pub_date.text\n",
    "        \n",
    "        # Extract DOI\n",
    "        article_id_list = article_element.findall(\".//ArticleId\")\n",
    "        for article_id in article_id_list:\n",
    "            if article_id.get(\"IdType\") == \"doi\":\n",
    "                article[\"doi\"] = article_id.text\n",
    "        \n",
    "        # Extract keywords\n",
    "        keyword_elements = article_element.findall(\".//Keyword\")\n",
    "        if keyword_elements:\n",
    "            keywords = [k.text for k in keyword_elements if k.text]\n",
    "            article[\"keywords\"] = \", \".join(keywords)\n",
    "        \n",
    "        return article\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error fetching article details: {e}\")\n",
    "        return None\n",
    "\n",
    "@mcp.tool()\n",
    "def pubmed_search(query: str, max_results: int = 10):\n",
    "    \"\"\"Search PubMed for articles matching the query.\"\"\"\n",
    "    logger.info(f\"Searching PubMed for: {query}\")\n",
    "    results = search_pubmed(query, max_results)\n",
    "    logger.info(f\"Found {len(results)} results\")\n",
    "    return results\n",
    "\n",
    "@mcp.tool()\n",
    "def pubmed_get_article(pmid: str):\n",
    "    \"\"\"Get detailed information about a specific PubMed article.\"\"\"\n",
    "    logger.info(f\"Fetching PubMed article: {pmid}\")\n",
    "    result = get_pubmed_article_details(pmid)\n",
    "    if result:\n",
    "        logger.info(f\"Successfully fetched article: {pmid}\")\n",
    "    else:\n",
    "        logger.info(f\"Failed to fetch article: {pmid}\")\n",
    "    return result\n",
    "\n",
    "@mcp.tool()\n",
    "def pubmed_search_by_protein(protein_name: str, max_results: int = 10):\n",
    "    \"\"\"Search PubMed for articles about a specific protein.\"\"\"\n",
    "    query = f\"{protein_name}[Title/Abstract] AND protein[Title/Abstract]\"\n",
    "    logger.info(f\"Searching PubMed for protein: {protein_name}\")\n",
    "    results = search_pubmed(query, max_results)\n",
    "    logger.info(f\"Found {len(results)} results for protein: {protein_name}\")\n",
    "    return results\n",
    "\n",
    "@mcp.tool()\n",
    "def pubmed_search_by_disease(disease_name: str, max_results: int = 10):\n",
    "    \"\"\"Search PubMed for articles about a specific disease.\"\"\"\n",
    "    query = f\"{disease_name}[Title/Abstract] AND (disease[Title/Abstract] OR disorder[Title/Abstract] OR condition[Title/Abstract])\"\n",
    "    logger.info(f\"Searching PubMed for disease: {disease_name}\")\n",
    "    results = search_pubmed(query, max_results)\n",
    "    logger.info(f\"Found {len(results)} results for disease: {disease_name}\")\n",
    "    return results\n",
    "\n",
    "@mcp.tool()\n",
    "def pubmed_search_by_drug(drug_name: str, max_results: int = 10):\n",
    "    \"\"\"Search PubMed for articles about a specific drug.\"\"\"\n",
    "    query = f\"{drug_name}[Title/Abstract] AND (drug[Title/Abstract] OR medication[Title/Abstract] OR compound[Title/Abstract])\"\n",
    "    logger.info(f\"Searching PubMed for drug: {drug_name}\")\n",
    "    results = search_pubmed(query, max_results)\n",
    "    logger.info(f\"Found {len(results)} results for drug: {drug_name}\")\n",
    "    return results\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    mcp.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ClinicalTrials MCP ì„œë²„ë¥¼ êµ¬ì„±í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile mcp_server_clinicaltrial.py\n",
    "# Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.\n",
    "# SPDX-License-Identifier: MIT-0\n",
    "\n",
    "import logging\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import pandas as pd\n",
    "from mcp.server.fastmcp import FastMCP\n",
    "from pytrials.client import ClinicalTrials\n",
    "\n",
    "MAX_OUTPUT_CHARS = 20000\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(filename)s:%(lineno)d | %(message)s',\n",
    "    handlers=[logging.StreamHandler(sys.stderr)]\n",
    ")\n",
    "logger = logging.getLogger(\"clinicaltrial_mcp\")\n",
    "\n",
    "try:\n",
    "    mcp = FastMCP(name=\"clinicaltrial_tools\")\n",
    "    logger.info(\"Clinical Trial MCP server initialized successfully\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error: {str(e)}\")\n",
    "\n",
    "ct = ClinicalTrials()\n",
    "\n",
    "# Helper functions\n",
    "def load_csv_file(filename):\n",
    "    \"\"\"Load data from a CSV file\"\"\"\n",
    "    if os.path.exists(filename):\n",
    "        return pd.read_csv(filename)\n",
    "    return None\n",
    "\n",
    "def format_limited_output(df, max_rows=None, max_chars=MAX_OUTPUT_CHARS):\n",
    "    \"\"\"Format DataFrame output with character limit and metadata\"\"\"\n",
    "    if df is None or df.empty:\n",
    "        return \"No data available\"\n",
    "\n",
    "    total_rows = len(df)\n",
    "\n",
    "    # If maximum rows are specified, limit the output rows\n",
    "    if max_rows and max_rows < total_rows:\n",
    "        display_df = df.head(max_rows)\n",
    "        rows_shown = max_rows\n",
    "    else:\n",
    "        display_df = df\n",
    "        rows_shown = total_rows\n",
    "\n",
    "    # Convert to string\n",
    "    output = display_df.to_string()\n",
    "\n",
    "    # If exceeding character limit, truncate\n",
    "    if len(output) > max_chars:\n",
    "        output = output[:max_chars] + \"\\n...[Output truncated]\"\n",
    "\n",
    "    # Add metadata\n",
    "    metadata = f\"\\n\\nData summary: Total {total_rows} records, showing {rows_shown} records.\"\n",
    "\n",
    "    return output + metadata\n",
    "\n",
    "def list_available_csv_files():\n",
    "    \"\"\"List all available CSV files in the current directory\"\"\"\n",
    "    return [f for f in os.listdir(\".\") if f.endswith(\".csv\")]\n",
    "\n",
    "@mcp.tool()\n",
    "def search_clinical_trials_and_save_studies_to_csv(\n",
    "    search_expr: str,\n",
    "    max_studies: int = 10,\n",
    "    save_csv: bool = False,\n",
    "    filename: str = \"search_results.csv\",\n",
    "    fields: list = None,\n",
    ") -> str:\n",
    "    \"\"\"Search for clinical trials using a search expression\"\"\"\n",
    "    try:\n",
    "        # Default fields if none provided\n",
    "        if fields is None:\n",
    "            fields = [\"NCT Number\", \"Conditions\", \"Study Title\", \"Brief Summary\"]\n",
    "\n",
    "        # Get study fields\n",
    "        results = ct.get_study_fields(\n",
    "            search_expr=search_expr, fields=fields, max_studies=max_studies\n",
    "        )\n",
    "\n",
    "        if len(results) > 1:  # Header + data\n",
    "            df = pd.DataFrame.from_records(results[1:], columns=results[0])\n",
    "\n",
    "            # Save to CSV if requested\n",
    "            if save_csv:\n",
    "                csv_filename = filename or f\"search_results_{search_expr.replace('+', '_')}.csv\"\n",
    "                df.to_csv(csv_filename, index=False)\n",
    "                storage_info = f\"Complete results have been saved to file {csv_filename}\"\n",
    "                return f\"Results saved to {csv_filename}\\n\\n{format_limited_output(df)}\\n{storage_info}\"\n",
    "\n",
    "            return format_limited_output(df)\n",
    "        return \"No results found\"\n",
    "    except Exception as e:\n",
    "        return f\"Error searching clinical trials: {str(e)}\"\n",
    "\n",
    "@mcp.tool()\n",
    "def get_full_study_details(nct_id: str) -> str:\n",
    "    \"\"\"Get detailed information about a specific clinical trial\"\"\"\n",
    "    try:\n",
    "        study = ct.get_full_studies(search_expr=f\"NCT Number={nct_id}\", max_studies=1)\n",
    "        if len(study) > 1:  # Header + data\n",
    "            df = pd.DataFrame.from_records(study[1:], columns=study[0])\n",
    "            return format_limited_output(df)\n",
    "        return f\"Study with NCT ID {nct_id} not found\"\n",
    "    except Exception as e:\n",
    "        return f\"Error fetching study details: {str(e)}\"\n",
    "\n",
    "@mcp.tool()\n",
    "def get_studies_by_keyword(\n",
    "    keyword: str, max_studies: int = 20, save_csv: bool = False, filename: str = None\n",
    ") -> str:\n",
    "    \"\"\"Get studies related to a specific keyword\"\"\"\n",
    "    try:\n",
    "        fields = [\"NCT Number\", \"Conditions\", \"Study Title\", \"Brief Summary\"]\n",
    "        results = ct.get_study_fields(\n",
    "            search_expr=keyword, fields=fields, max_studies=max_studies\n",
    "        )\n",
    "\n",
    "        if len(results) > 1:  # Header + data\n",
    "            df = pd.DataFrame.from_records(results[1:], columns=results[0])\n",
    "\n",
    "            # Save to CSV if requested\n",
    "            if save_csv:\n",
    "                csv_filename = filename or f\"keyword_results_{keyword.replace(' ', '_')}.csv\"\n",
    "                df.to_csv(csv_filename, index=False)\n",
    "                storage_info = f\"Complete results have been saved to file {csv_filename}\"\n",
    "                return f\"Results saved to {csv_filename}\\n\\n{format_limited_output(df)}\\n{storage_info}\"\n",
    "\n",
    "            return format_limited_output(df)\n",
    "        return f\"No studies found for keyword: {keyword}\"\n",
    "    except Exception as e:\n",
    "        return f\"Error searching studies by keyword: {str(e)}\"\n",
    "\n",
    "@mcp.tool()\n",
    "def get_full_studies_and_save(\n",
    "    search_expr: str, max_studies: int = 20, filename: str = \"full_studies.csv\"\n",
    ") -> str:\n",
    "    \"\"\"Get full studies data and save to CSV\"\"\"\n",
    "    try:\n",
    "        # Get full studies\n",
    "        full_studies = ct.get_full_studies(\n",
    "            search_expr=search_expr, max_studies=max_studies\n",
    "        )\n",
    "\n",
    "        if len(full_studies) > 1:  # Header + data\n",
    "            # Convert to DataFrame\n",
    "            df = pd.DataFrame.from_records(full_studies[1:], columns=full_studies[0])\n",
    "\n",
    "            # Save to CSV\n",
    "            df.to_csv(filename, index=False)\n",
    "\n",
    "            return f\"Successfully saved {len(df)} full studies to {filename}\"\n",
    "        return \"No results found to save\"\n",
    "    except Exception as e:\n",
    "        return f\"Error saving full studies to CSV: {str(e)}\"\n",
    "\n",
    "@mcp.tool()\n",
    "def load_csv_data(filename: str) -> str:\n",
    "    \"\"\"Load and display data from a CSV file\"\"\"\n",
    "    # Ensure the filename ends with .csv\n",
    "    if not filename.endswith(\".csv\"):\n",
    "        filename += \".csv\"\n",
    "\n",
    "    df = load_csv_file(filename)\n",
    "    if df is not None:\n",
    "        return f\"Loaded data from {filename}:\\n\\n{format_limited_output(df)}\"\n",
    "    return f\"CSV file {filename} not found or could not be loaded\"\n",
    "\n",
    "@mcp.tool()\n",
    "def list_saved_csv_files() -> str:\n",
    "    \"\"\"List all available CSV files in the current directory\"\"\"\n",
    "    files = list_available_csv_files()\n",
    "    if files:\n",
    "        return f\"Available CSV files:\\n\\n{chr(10).join(files)}\"\n",
    "    return \"No CSV files available\"\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    mcp.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. MCP í´ë¼ì´ì–¸íŠ¸ ì„¤ì •\n",
    "\n",
    "ê° ì™¸ë¶€ ë°ì´í„°ë² ì´ìŠ¤ì— ëŒ€í•œ MCP í´ë¼ì´ì–¸íŠ¸ë¥¼ ì„¤ì •í•©ë‹ˆë‹¤.\n",
    "MCP ì„œë²„ëŠ” ë³„ë„ í”„ë¡œì„¸ìŠ¤ë¡œ ì‹¤í–‰ë˜ì–´ íŠ¹ì • ê¸°ëŠ¥(ì˜ˆ: arXiv ë…¼ë¬¸ ê²€ìƒ‰)ì„ ì œê³µí•˜ê³ , í´ë¼ì´ì–¸íŠ¸ëŠ” JSON-RPC í”„ë¡œí† ì½œ ê¸°ë°˜ ë©”ì‹œì§€ êµ¬ì¡°ë¡œ í•´ë‹¹ ê¸°ëŠ¥ì„ í˜¸ì¶œí•  ìˆ˜ ìˆê²Œ ë©ë‹ˆë‹¤. ì´ë•Œ í†µì‹  ë°©ì‹ì€ stdin/stdoutë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MCP ì„œë²„ íŒŒë¼ë¯¸í„° ì„¤ì •\n",
    "arxiv_server_params = StdioServerParameters(\n",
    "    command=\"python\",\n",
    "    args=[\"mcp_server_arxiv.py\"]\n",
    ")\n",
    "\n",
    "chembl_server_params = StdioServerParameters(\n",
    "    command=\"python\",\n",
    "    args=[\"mcp_server_chembl.py\"]\n",
    ")\n",
    "\n",
    "pubmed_server_params = StdioServerParameters(\n",
    "    command=\"python\",\n",
    "    args=[\"mcp_server_pubmed.py\"]\n",
    ")\n",
    "\n",
    "clinicaltrial_server_params = StdioServerParameters(\n",
    "    command=\"python\",\n",
    "    args=[\"mcp_server_clinicaltrial.py\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "mcp ë¼ì´ë¸ŒëŸ¬ë¦¬ì˜ MCPClient í´ë˜ìŠ¤ë¡œ ê°„ë‹¨í•˜ê²Œ í´ë¼ì´ì–¸íŠ¸ ì¸ìŠ¤í„´ìŠ¤ë¥¼ ìƒì„±í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MCP í´ë¼ì´ì–¸íŠ¸ ìƒì„± (lambdaì™€ stdio_client ì‚¬ìš©)\n",
    "arxiv_client = MCPClient(\n",
    "    lambda: stdio_client(arxiv_server_params)\n",
    ")\n",
    "chembl_client = MCPClient(\n",
    "    lambda: stdio_client(chembl_server_params)\n",
    ")\n",
    "pubmed_client = MCPClient(\n",
    "    lambda: stdio_client(pubmed_server_params)\n",
    ")\n",
    "clinicaltrial_client = MCPClient(\n",
    "    lambda: stdio_client(clinicaltrial_server_params)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. ê°œë³„ ë°ì´í„°ë² ì´ìŠ¤ ì—ì´ì „íŠ¸ ìƒì„±\n",
    "\n",
    "ê° MCP í´ë¼ì´ì–¸íŠ¸ë¥¼ ì‚¬ìš©í•˜ëŠ” ì „ë¬¸ ì—ì´ì „íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bedrock ëª¨ë¸ ì„¤ì •\n",
    "model = BedrockModel(\n",
    "    model_id=\"global.anthropic.claude-sonnet-4-5-20250929-v1:0\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ArXiv ì „ë¬¸ ì—ì´ì „íŠ¸\n",
    "@tool\n",
    "def arxiv_agent(query: str) -> str:\n",
    "    \"\"\"\n",
    "    Specialized agent for searching Arxiv database for scientific papers.\n",
    "    \n",
    "    Args:\n",
    "        query: The search query for Arxiv\n",
    "        \n",
    "    Returns:\n",
    "        Summarized findings from Arxiv papers\n",
    "    \"\"\"\n",
    "    try:\n",
    "        system_prompt = \"\"\"ë‹¹ì‹ ì€ arXivì—ì„œ í•™ìˆ  ë…¼ë¬¸ì„ ê²€ìƒ‰í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤. \n",
    "        ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ë”°ë¼ ê´€ë ¨ëœ ê³¼í•™ ë¬¸í—Œì„ ì°¾ì•„ ì œê³µí•˜ì„¸ìš”.\n",
    "        ê²€ìƒ‰ ê²°ê³¼ëŠ” ë…¼ë¬¸ ì œëª©, ì €ì, ìš”ì•½, URLì„ í¬í•¨í•´ì•¼ í•©ë‹ˆë‹¤.\"\"\"\n",
    "        \n",
    "        with arxiv_client as client:\n",
    "            tools = client.list_tools_sync()\n",
    "            \n",
    "            agent = Agent(\n",
    "                name=\"ArXivAgent\",\n",
    "                system_prompt=system_prompt,\n",
    "                model=model,\n",
    "                tools=tools\n",
    "            )\n",
    "            \n",
    "            response = agent(query)\n",
    "            return str(response)\n",
    "    except Exception as e:\n",
    "        return f\"Error in arxiv agent: {str(e)}\"\n",
    "\n",
    "# ChEMBL ì „ë¬¸ ì—ì´ì „íŠ¸\n",
    "@tool\n",
    "def chembl_agent(query: str) -> str:\n",
    "    \"\"\"\n",
    "    Specialized agent for searching ChEMBL database for compound information.\n",
    "    \n",
    "    Args:\n",
    "        query: The search query for ChEMBL\n",
    "        \n",
    "    Returns:\n",
    "        Structured information about compounds, targets, and bioactivity\n",
    "    \"\"\"\n",
    "    try:\n",
    "        system_prompt = \"\"\"ë‹¹ì‹ ì€ ChEMBL ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ í™”í•™ í™”í•©ë¬¼ ì •ë³´ë¥¼ ê²€ìƒ‰í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤.\n",
    "        í™”í•©ë¬¼ì˜ êµ¬ì¡°, íŠ¹ì„±, ìƒë¬¼í•™ì  í™œì„± ë°ì´í„°ë¥¼ ì œê³µí•˜ì„¸ìš”.\n",
    "        ê²€ìƒ‰ ê²°ê³¼ëŠ” ChEMBL ID, í™”í•©ë¬¼ëª…, ë¶„ìì‹, ë¶„ìëŸ‰ ë“±ì„ í¬í•¨í•´ì•¼ í•©ë‹ˆë‹¤.\"\"\"\n",
    "        \n",
    "        with chembl_client as client:\n",
    "            tools = client.list_tools_sync()\n",
    "            \n",
    "            agent = Agent(\n",
    "                name=\"ChEMBLAgent\",\n",
    "                system_prompt=system_prompt,\n",
    "                model=model,\n",
    "                tools=tools\n",
    "            )\n",
    "            \n",
    "            response = agent(query)\n",
    "            return str(response)\n",
    "    except Exception as e:\n",
    "        return f\"Error in chembl agent: {str(e)}\"\n",
    "\n",
    "# PubMed ì „ë¬¸ ì—ì´ì „íŠ¸\n",
    "@tool\n",
    "def pubmed_agent(query: str) -> str:\n",
    "    \"\"\"\n",
    "    Specialized agent for searching PubMed database for medical papers.\n",
    "    \n",
    "    Args:\n",
    "        query: The search query for PubMed\n",
    "        \n",
    "    Returns:\n",
    "        Summarized findings from PubMed papers\n",
    "    \"\"\"\n",
    "    try:\n",
    "        system_prompt = \"\"\"ë‹¹ì‹ ì€ PubMedì—ì„œ ìƒì˜í•™ ë¬¸í—Œì„ ê²€ìƒ‰í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤.\n",
    "        ì˜í•™, ìƒë¬¼í•™, ìƒëª…ê³¼í•™ ê´€ë ¨ ì—°êµ¬ ë…¼ë¬¸ì„ ì°¾ì•„ ì œê³µí•˜ì„¸ìš”.\n",
    "        ê²€ìƒ‰ ê²°ê³¼ëŠ” PMID, ë…¼ë¬¸ ì œëª©, ì €ì, ì¶œíŒ ì •ë³´ë¥¼ í¬í•¨í•´ì•¼ í•©ë‹ˆë‹¤.\"\"\"\n",
    "        \n",
    "        with pubmed_client as client:\n",
    "            tools = client.list_tools_sync()\n",
    "            \n",
    "            agent = Agent(\n",
    "                name=\"PubMedAgent\",\n",
    "                system_prompt=system_prompt,\n",
    "                model=model,\n",
    "                tools=tools\n",
    "            )\n",
    "            \n",
    "            response = agent(query)\n",
    "            return str(response)\n",
    "    except Exception as e:\n",
    "        return f\"Error in pubmed agent: {str(e)}\"\n",
    "\n",
    "# ClinicalTrials ì „ë¬¸ ì—ì´ì „íŠ¸\n",
    "@tool\n",
    "def clinicaltrial_agent(query: str) -> str:\n",
    "    \"\"\"\n",
    "    Specialized agent for searching ClinicalTrials.gov database.\n",
    "    \n",
    "    Args:\n",
    "        query: The search query for ClinicalTrials.gov\n",
    "        \n",
    "    Returns:\n",
    "        Information about relevant clinical trials\n",
    "    \"\"\"\n",
    "    try:\n",
    "        system_prompt = \"\"\"ë‹¹ì‹ ì€ ClinicalTrials.govì—ì„œ ì„ìƒì‹œí—˜ ì •ë³´ë¥¼ ê²€ìƒ‰í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤.\n",
    "        ì§„í–‰ ì¤‘ì´ê±°ë‚˜ ì™„ë£Œëœ ì„ìƒ ì—°êµ¬ì— ëŒ€í•œ ì •ë³´ë¥¼ ì œê³µí•˜ì„¸ìš”.\n",
    "        ê²€ìƒ‰ ê²°ê³¼ëŠ” NCT ID, ì—°êµ¬ ì œëª©, ìƒíƒœ, ë‹¨ê³„, ì¡°ê±´ì„ í¬í•¨í•´ì•¼ í•©ë‹ˆë‹¤.\"\"\"\n",
    "        \n",
    "        with clinicaltrial_client as client:\n",
    "            tools = client.list_tools_sync()\n",
    "            \n",
    "            agent = Agent(\n",
    "                name=\"ClinicalTrialAgent\",\n",
    "                system_prompt=system_prompt,\n",
    "                model=model,\n",
    "                tools=tools\n",
    "            )\n",
    "            \n",
    "            response = agent(query)\n",
    "            return str(response)\n",
    "    except Exception as e:\n",
    "        return f\"Error in clinicaltrial agent: {str(e)}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. ë©”ì¸ ì—°êµ¬ ì—ì´ì „íŠ¸ ìƒì„± (Agents-as-Tools íŒ¨í„´)\n",
    "\n",
    "ê°œë³„ ë°ì´í„°ë² ì´ìŠ¤ ì—ì´ì „íŠ¸ë“¤ì„ ë„êµ¬ë¡œ ì‚¬ìš©í•˜ëŠ” í†µí•© ì—°êµ¬ ì—ì´ì „íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤. \n",
    "Agents-as-Tools íŒ¨í„´ì„ ì‚¬ìš©í•˜ë©´ íŠ¹ìˆ˜í™”ëœ ì—ì´ì „íŠ¸ë¥¼ í˜¸ì¶œ ê°€ëŠ¥í•œ í•¨ìˆ˜ë¡œ ë³€í™˜í•˜ì—¬ ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´í„° ì—ì´ì „íŠ¸ê°€ ì‚¬ìš©í•  ìˆ˜ ìˆê²Œ í•©ë‹ˆë‹¤.\n",
    "\n",
    "![Agents-as-Tools-Pattern](./images/Agents-as-Tools-Pattern.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# í†µí•© ì—°êµ¬ ì—ì´ì „íŠ¸ ìƒì„±\n",
    "research_agent = Agent(\n",
    "    name=\"ìƒëª…ê³¼í•™ì—°êµ¬ì—ì´ì „íŠ¸\",\n",
    "    system_prompt=\"\"\"ë‹¹ì‹ ì€ ì¢…í•©ì ì¸ ìƒëª…ê³¼í•™ ì—°êµ¬ ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. \n",
    "    ë‹¤ìŒê³¼ ê°™ì€ ì „ë¬¸ ë°ì´í„°ë² ì´ìŠ¤ ì—ì´ì „íŠ¸ë“¤ì„ ë„êµ¬ë¡œ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:\n",
    "    \n",
    "    - arxiv_agent: í•™ìˆ  ë…¼ë¬¸ ë° í”„ë¦¬í”„ë¦°íŠ¸ ê²€ìƒ‰\n",
    "    - chembl_agent: í™”í•™ í™”í•©ë¬¼ ë°ì´í„° ê²€ìƒ‰\n",
    "    - pubmed_agent: ìƒì˜í•™ ë¬¸í—Œ ê²€ìƒ‰\n",
    "    - clinicaltrial_agent: ì„ìƒì‹œí—˜ ì •ë³´ ê²€ìƒ‰\n",
    "    \n",
    "    ì‚¬ìš©ìì˜ ì—°êµ¬ ì§ˆë¬¸ì— ë”°ë¼ ì ì ˆí•œ ì—ì´ì „íŠ¸ë¥¼ ì„ íƒí•˜ì—¬ ì‚¬ìš©í•˜ì„¸ìš”.\n",
    "    í¬ê´„ì ì¸ ë‹µë³€ì„ ìœ„í•´ ì—¬ëŸ¬ ì—ì´ì „íŠ¸ë¥¼ ì¡°í•©í•˜ì—¬ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "    ê° ì—ì´ì „íŠ¸ì˜ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ì¢…í•©í•˜ì—¬ ìœ ìš©í•œ ì •ë³´ë¥¼ ì œê³µí•˜ì„¸ìš”.\"\"\",\n",
    "    model=model,\n",
    "    tools=[\n",
    "        arxiv_agent,\n",
    "        chembl_agent,\n",
    "        pubmed_agent,\n",
    "        clinicaltrial_agent\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. í…ŒìŠ¤íŠ¸ ë° ì‹¤ìŠµ\n",
    "\n",
    "### 6.1 ê°œë³„ ì—ì´ì „íŠ¸ í…ŒìŠ¤íŠ¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ArXiv ì—ì´ì „íŠ¸ í…ŒìŠ¤íŠ¸\n",
    "print(\"=== ArXiv ì—ì´ì „íŠ¸ í…ŒìŠ¤íŠ¸ ===\")\n",
    "arxiv_result = arxiv_agent(\"Machine Learning Small Molecule Properties in Drug Discovery\")\n",
    "print(arxiv_result)\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ChEMBL ì—ì´ì „íŠ¸ í…ŒìŠ¤íŠ¸\n",
    "print(\"=== ChEMBL ì—ì´ì „íŠ¸ í…ŒìŠ¤íŠ¸ ===\")\n",
    "chembl_result = chembl_agent(\"aspirin\")\n",
    "print(chembl_result)\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PubMed ì—ì´ì „íŠ¸ í…ŒìŠ¤íŠ¸\n",
    "print(\"=== PubMed ì—ì´ì „íŠ¸ í…ŒìŠ¤íŠ¸ ===\")\n",
    "pubmed_result = pubmed_agent(\"COVID-19 ì¹˜ë£Œë²•\")\n",
    "print(pubmed_result)\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ClinicalTrial ì—ì´ì „íŠ¸ í…ŒìŠ¤íŠ¸\n",
    "print(\"=== ClinicalTrial ì—ì´ì „íŠ¸ í…ŒìŠ¤íŠ¸ ===\")\n",
    "ct_result = clinicaltrial_agent(\"ë‹¹ë‡¨ë³‘\")\n",
    "print(ct_result)\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 í†µí•© ì—°êµ¬ ì—ì´ì „íŠ¸ í…ŒìŠ¤íŠ¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì¢…í•©ì ì¸ ì—°êµ¬ ì§ˆë¬¸ í…ŒìŠ¤íŠ¸\n",
    "research_query = \"ì‹¬í˜ˆê´€ ì§ˆí™˜ ì˜ˆë°©ì„ ìœ„í•œ ì•„ìŠ¤í”¼ë¦°ì˜ íš¨ê³¼ì— ëŒ€í•œ ìµœì‹  ì—°êµ¬ ë™í–¥ì„ ì•Œë ¤ì£¼ì„¸ìš”. ê´€ë ¨ ë…¼ë¬¸, í™”í•©ë¬¼ ì •ë³´, ì„ìƒì‹œí—˜ ë°ì´í„°ë¥¼ í¬í•¨í•´ì„œ ë‹µë³€í•´ì£¼ì„¸ìš”.\"\n",
    "\n",
    "print(f\"ğŸ”¬ ì—°êµ¬ ì§ˆë¬¸: {research_query}\\n\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "research_result = research_agent(research_query)\n",
    "print(f\"\\nğŸ“Š ì—°êµ¬ ê²°ê³¼:\\n{research_result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MCPë¥¼ í†µí•œ ì™¸ë¶€ ë°ì´í„°ë² ì´ìŠ¤ë¥¼ í™œìš©í•˜ëŠ” í†µí•© ì—°êµ¬ ì—ì´ì „íŠ¸ í…ŒìŠ¤íŠ¸ë¥¼ ì™„ë£Œí•˜ì˜€ìŠµë‹ˆë‹¤."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
