{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5c0122e65c053f38",
   "metadata": {},
   "source": [
    "# Amazon Bedrock AgentCore Runtimeì—ì„œ Bedrock Manus í˜¸ìŠ¤íŒ…í•˜ê¸° ê°€ì´ë“œ\n",
    "\n",
    "ì´ íŠœí† ë¦¬ì–¼ì—ì„œëŠ” Amazon Bedrock AgentCore Runtimeì„ ì‚¬ìš©í•˜ì—¬ ê¸°ì¡´ Strands SDK ê¸°ë°˜ìœ¼ë¡œ ë¡œì»¬ì—ì„œ ëŒì•„ê°€ë˜ Bedrock Manus ì—ì´ì „íŠ¸ë¥¼ Bedrock AgentCore í™˜ê²½ì— í˜¸ìŠ¤íŒ…í•˜ëŠ” ë°©ë²•ì„ ë°°ì›ë‹ˆë‹¤.\n",
    "\n",
    "\n",
    "### Overview\n",
    "\n",
    "ì‹œì—° ëª©ì ìœ¼ë¡œ **1) Strands ì—ì´ì „íŠ¸ë¥¼ ë¡œì»¬ì—ì„œ í˜¸ìŠ¤íŒ…í•˜ëŠ” ì˜ˆì œ**ë¥¼ ë¨¼ì € ì‚´í´ë³´ê³ , **2) ê°™ì€ ì—ì´ì „íŠ¸ë¥¼ AgentCore Runtimeì— ì˜¬ë¦¬ëŠ” ì˜ˆì œ**ë¥¼ ì´ì–´ì„œ ë³´ë©° ë‘ ë°©ì‹ì„ ë¹„êµí•´ë³´ë„ë¡ í•˜ê² ìŠµë‹ˆë‹¤.\n",
    "\n",
    "\n",
    "| ì •ë³´                | ì„¸ë¶€ì‚¬í•­                                                                      |\n",
    "|:--------------------|:-----------------------------------------------------------------------------|\n",
    "| ì—ì´ì „íŠ¸ ìœ í˜•        | ë©€í‹° ì—ì´ì „íŠ¸ (Planner, Supervisor, Coder, Reporter)                                                                         |\n",
    "| ì—ì´ì „í‹± í”„ë ˆì„ì›Œí¬   | Strands                                                          |\n",
    "| LLM ëª¨ë¸            | Anthropic Claude Sonnet 3.7                                                    |\n",
    "| íŠœí† ë¦¬ì–¼ êµ¬ì„±ìš”ì†Œ    | AgentCore Runtimeì—ì„œ ì—ì´ì „íŠ¸ í˜¸ìŠ¤íŒ…. Strands SDK ë° Amazon Bedrock ëª¨ë¸ ì‚¬ìš© |\n",
    "| íŠœí† ë¦¬ì–¼ ë¶„ì•¼        | ë²”ìš©                                                                         |\n",
    "| ì˜ˆì œ ë³µì¡ë„         | ì‰¬ì›€                                                                         |\n",
    "| ì‚¬ìš©ëœ SDK          | Amazon BedrockAgentCore Python SDK ë° boto3                                 |\n",
    "\n",
    "\n",
    "### ëª©ì°¨\n",
    "\n",
    "#### 1) ë¡œì»¬ì—ì„œ ì—ì´ì „íŠ¸ ì‹¤í–‰/í˜¸ì¶œí•˜ê¸°\n",
    "AgentCore Runtime ë°°í¬ ì „ ë¡œì»¬ í™˜ê²½ì—ì„œì˜ ê°œë°œ ë° í…ŒìŠ¤íŠ¸ ê³¼ì •\n",
    "\n",
    "- **Step 1.** ì—ì´ì „íŠ¸ ì½”ë“œ ì‘ì„±\n",
    "- **Step 2.** ë¡œì»¬ ì‹¤í–‰ í›„ í…ŒìŠ¤íŠ¸\n",
    "\n",
    "#### 2) AgentCore Runtimeì— ì—ì´ì „íŠ¸ ë°°í¬ í›„ í˜¸ì¶œí•˜ê¸°\n",
    "Amazon Bedrock AgentCore starter toolkitì„ í™œìš©í•œ í”„ë¡œë•ì…˜ ë°°í¬ ê³¼ì •\n",
    "\n",
    "- **Step 1.** AgentCore Runtimeìš© ì—ì´ì „íŠ¸ ì½”ë“œ ì‘ì„±\n",
    "- **Step 2.** ë°°í¬ ì „ ì„¤ì •: agentcore_runtime.configure()\n",
    "- **Step 3.** ë°°í¬: agentcore_runtime.launch()\n",
    "- **Step 4.** í˜¸ì¶œ: agentcore_runtime.invoke()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a676f58ecf52b42",
   "metadata": {},
   "source": [
    "## ì‚¬ì „ ì¤€ë¹„ì‚¬í•­\n",
    "* Python 3.11+\n",
    "* AWS credentials\n",
    "* Amazon Bedrock AgentCore SDK\n",
    "* Docker running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "df2e9bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b8a5e570",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "module_path = \"../../../..\"\n",
    "sys.path.append(os.path.abspath(module_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca924a7a2731e26f",
   "metadata": {},
   "source": [
    "# 1) Localì—ì„œ ì—ì´ì „íŠ¸ ì‹¤í–‰/í˜¸ì¶œí•˜ê¸° \n",
    "\n",
    "AgentCore Runtimeì— ì—ì´ì „íŠ¸ë¥¼ ë°°í¬í•˜ê¸° ì „ì—, ì‹œì—° ëª©ì ìœ¼ë¡œ Bedrock Manusë¥¼ ë¡œì»¬ì—ì„œ ê°œë°œí•˜ê³  ì‹¤í–‰í•˜ëŠ” ê³¼ì •ì„ ë³´ê² ìŠµë‹ˆë‹¤.\n",
    "\n",
    "- Step 1. ì—ì´ì „íŠ¸ ì½”ë“œ ì‘ì„±\n",
    "- Step 2. ë¡œì»¬ ì‹¤í–‰ í›„ í…ŒìŠ¤íŠ¸ \n",
    "\n",
    "### Step 1. ì—ì´ì „íŠ¸ ì½”ë“œ ì‘ì„±\n",
    "ë¨¼ì € Strands SDKë¡œ ì‘ì„±ëœ Bedrock Manus ì—ì´ì „íŠ¸ì˜ í•µì‹¬ ë©”ì¸ ì½”ë“œë¥¼ ì‚´í´ë³´ê² ìŠµë‹ˆë‹¤.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "61f88ac9-2fda-4d44-9d64-358e3cdfa3c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting local_agent.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile local_agent.py\n",
    "\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "import asyncio\n",
    "import argparse\n",
    "import json\n",
    "from src.graph.builder import build_graph\n",
    "from src.utils.strands_sdk_utils import strands_utils\n",
    "\n",
    "# Import event queue for unified event processing\n",
    "from src.utils.event_queue import clear_queue\n",
    "\n",
    "def remove_artifact_folder(folder_path=\"./artifacts/\"): # ./artifact/ í´ë”ê°€ ì¡´ì¬í•˜ë©´ ì‚­ì œí•˜ëŠ” í•¨ìˆ˜\n",
    "    if os.path.exists(folder_path):\n",
    "        print(f\"'{folder_path}' í´ë”ë¥¼ ì‚­ì œí•©ë‹ˆë‹¤...\")\n",
    "        try:\n",
    "            shutil.rmtree(folder_path)\n",
    "            print(f\"'{folder_path}' í´ë”ê°€ ì„±ê³µì ìœ¼ë¡œ ì‚­ì œë˜ì—ˆìŠµë‹ˆë‹¤.\")\n",
    "        except Exception as e: \n",
    "            print(f\"ì˜¤ë¥˜ ë°œìƒ: {e}\")\n",
    "    else:\n",
    "        print(f\"'{folder_path}' í´ë”ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.\")\n",
    "\n",
    "def _setup_execution():\n",
    "    \"\"\"Initialize execution environment\"\"\"\n",
    "    remove_artifact_folder()\n",
    "    clear_queue()\n",
    "    print(\"\\n=== Starting Local Runtime Event Stream ===\")\n",
    "\n",
    "def _print_conversation_history():\n",
    "    \"\"\"Print final conversation history\"\"\"\n",
    "    print(\"\\n=== Conversation History ===\")\n",
    "    from src.graph.nodes import _global_node_states\n",
    "    shared_state = _global_node_states.get('shared', {})\n",
    "    history = shared_state.get('history', [])\n",
    "\n",
    "    if history:\n",
    "        for hist_item in history:\n",
    "            print(f\"[{hist_item['agent']}] {hist_item['message']}\")\n",
    "    else:\n",
    "        print(\"No conversation history found\")\n",
    "\n",
    "async def graph_streaming_execution(payload):\n",
    "    \"\"\"\n",
    "    Execute full graph streaming workflow in local environment\n",
    "    Direct event processing without AgentCore API\n",
    "    \"\"\"\n",
    "    # Get user query from payload\n",
    "    user_query = payload.get(\"prompt\", \"\")\n",
    "\n",
    "    if not user_query:\n",
    "        # Use default query if none provided\n",
    "        user_query = \"ë„ˆê°€ ì‘ì„±í•  ê²ƒì€ moon market ì˜ íŒë§¤ í˜„í™© ë³´ê³ ì„œì•¼. ì„¸ì¼ì¦ˆ ë° ë§ˆì¼€íŒ… ê´€ì ìœ¼ë¡œ ë¶„ì„ì„ í•´ì£¼ê³ , ì°¨íŠ¸ ìƒì„± ë° ì¸ì‚¬ì´íŠ¸ë„ ë½‘ì•„ì„œ pdf íŒŒì¼ë¡œ ë§Œë“¤ì–´ì¤˜. ë¶„ì„ëŒ€ìƒì€ './data/Dat-fresh-food-claude.csv' íŒŒì¼ ì…ë‹ˆë‹¤. Coder ì—ì´ì „íŠ¸ê°€ í• ì¼ì€ ìµœëŒ€í•œ ì‘ê²Œ í•´ì¤˜. ì™œëƒí•˜ë©´ reporter ì—ì´ì „íŠ¸ í…ŒìŠ¤íŠ¸ ì¤‘ì´ë¼ ë¹¨ë¦¬ ì½”ë” ë‹¨ê³„ë¥¼ ë„˜ì–´ ê°€ì•¼ í•˜ê±°ë“ . ë¶€íƒí•´.\"\n",
    "\n",
    "    _setup_execution()\n",
    "\n",
    "    # Build graph and use stream_async method\n",
    "    graph = build_graph()\n",
    "    event_count = 0\n",
    "    events_list = []\n",
    "\n",
    "    # Stream events from graph execution\n",
    "    async for event in graph.stream_async({\n",
    "        \"request\": user_query,\n",
    "        \"request_prompt\": f\"Here is a user request: <user_request>{user_query}</user_request>\"\n",
    "    }):\n",
    "        event_count += 1\n",
    "        # Add local runtime metadata\n",
    "        event[\"event_id\"] = event_count\n",
    "        event[\"runtime_source\"] = \"bedrock_manus_local\"\n",
    "\n",
    "        # Print event for local debugging\n",
    "        print(f\"Event {event_count}: {event.get('type', 'unknown')}\")\n",
    "\n",
    "        # Store events for final processing\n",
    "        events_list.append(event)\n",
    "\n",
    "        # Mark final event\n",
    "        if event.get(\"type\") == \"workflow_complete\":\n",
    "            event[\"total_events\"] = event_count\n",
    "            event[\"message\"] = \"All events processed locally without AgentCore Runtime\"\n",
    "\n",
    "    _print_conversation_history()\n",
    "    print(\"=== Local Runtime Event Stream Complete ===\")\n",
    "\n",
    "    # Return final result for local execution\n",
    "    return {\n",
    "        \"total_events\": event_count,\n",
    "        \"final_message\": \"Local execution completed successfully\",\n",
    "        \"events\": events_list\n",
    "    }\n",
    "\n",
    "def main_local_execution(payload):\n",
    "    \"\"\"\n",
    "    Main function for local execution\n",
    "    Synchronous wrapper for async graph execution\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Run async function in event loop\n",
    "        result = asyncio.run(graph_streaming_execution(payload))\n",
    "        return result\n",
    "    except Exception as e:\n",
    "        print(f\"Local execution error: {e}\")\n",
    "        return {\"error\": str(e)}\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser(description=\"Bedrock Manus Multi-Agent Local Runtime\")\n",
    "    parser.add_argument(\"payload\", type=str, help=\"JSON payload with prompt\")\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    try:\n",
    "        payload = json.loads(args.payload)\n",
    "        result = main_local_execution(payload)\n",
    "        print(\"\\n=== Final Result ===\")\n",
    "        print(json.dumps(result, indent=2, ensure_ascii=False))\n",
    "    except json.JSONDecodeError:\n",
    "        print(\"Error: Invalid JSON payload\")\n",
    "    except Exception as e:\n",
    "        print(f\"Execution error: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b41ce8bc-7177-42e6-8faa-aca57eb7256f",
   "metadata": {},
   "source": [
    "### Step 2. ë¡œì»¬ ì‹¤í–‰ ë° í…ŒìŠ¤íŠ¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3401b0b3-4545-47b3-a6fa-09a61e8de677",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'./artifacts/' í´ë”ë¥¼ ì‚­ì œí•©ë‹ˆë‹¤...\n",
      "'./artifacts/' í´ë”ê°€ ì„±ê³µì ìœ¼ë¡œ ì‚­ì œë˜ì—ˆìŠµë‹ˆë‹¤.\n",
      "\n",
      "=== Starting Local Runtime Event Stream ===\n",
      "WARNING:strands.multiagent.graph:Graph without execution limits may run indefinitely if cycles exist\n",
      "\n",
      "INFO:src.graph.nodes:\u001b[92m===== Coordinator started =====\u001b[0m\n",
      "INFO:src.utils.strands_sdk_utils:\u001b[92mCOORDINATOR - Prompt Cache Disabled\u001b[0m\n",
      "Event 1: agent_text_stream\n",
      "Event 2: agent_text_stream\n",
      "Event 3: agent_text_stream\n",
      "Event 4: agent_text_stream\n",
      "WARNING:src.utils.agentcore_observability:Invalid or non-recording span for event: input_message\n",
      "WARNING:src.utils.agentcore_observability:Invalid or non-recording span for event: response\n",
      "\n",
      "INFO:src.graph.nodes:\u001b[92m===== Coordinator completed =====\u001b[0m\n",
      "WARNING:src.utils.agentcore_observability:Invalid or non-recording span for event: input_message\n",
      "WARNING:src.utils.agentcore_observability:Invalid or non-recording span for event: response\n",
      "WARNING:src.utils.agentcore_observability:Invalid or non-recording span for event: input_message\n",
      "WARNING:src.utils.agentcore_observability:Invalid or non-recording span for event: response\n",
      "\n",
      "INFO:src.graph.nodes:\u001b[92m===== Planner started =====\u001b[0m\n",
      "INFO:src.utils.strands_sdk_utils:\u001b[92mPLANNER - Prompt Cache Disabled\u001b[0m\n",
      "Event 5: agent_reasoning_stream\n",
      "Event 6: agent_reasoning_stream\n",
      "Event 7: agent_reasoning_stream\n",
      "Event 8: agent_reasoning_stream\n",
      "Event 9: agent_reasoning_stream\n",
      "Event 10: agent_reasoning_stream\n",
      "Event 11: agent_reasoning_stream\n",
      "Event 12: agent_reasoning_stream\n",
      "Event 13: agent_reasoning_stream\n",
      "Event 14: agent_reasoning_stream\n",
      "Event 15: agent_reasoning_stream\n",
      "Event 16: agent_reasoning_stream\n",
      "Event 17: agent_reasoning_stream\n",
      "Event 18: agent_reasoning_stream\n",
      "Event 19: agent_reasoning_stream\n",
      "Event 20: agent_reasoning_stream\n",
      "Event 21: agent_reasoning_stream\n",
      "Event 22: agent_reasoning_stream\n",
      "Event 23: agent_reasoning_stream\n",
      "Event 24: agent_reasoning_stream\n",
      "Event 25: agent_reasoning_stream\n",
      "Event 26: agent_reasoning_stream\n",
      "Event 27: agent_reasoning_stream\n",
      "Event 28: agent_reasoning_stream\n",
      "Event 29: agent_reasoning_stream\n",
      "Event 30: agent_reasoning_stream\n",
      "Event 31: agent_reasoning_stream\n",
      "Event 32: agent_reasoning_stream\n",
      "Event 33: agent_reasoning_stream\n",
      "Event 34: agent_reasoning_stream\n",
      "Event 35: agent_reasoning_stream\n",
      "Event 36: agent_reasoning_stream\n",
      "Event 37: agent_reasoning_stream\n",
      "Event 38: agent_reasoning_stream\n",
      "Event 39: agent_reasoning_stream\n",
      "Event 40: agent_reasoning_stream\n",
      "Event 41: agent_reasoning_stream\n",
      "Event 42: agent_reasoning_stream\n",
      "Event 43: agent_reasoning_stream\n",
      "Event 44: agent_reasoning_stream\n",
      "Event 45: agent_reasoning_stream\n",
      "Event 46: agent_reasoning_stream\n",
      "Event 47: agent_reasoning_stream\n",
      "Event 48: agent_reasoning_stream\n",
      "Event 49: agent_reasoning_stream\n",
      "Event 50: agent_reasoning_stream\n",
      "Event 51: agent_reasoning_stream\n",
      "Event 52: agent_reasoning_stream\n",
      "Event 53: agent_reasoning_stream\n",
      "Event 54: agent_reasoning_stream\n",
      "Event 55: agent_reasoning_stream\n",
      "Event 56: agent_reasoning_stream\n",
      "Event 57: agent_reasoning_stream\n",
      "Event 58: agent_reasoning_stream\n",
      "Event 59: agent_reasoning_stream\n",
      "Event 60: agent_reasoning_stream\n",
      "Event 61: agent_reasoning_stream\n",
      "Event 62: agent_reasoning_stream\n",
      "Event 63: agent_reasoning_stream\n",
      "Event 64: agent_reasoning_stream\n",
      "Event 65: agent_reasoning_stream\n",
      "Event 66: agent_reasoning_stream\n",
      "Event 67: agent_reasoning_stream\n",
      "Event 68: agent_reasoning_stream\n",
      "Event 69: agent_reasoning_stream\n",
      "Event 70: agent_reasoning_stream\n",
      "Event 71: agent_reasoning_stream\n",
      "Event 72: agent_reasoning_stream\n",
      "Event 73: agent_reasoning_stream\n",
      "Event 74: agent_reasoning_stream\n",
      "Event 75: agent_reasoning_stream\n",
      "Event 76: agent_reasoning_stream\n",
      "Event 77: agent_reasoning_stream\n",
      "Event 78: agent_reasoning_stream\n",
      "Event 79: agent_reasoning_stream\n",
      "Event 80: agent_reasoning_stream\n",
      "Event 81: agent_reasoning_stream\n",
      "Event 82: agent_reasoning_stream\n",
      "Event 83: agent_reasoning_stream\n",
      "Event 84: agent_reasoning_stream\n",
      "Event 85: agent_reasoning_stream\n",
      "Event 86: agent_reasoning_stream\n",
      "Event 87: agent_reasoning_stream\n",
      "Event 88: agent_reasoning_stream\n",
      "Event 89: agent_reasoning_stream\n",
      "Event 90: agent_reasoning_stream\n",
      "Event 91: agent_reasoning_stream\n",
      "Event 92: agent_reasoning_stream\n",
      "Event 93: agent_reasoning_stream\n",
      "Event 94: agent_text_stream\n",
      "Event 95: agent_text_stream\n",
      "Event 96: agent_text_stream\n",
      "Event 97: agent_text_stream\n",
      "Event 98: agent_text_stream\n",
      "Event 99: agent_text_stream\n",
      "Event 100: agent_text_stream\n",
      "Event 101: agent_text_stream\n",
      "Event 102: agent_text_stream\n",
      "Event 103: agent_text_stream\n",
      "Event 104: agent_text_stream\n",
      "Event 105: agent_text_stream\n",
      "Event 106: agent_text_stream\n",
      "Event 107: agent_text_stream\n",
      "Event 108: agent_text_stream\n",
      "Event 109: agent_text_stream\n",
      "Event 110: agent_text_stream\n",
      "Event 111: agent_text_stream\n",
      "Event 112: agent_text_stream\n",
      "Event 113: agent_text_stream\n",
      "Event 114: agent_text_stream\n",
      "Event 115: agent_text_stream\n",
      "Event 116: agent_text_stream\n",
      "Event 117: agent_text_stream\n",
      "Event 118: agent_text_stream\n",
      "Event 119: agent_text_stream\n",
      "Event 120: agent_text_stream\n",
      "Event 121: agent_text_stream\n",
      "Event 122: agent_text_stream\n",
      "Event 123: agent_text_stream\n",
      "Event 124: agent_text_stream\n",
      "Event 125: agent_text_stream\n",
      "Event 126: agent_text_stream\n",
      "Event 127: agent_text_stream\n",
      "Event 128: agent_text_stream\n",
      "Event 129: agent_text_stream\n",
      "Event 130: agent_text_stream\n",
      "Event 131: agent_text_stream\n",
      "Event 132: agent_text_stream\n",
      "Event 133: agent_text_stream\n",
      "Event 134: agent_text_stream\n",
      "Event 135: agent_text_stream\n",
      "Event 136: agent_text_stream\n",
      "Event 137: agent_text_stream\n",
      "Event 138: agent_text_stream\n",
      "Event 139: agent_text_stream\n",
      "Event 140: agent_text_stream\n",
      "Event 141: agent_text_stream\n",
      "Event 142: agent_text_stream\n",
      "Event 143: agent_text_stream\n",
      "Event 144: agent_text_stream\n",
      "Event 145: agent_text_stream\n",
      "Event 146: agent_text_stream\n",
      "Event 147: agent_text_stream\n",
      "Event 148: agent_text_stream\n",
      "Event 149: agent_text_stream\n",
      "Event 150: agent_text_stream\n",
      "Event 151: agent_text_stream\n",
      "Event 152: agent_text_stream\n",
      "Event 153: agent_text_stream\n",
      "Event 154: agent_text_stream\n",
      "Event 155: agent_text_stream\n",
      "Event 156: agent_text_stream\n",
      "Event 157: agent_text_stream\n",
      "Event 158: agent_text_stream\n",
      "Event 159: agent_text_stream\n",
      "Event 160: agent_text_stream\n",
      "Event 161: agent_text_stream\n",
      "Event 162: agent_text_stream\n",
      "Event 163: agent_text_stream\n",
      "Event 164: agent_text_stream\n",
      "Event 165: agent_text_stream\n",
      "Event 166: agent_text_stream\n",
      "Event 167: agent_text_stream\n",
      "Event 168: agent_text_stream\n",
      "Event 169: agent_text_stream\n",
      "Event 170: agent_text_stream\n",
      "Event 171: agent_text_stream\n",
      "Event 172: agent_text_stream\n",
      "Event 173: agent_text_stream\n",
      "Event 174: agent_text_stream\n",
      "Event 175: agent_text_stream\n",
      "Event 176: agent_text_stream\n",
      "Event 177: agent_text_stream\n",
      "Event 178: agent_text_stream\n",
      "Event 179: agent_text_stream\n",
      "Event 180: agent_text_stream\n",
      "Event 181: agent_text_stream\n",
      "Event 182: agent_text_stream\n",
      "Event 183: agent_text_stream\n",
      "Event 184: agent_text_stream\n",
      "Event 185: agent_text_stream\n",
      "Event 186: agent_text_stream\n",
      "Event 187: agent_text_stream\n",
      "Event 188: agent_text_stream\n",
      "Event 189: agent_text_stream\n",
      "Event 190: agent_text_stream\n",
      "Event 191: agent_text_stream\n",
      "Event 192: agent_text_stream\n",
      "Event 193: agent_text_stream\n",
      "Event 194: agent_text_stream\n",
      "Event 195: agent_text_stream\n",
      "Event 196: agent_text_stream\n",
      "Event 197: agent_text_stream\n",
      "Event 198: agent_text_stream\n",
      "Event 199: agent_text_stream\n",
      "Event 200: agent_text_stream\n",
      "Event 201: agent_text_stream\n",
      "Event 202: agent_text_stream\n",
      "Event 203: agent_text_stream\n",
      "Event 204: agent_text_stream\n",
      "Event 205: agent_text_stream\n",
      "Event 206: agent_text_stream\n",
      "Event 207: agent_text_stream\n",
      "Event 208: agent_text_stream\n",
      "Event 209: agent_text_stream\n",
      "Event 210: agent_text_stream\n",
      "Event 211: agent_text_stream\n",
      "Event 212: agent_text_stream\n",
      "Event 213: agent_text_stream\n",
      "Event 214: agent_text_stream\n",
      "Event 215: agent_text_stream\n",
      "Event 216: agent_text_stream\n",
      "Event 217: agent_text_stream\n",
      "Event 218: agent_text_stream\n",
      "Event 219: agent_text_stream\n",
      "Event 220: agent_text_stream\n",
      "Event 221: agent_text_stream\n",
      "Event 222: agent_text_stream\n",
      "Event 223: agent_text_stream\n",
      "Event 224: agent_text_stream\n",
      "Event 225: agent_text_stream\n",
      "Event 226: agent_text_stream\n",
      "Event 227: agent_text_stream\n",
      "Event 228: agent_text_stream\n",
      "Event 229: agent_text_stream\n",
      "Event 230: agent_text_stream\n",
      "Event 231: agent_text_stream\n",
      "Event 232: agent_text_stream\n",
      "Event 233: agent_text_stream\n",
      "Event 234: agent_text_stream\n",
      "Event 235: agent_text_stream\n",
      "Event 236: agent_text_stream\n",
      "Event 237: agent_text_stream\n",
      "Event 238: agent_text_stream\n",
      "Event 239: agent_text_stream\n",
      "Event 240: agent_text_stream\n",
      "Event 241: agent_text_stream\n",
      "Event 242: agent_text_stream\n",
      "Event 243: agent_text_stream\n",
      "Event 244: agent_text_stream\n",
      "Event 245: agent_text_stream\n",
      "Event 246: agent_text_stream\n",
      "Event 247: agent_text_stream\n",
      "Event 248: agent_text_stream\n",
      "Event 249: agent_text_stream\n",
      "Event 250: agent_text_stream\n",
      "Event 251: agent_text_stream\n",
      "Event 252: agent_text_stream\n",
      "Event 253: agent_text_stream\n",
      "Event 254: agent_text_stream\n",
      "Event 255: agent_text_stream\n",
      "Event 256: agent_text_stream\n",
      "Event 257: agent_text_stream\n",
      "Event 258: agent_text_stream\n",
      "Event 259: agent_text_stream\n",
      "Event 260: agent_text_stream\n",
      "Event 261: agent_text_stream\n",
      "Event 262: agent_text_stream\n",
      "Event 263: agent_text_stream\n",
      "Event 264: agent_text_stream\n",
      "Event 265: agent_text_stream\n",
      "Event 266: agent_text_stream\n",
      "Event 267: agent_text_stream\n",
      "Event 268: agent_text_stream\n",
      "Event 269: agent_text_stream\n",
      "Event 270: agent_text_stream\n",
      "Event 271: agent_text_stream\n",
      "Event 272: agent_text_stream\n",
      "WARNING:src.utils.agentcore_observability:Invalid or non-recording span for event: input_message\n",
      "WARNING:src.utils.agentcore_observability:Invalid or non-recording span for event: response\n",
      "\n",
      "INFO:src.graph.nodes:\u001b[92m===== Planner completed =====\u001b[0m\n",
      "\n",
      "INFO:src.graph.nodes:\u001b[92m===== Supervisor started =====\u001b[0m\n",
      "INFO:src.utils.strands_sdk_utils:\u001b[92mSUPERVISOR - Prompt Cache Enabled\u001b[0m\n",
      "Event 273: agent_text_stream\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "# ì‹¤í–‰ ì˜ˆì‹œ:\n",
    "!python local_agent.py '{\"prompt\": \"moon market íŒë§¤ í˜„í™© ë³´ê³ ì„œë¥¼ ì‘ì„±í•´ì£¼ì„¸ìš”.\"}'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "932110e6-fca6-47b6-b7c5-c4714a866a80",
   "metadata": {},
   "source": [
    "\n",
    "- - - \n",
    "# 2) AgentCore Runtimeì— ì—ì´ì „íŠ¸ ë°°í¬ í›„ í˜¸ì¶œí•˜ê¸° \n",
    "\n",
    "ì´ì œ ìœ„ Bedrock Manusë¥¼ AgentCore Runtimeì— ë°°í¬í•´ë³´ê² ìŠµë‹ˆë‹¤.    \n",
    "\n",
    "[Amazon Bedrock AgentCore starter toolkit](https://docs.aws.amazon.com/bedrock-agentcore/latest/devguide/getting-started-starter-toolkit.html)ì„ ì´ìš©í•´ ì—ì´ì „íŠ¸ë¥¼ AgentCore ëŸ°íƒ€ì„ì— ì‰½ê²Œ ë°°í¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì•„ë˜ 4ê°œì˜ Stepì„ ë”°ë¼ê°€ì‹œë©´, Bedrock Manusë¥¼ AgentCore Runtimeì—ì„œ í…ŒìŠ¤íŠ¸í•´ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. \n",
    "\n",
    "- Step 1. AgentCore Runtimeìš© ì—ì´ì „íŠ¸ ì½”ë“œ ì‘ì„±\n",
    "- Step 2. ë°°í¬ ì „ ì„¤ì •: `agentcore_runtime.configure()`\n",
    "- Step 3. ë°°í¬: `agentcore_runtime.launch`\n",
    "- Step 4. í˜¸ì¶œ: `agentcore_runtime.invoke()`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2cf2dd6-480c-4bb5-8805-e8e3ba348e68",
   "metadata": {},
   "source": [
    "\n",
    "### Step 1. AgentCore Runtimeìš© ì—ì´ì „íŠ¸ ì½”ë“œ ì‘ì„±\n",
    "ê¸°ì¡´ ì½”ë“œë¥¼ AgentCore Runtime ìš© ì—ì´ì „íŠ¸ë¡œ ë³€ê²½í•˜ëŠ” ê²ƒì€ ì–´ë µì§€ ì•ŠìŠµë‹ˆë‹¤. ìœ„ì˜ ì½”ë“œì—ì„œ ì•„ë˜ 4ê°€ì§€ë§Œ ì¶”ê°€í•´ì£¼ë©´ ë©ë‹ˆë‹¤:\n",
    "\n",
    "- `from bedrock_agentcore.runtime import BedrockAgentCoreApp`ë¡œ Runtime App ê°€ì ¸ì˜¤ê¸°\n",
    "- ì½”ë“œì—ì„œ `app = BedrockAgentCoreApp()`ë¡œ App ì´ˆê¸°í™”\n",
    "- í˜¸ì¶œí•  ë©”ì¸ í•¨ìˆ˜ë¥¼ `@app.entrypoint` ë°ì½”ë ˆì´í„°ë¡œ ì¥ì‹\n",
    "- `app.run()`ìœ¼ë¡œ AgentCoreRuntimeì´ ì—ì´ì „íŠ¸ ì‹¤í–‰ì„ ì œì–´í•˜ë„ë¡ í•˜ê¸°\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f0261f0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing agentcore_runtime.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile agentcore_runtime.py\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "import asyncio\n",
    "from bedrock_agentcore.runtime import BedrockAgentCoreApp\n",
    "from src.graph.builder import build_graph\n",
    "from src.utils.strands_sdk_utils import strands_utils\n",
    "\n",
    "# Import event queue for unified event processing\n",
    "from src.utils.event_queue import clear_queue\n",
    "\n",
    "app = BedrockAgentCoreApp()\n",
    "\n",
    "def remove_artifact_folder(folder_path=\"./artifacts/\"): # ./artifact/ í´ë”ê°€ ì¡´ì¬í•˜ë©´ ì‚­ì œí•˜ëŠ” í•¨ìˆ˜\n",
    "    if os.path.exists(folder_path):\n",
    "        print(f\"'{folder_path}' í´ë”ë¥¼ ì‚­ì œí•©ë‹ˆë‹¤...\")\n",
    "        try:\n",
    "            shutil.rmtree(folder_path)\n",
    "            print(f\"'{folder_path}' í´ë”ê°€ ì„±ê³µì ìœ¼ë¡œ ì‚­ì œë˜ì—ˆìŠµë‹ˆë‹¤.\")\n",
    "        except Exception as e: \n",
    "            print(f\"ì˜¤ë¥˜ ë°œìƒ: {e}\")\n",
    "    else:\n",
    "        print(f\"'{folder_path}' í´ë”ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.\")\n",
    "\n",
    "def _setup_execution():\n",
    "    \"\"\"Initialize execution environment\"\"\"\n",
    "    remove_artifact_folder()\n",
    "    clear_queue()\n",
    "    print(\"\\n=== Starting AgentCore Runtime Event Stream ===\")\n",
    "\n",
    "def _print_conversation_history():\n",
    "    \"\"\"Print final conversation history\"\"\"\n",
    "    print(\"\\n=== Conversation History ===\")\n",
    "    from src.graph.nodes import _global_node_states\n",
    "    shared_state = _global_node_states.get('shared', {})\n",
    "    history = shared_state.get('history', [])\n",
    "    \n",
    "    if history:\n",
    "        for hist_item in history:\n",
    "            print(f\"[{hist_item['agent']}] {hist_item['message']}\")\n",
    "    else:\n",
    "        print(\"No conversation history found\")\n",
    "\n",
    "@app.entrypoint\n",
    "async def graph_streaming_execution(payload):\n",
    "    \"\"\"\n",
    "    Execute full graph streaming workflow through AgentCore Runtime\n",
    "    Queue-only event processing compatible with AgentCore API\n",
    "    \"\"\"\n",
    "    # Get user query from payload\n",
    "    user_query = payload.get(\"prompt\", \"\")\n",
    "    \n",
    "    if not user_query:\n",
    "        # Use default query if none provided\n",
    "        user_query = \"ë„ˆê°€ ì‘ì„±í•  ê²ƒì€ moon market ì˜ íŒë§¤ í˜„í™© ë³´ê³ ì„œì•¼. ì„¸ì¼ì¦ˆ ë° ë§ˆì¼€íŒ… ê´€ì ìœ¼ë¡œ ë¶„ì„ì„ í•´ì£¼ê³ , ì°¨íŠ¸ ìƒì„± ë° ì¸ì‚¬ì´íŠ¸ë„ ë½‘ì•„ì„œ pdf íŒŒì¼ë¡œ ë§Œë“¤ì–´ì¤˜. ë¶„ì„ëŒ€ìƒì€ './data/Dat-fresh-food-claude.csv' íŒŒì¼ ì…ë‹ˆë‹¤. Coder ì—ì´ì „íŠ¸ê°€ í• ì¼ì€ ìµœëŒ€í•œ ì‘ê²Œ í•´ì¤˜. ì™œëƒí•˜ë©´ reporter ì—ì´ì „íŠ¸ í…ŒìŠ¤íŠ¸ ì¤‘ì´ë¼ ë¹¨ë¦¬ ì½”ë” ë‹¨ê³„ë¥¼ ë„˜ì–´ ê°€ì•¼ í•˜ê±°ë“ . ë¶€íƒí•´.\"\n",
    "    \n",
    "    _setup_execution()\n",
    "    \n",
    "    # Build graph and use stream_async method\n",
    "    graph = build_graph()\n",
    "    event_count = 0\n",
    "    \n",
    "    # Stream events from graph execution\n",
    "    async for event in graph.stream_async({\n",
    "        \"request\": user_query,\n",
    "        \"request_prompt\": f\"Here is a user request: <user_request>{user_query}</user_request>\"\n",
    "    }):\n",
    "        event_count += 1\n",
    "        # Add AgentCore runtime metadata\n",
    "        event[\"event_id\"] = event_count\n",
    "        event[\"runtime_source\"] = \"bedrock_manus_agentcore\"\n",
    "        \n",
    "        # Mark final event\n",
    "        if event.get(\"type\") == \"workflow_complete\":\n",
    "            event[\"total_events\"] = event_count\n",
    "            event[\"message\"] = \"All events processed through global queue via AgentCore Runtime\"\n",
    "        \n",
    "        yield event\n",
    "    \n",
    "    _print_conversation_history()\n",
    "    print(\"=== AgentCore Runtime Event Stream Complete ===\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    app.run()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c64db7b5-0f1b-475f-9bf2-467b4449d46a",
   "metadata": {},
   "source": [
    "**AgentCore Runtimeì— ë°°í¬í•  ì—ì´ì „íŠ¸ê°€ ì¤€ë¹„ë˜ì—ˆìŠµë‹ˆë‹¤!** (agentcore_runtime.py) \n",
    "\n",
    "ì´ì œ ì´ ì—ì´ì „íŠ¸ë¥¼ í´ë¼ìš°ë“œ í™˜ê²½ì— ë°°í¬í•´ë³¼ê¹Œìš”?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef885f7a-7a4c-4a54-88e8-e546533a087e",
   "metadata": {},
   "source": [
    "\n",
    "### Step 2. IAM Role ì„¤ì • ë° ë°°í¬ êµ¬ì„±: `agentcore_runtime.configure()`  \n",
    "\n",
    "ë¨¼ì € AgentCore Runtimeì´ ì‚¬ìš©í•  IAM Roleì„ ë§Œë“¤ê³ ,    \n",
    "ë‹¤ìŒìœ¼ë¡œ starter toolkitì„ ì‚¬ìš©í•˜ì—¬ ì§„ì…ì  (entrypoint)ê³¼ ìƒì„±í•œ IAM Role, ê·¸ë¦¬ê³  íŒ¨í‚¤ì§€ ì„¤ì¹˜ë¥¼ ìœ„í•œ requirements íŒŒì¼ì„ ì—°ê²°í•˜ì—¬ AgentCore Runtime ë°°í¬ë¥¼ êµ¬ì„±í•˜ê² ìŠµë‹ˆë‹¤. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "54dd2fdf-985c-4a70-8b87-071783a209de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sys.path[0]: /home/ec2-user/SageMaker/aws-ai-ml-workshop-kr/genai/aws-gen-ai-kr/20_applications\n",
      "attaching role policy agentcore-bedrock_manus_runtime-role\n"
     ]
    }
   ],
   "source": [
    "# IAM Role ìƒì„±\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "current_dir = os.path.dirname(os.path.abspath('__file__' if '__file__' in globals() else '.'))\n",
    "utils_dir = os.path.join(current_dir, '..')\n",
    "utils_dir = os.path.join(utils_dir, '..')\n",
    "utils_dir = os.path.abspath(utils_dir)\n",
    "\n",
    "sys.path.insert(0, utils_dir)\n",
    "print(\"sys.path[0]:\", sys.path[0])\n",
    "\n",
    "from src.utils.agentcore import create_agentcore_role\n",
    "\n",
    "agent_name=\"bedrock_manus_runtime\"\n",
    "agentcore_iam_role = create_agentcore_role(agent_name=agent_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2e79eba2-ca59-463f-9ebf-56e362d7ae66",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Entrypoint parsed: file=/home/ec2-user/SageMaker/aws-ai-ml-workshop-kr/genai/aws-gen-ai-kr/20_applications/08_bedrock_manus/use_cases/05_insight_extractor_strands_sdk_workshop_phase_2/runtime.py, bedrock_agentcore_name=runtime\n",
      "INFO:bedrock_agentcore_starter_toolkit.utils.runtime.entrypoint:Entrypoint parsed: file=/home/ec2-user/SageMaker/aws-ai-ml-workshop-kr/genai/aws-gen-ai-kr/20_applications/08_bedrock_manus/use_cases/05_insight_extractor_strands_sdk_workshop_phase_2/runtime.py, bedrock_agentcore_name=runtime\n",
      "Configuring BedrockAgentCore agent: bedrock_manus_runtime\n",
      "INFO:bedrock_agentcore_starter_toolkit.operations.runtime.configure:Configuring BedrockAgentCore agent: bedrock_manus_runtime\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold; text-decoration: underline\">âš ï¸  [WARNING] Platform mismatch: Current system is </span><span style=\"color: #008000; text-decoration-color: #008000; text-decoration: underline\">'linux/amd64'</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold; text-decoration: underline\"> but Bedrock AgentCore requires </span><span style=\"color: #008000; text-decoration-color: #008000; text-decoration: underline\">'linux/arm64'</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold; text-decoration: underline\">.</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold; text-decoration: underline\">For deployment options and workarounds, see: </span>\n",
       "<span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://docs.aws.amazon.com/bedrock-agentcore/latest/devguide/getting-started-custom.html</span>\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[1;4;33mâš ï¸  \u001b[0m\u001b[1;4;33m[\u001b[0m\u001b[1;4;33mWARNING\u001b[0m\u001b[1;4;33m]\u001b[0m\u001b[1;4;33m Platform mismatch: Current system is \u001b[0m\u001b[4;32m'linux/amd64'\u001b[0m\u001b[1;4;33m but Bedrock AgentCore requires \u001b[0m\u001b[4;32m'linux/arm64'\u001b[0m\u001b[1;4;33m.\u001b[0m\n",
       "\u001b[1;4;33mFor deployment options and workarounds, see: \u001b[0m\n",
       "\u001b[4;94mhttps://docs.aws.amazon.com/bedrock-agentcore/latest/devguide/getting-started-custom.html\u001b[0m\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generated Dockerfile: /home/ec2-user/SageMaker/aws-ai-ml-workshop-kr/genai/aws-gen-ai-kr/20_applications/08_bedrock_manus/use_cases/05_insight_extractor_strands_sdk_workshop_phase_2/Dockerfile\n",
      "INFO:bedrock_agentcore_starter_toolkit.operations.runtime.configure:Generated Dockerfile: /home/ec2-user/SageMaker/aws-ai-ml-workshop-kr/genai/aws-gen-ai-kr/20_applications/08_bedrock_manus/use_cases/05_insight_extractor_strands_sdk_workshop_phase_2/Dockerfile\n",
      "Generated .dockerignore: /home/ec2-user/SageMaker/aws-ai-ml-workshop-kr/genai/aws-gen-ai-kr/20_applications/08_bedrock_manus/use_cases/05_insight_extractor_strands_sdk_workshop_phase_2/.dockerignore\n",
      "INFO:bedrock_agentcore_starter_toolkit.operations.runtime.configure:Generated .dockerignore: /home/ec2-user/SageMaker/aws-ai-ml-workshop-kr/genai/aws-gen-ai-kr/20_applications/08_bedrock_manus/use_cases/05_insight_extractor_strands_sdk_workshop_phase_2/.dockerignore\n",
      "Changing default agent from 'manus_runtime_1147' to 'bedrock_manus_runtime'\n",
      "INFO:bedrock_agentcore_starter_toolkit.utils.runtime.config:Changing default agent from 'manus_runtime_1147' to 'bedrock_manus_runtime'\n",
      "Bedrock AgentCore configured: /home/ec2-user/SageMaker/aws-ai-ml-workshop-kr/genai/aws-gen-ai-kr/20_applications/08_bedrock_manus/use_cases/05_insight_extractor_strands_sdk_workshop_phase_2/.bedrock_agentcore.yaml\n",
      "INFO:bedrock_agentcore_starter_toolkit.notebook.runtime.bedrock_agentcore:Bedrock AgentCore configured: /home/ec2-user/SageMaker/aws-ai-ml-workshop-kr/genai/aws-gen-ai-kr/20_applications/08_bedrock_manus/use_cases/05_insight_extractor_strands_sdk_workshop_phase_2/.bedrock_agentcore.yaml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ConfigureResult(config_path=PosixPath('/home/ec2-user/SageMaker/aws-ai-ml-workshop-kr/genai/aws-gen-ai-kr/20_applications/08_bedrock_manus/use_cases/05_insight_extractor_strands_sdk_workshop_phase_2/.bedrock_agentcore.yaml'), dockerfile_path=PosixPath('/home/ec2-user/SageMaker/aws-ai-ml-workshop-kr/genai/aws-gen-ai-kr/20_applications/08_bedrock_manus/use_cases/05_insight_extractor_strands_sdk_workshop_phase_2/Dockerfile'), dockerignore_path=PosixPath('/home/ec2-user/SageMaker/aws-ai-ml-workshop-kr/genai/aws-gen-ai-kr/20_applications/08_bedrock_manus/use_cases/05_insight_extractor_strands_sdk_workshop_phase_2/.dockerignore'), runtime='Docker', region='us-west-2', account_id='654654304740', execution_role='arn:aws:iam::654654304740:role/agentcore-bedrock_manus_runtime-role', ecr_repository=None, auto_create_ecr=True)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from bedrock_agentcore_starter_toolkit import Runtime\n",
    "from boto3.session import Session\n",
    "boto_session = Session()\n",
    "region = boto_session.region_name\n",
    "region\n",
    "\n",
    "agentcore_runtime = Runtime()\n",
    "\n",
    "response = agentcore_runtime.configure(\n",
    "    agent_name=agent_name,\n",
    "    entrypoint=\"runtime.py\",\n",
    "    execution_role=agentcore_iam_role['Role']['Arn'],\n",
    "    auto_create_ecr=True,  # starter toolkitì´ ì‹œì‘ ì‹œ Amazon ECR ë¦¬í¬ì§€í† ë¦¬ë¥¼ ìë™ìœ¼ë¡œ ìƒì„±í•˜ë„ë¡ êµ¬ì„±\n",
    "    requirements_file=\"requirements.txt\",\n",
    "    region=region\n",
    ")\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a5fd644",
   "metadata": {},
   "source": [
    "#### ë” ì•Œì•„ë³´ê¸°\n",
    "**Signature:**\n",
    "```python\n",
    "agentcore_runtime.configure(\n",
    "    entrypoint: str,\n",
    "    execution_role: Optional[str] = None,\n",
    "    agent_name: Optional[str] = None,\n",
    "    requirements: Optional[List[str]] = None,\n",
    "    requirements_file: Optional[str] = None,\n",
    "    ecr_repository: Optional[str] = None,\n",
    "    container_runtime: Optional[str] = None,\n",
    "    auto_create_ecr: bool = True,\n",
    "    auto_create_execution_role: bool = False,\n",
    "    authorizer_configuration: Optional[Dict[str, Any]] = None,\n",
    "    region: Optional[str] = None,\n",
    "    protocol: Optional[Literal['HTTP', 'MCP']] = None,\n",
    ") -> bedrock_agentcore_starter_toolkit.operations.runtime.models.ConfigureResult\n",
    "```\n",
    "\n",
    "**Docstring:**  \n",
    "Configure Bedrock AgentCore from notebook using an entrypoint file.\n",
    "\n",
    "**Args:**\n",
    "- **entrypoint**: Path to Python file with optional Bedrock AgentCore name (e.g., \"handler.py\" or \"handler.py:bedrock_agentcore\")\n",
    "- **execution_role**: AWS IAM execution role ARN or name (optional if auto_create_execution_role=True)\n",
    "- **agent_name**: name of the agent\n",
    "- **requirements**: Optional list of requirements to generate requirements.txt\n",
    "- **requirements_file**: Optional path to existing requirements file\n",
    "- **ecr_repository**: Optional ECR repository URI\n",
    "- **container_runtime**: Optional container runtime (docker/podman)\n",
    "- **auto_create_ecr**: Whether to auto-create ECR repository\n",
    "- **auto_create_execution_role**: Whether to auto-create execution role (makes execution_role optional)\n",
    "- **authorizer_configuration**: JWT authorizer configuration dictionary\n",
    "- **region**: AWS region for deployment\n",
    "- **protocol**: agent server protocol, must be either HTTP or MCP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e1b84cc-798e-472c-ac0b-2c315f4b704d",
   "metadata": {},
   "source": [
    "### Step 3. ë°°í¬: `agentcore_runtime.launch()`\n",
    "ìœ„ configure ëª…ë ¹ì–´ë¥¼ í†µí•´, **ì—ì´ì „íŠ¸ ë°°í¬ë¥¼ ìœ„í•œ Dockerfileì´ ì¤€ë¹„**ë˜ì—ˆìŠµë‹ˆë‹¤. \n",
    "\n",
    "ì´ì œ `launch()` ëª…ë ¹ì–´ë¥¼ ì´ìš©í•´ì„œ ìœ„ Dockerfileì„ ê¸°ë°˜ìœ¼ë¡œ ì—ì´ì „íŠ¸ì˜ ì»¨í…Œì´ë„ˆ ì´ë¯¸ì§€ë¥¼ ìƒì„±í•´ Amazon ECR ë ˆí¬ì§€í† ë¦¬ì— ì˜¬ë¦° ë’¤, AgentCore Runtimeì— ë°°í¬í•´ë³´ê² ìŠµë‹ˆë‹¤.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "17a32ab8-7701-4900-8055-e24364bdf35c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ğŸš€ CodeBuild mode: building in cloud (RECOMMENDED - DEFAULT)\n",
      "INFO:bedrock_agentcore_starter_toolkit.notebook.runtime.bedrock_agentcore:ğŸš€ CodeBuild mode: building in cloud (RECOMMENDED - DEFAULT)\n",
      "   â€¢ Build ARM64 containers in the cloud with CodeBuild\n",
      "INFO:bedrock_agentcore_starter_toolkit.notebook.runtime.bedrock_agentcore:   â€¢ Build ARM64 containers in the cloud with CodeBuild\n",
      "   â€¢ No local Docker required\n",
      "INFO:bedrock_agentcore_starter_toolkit.notebook.runtime.bedrock_agentcore:   â€¢ No local Docker required\n",
      "ğŸ’¡ Available deployment modes:\n",
      "INFO:bedrock_agentcore_starter_toolkit.notebook.runtime.bedrock_agentcore:ğŸ’¡ Available deployment modes:\n",
      "   â€¢ runtime.launch()                           â†’ CodeBuild (current)\n",
      "INFO:bedrock_agentcore_starter_toolkit.notebook.runtime.bedrock_agentcore:   â€¢ runtime.launch()                           â†’ CodeBuild (current)\n",
      "   â€¢ runtime.launch(local=True)                 â†’ Local development\n",
      "INFO:bedrock_agentcore_starter_toolkit.notebook.runtime.bedrock_agentcore:   â€¢ runtime.launch(local=True)                 â†’ Local development\n",
      "   â€¢ runtime.launch(local_build=True)           â†’ Local build + cloud deploy (NEW)\n",
      "INFO:bedrock_agentcore_starter_toolkit.notebook.runtime.bedrock_agentcore:   â€¢ runtime.launch(local_build=True)           â†’ Local build + cloud deploy (NEW)\n",
      "Starting CodeBuild ARM64 deployment for agent 'bedrock_manus_runtime' to account 654654304740 (us-west-2)\n",
      "INFO:bedrock_agentcore_starter_toolkit.operations.runtime.launch:Starting CodeBuild ARM64 deployment for agent 'bedrock_manus_runtime' to account 654654304740 (us-west-2)\n",
      "Setting up AWS resources (ECR repository, execution roles)...\n",
      "INFO:bedrock_agentcore_starter_toolkit.operations.runtime.launch:Setting up AWS resources (ECR repository, execution roles)...\n",
      "Getting or creating ECR repository for agent: bedrock_manus_runtime\n",
      "INFO:bedrock_agentcore_starter_toolkit.operations.runtime.launch:Getting or creating ECR repository for agent: bedrock_manus_runtime\n",
      "âœ… ECR repository available: 654654304740.dkr.ecr.us-west-2.amazonaws.com/bedrock-agentcore-bedrock_manus_runtime\n",
      "INFO:bedrock_agentcore_starter_toolkit.operations.runtime.launch:âœ… ECR repository available: 654654304740.dkr.ecr.us-west-2.amazonaws.com/bedrock-agentcore-bedrock_manus_runtime\n",
      "Using execution role from config: arn:aws:iam::654654304740:role/agentcore-bedrock_manus_runtime-role\n",
      "INFO:bedrock_agentcore_starter_toolkit.operations.runtime.launch:Using execution role from config: arn:aws:iam::654654304740:role/agentcore-bedrock_manus_runtime-role\n",
      "Preparing CodeBuild project and uploading source...\n",
      "INFO:bedrock_agentcore_starter_toolkit.operations.runtime.launch:Preparing CodeBuild project and uploading source...\n",
      "Getting or creating CodeBuild execution role for agent: bedrock_manus_runtime\n",
      "INFO:bedrock_agentcore_starter_toolkit.services.codebuild:Getting or creating CodeBuild execution role for agent: bedrock_manus_runtime\n",
      "Role name: AmazonBedrockAgentCoreSDKCodeBuild-us-west-2-b015ee5cd0\n",
      "INFO:bedrock_agentcore_starter_toolkit.services.codebuild:Role name: AmazonBedrockAgentCoreSDKCodeBuild-us-west-2-b015ee5cd0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Reusing existing ECR repository: 654654304740.dkr.ecr.us-west-2.amazonaws.com/bedrock-agentcore-bedrock_manus_runtime\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing existing CodeBuild execution role: arn:aws:iam::654654304740:role/AmazonBedrockAgentCoreSDKCodeBuild-us-west-2-b015ee5cd0\n",
      "INFO:bedrock_agentcore_starter_toolkit.services.codebuild:Reusing existing CodeBuild execution role: arn:aws:iam::654654304740:role/AmazonBedrockAgentCoreSDKCodeBuild-us-west-2-b015ee5cd0\n",
      "Using .dockerignore with 43 patterns\n",
      "INFO:bedrock_agentcore_starter_toolkit.services.codebuild:Using .dockerignore with 43 patterns\n",
      "Uploaded source to S3: bedrock_manus_runtime/source.zip\n",
      "INFO:bedrock_agentcore_starter_toolkit.services.codebuild:Uploaded source to S3: bedrock_manus_runtime/source.zip\n",
      "Updated CodeBuild project: bedrock-agentcore-bedrock_manus_runtime-builder\n",
      "INFO:bedrock_agentcore_starter_toolkit.services.codebuild:Updated CodeBuild project: bedrock-agentcore-bedrock_manus_runtime-builder\n",
      "Starting CodeBuild build (this may take several minutes)...\n",
      "INFO:bedrock_agentcore_starter_toolkit.operations.runtime.launch:Starting CodeBuild build (this may take several minutes)...\n",
      "Starting CodeBuild monitoring...\n",
      "INFO:bedrock_agentcore_starter_toolkit.services.codebuild:Starting CodeBuild monitoring...\n",
      "ğŸ”„ QUEUED started (total: 0s)\n",
      "INFO:bedrock_agentcore_starter_toolkit.services.codebuild:ğŸ”„ QUEUED started (total: 0s)\n",
      "âœ… QUEUED completed in 1.0s\n",
      "INFO:bedrock_agentcore_starter_toolkit.services.codebuild:âœ… QUEUED completed in 1.0s\n",
      "ğŸ”„ PROVISIONING started (total: 1s)\n",
      "INFO:bedrock_agentcore_starter_toolkit.services.codebuild:ğŸ”„ PROVISIONING started (total: 1s)\n",
      "âœ… PROVISIONING completed in 8.3s\n",
      "INFO:bedrock_agentcore_starter_toolkit.services.codebuild:âœ… PROVISIONING completed in 8.3s\n",
      "ğŸ”„ DOWNLOAD_SOURCE started (total: 9s)\n",
      "INFO:bedrock_agentcore_starter_toolkit.services.codebuild:ğŸ”„ DOWNLOAD_SOURCE started (total: 9s)\n",
      "âœ… DOWNLOAD_SOURCE completed in 2.1s\n",
      "INFO:bedrock_agentcore_starter_toolkit.services.codebuild:âœ… DOWNLOAD_SOURCE completed in 2.1s\n",
      "ğŸ”„ BUILD started (total: 11s)\n",
      "INFO:bedrock_agentcore_starter_toolkit.services.codebuild:ğŸ”„ BUILD started (total: 11s)\n",
      "âœ… BUILD completed in 36.1s\n",
      "INFO:bedrock_agentcore_starter_toolkit.services.codebuild:âœ… BUILD completed in 36.1s\n",
      "ğŸ”„ POST_BUILD started (total: 47s)\n",
      "INFO:bedrock_agentcore_starter_toolkit.services.codebuild:ğŸ”„ POST_BUILD started (total: 47s)\n",
      "âœ… POST_BUILD completed in 46.4s\n",
      "INFO:bedrock_agentcore_starter_toolkit.services.codebuild:âœ… POST_BUILD completed in 46.4s\n",
      "ğŸ”„ COMPLETED started (total: 94s)\n",
      "INFO:bedrock_agentcore_starter_toolkit.services.codebuild:ğŸ”„ COMPLETED started (total: 94s)\n",
      "âœ… COMPLETED completed in 1.0s\n",
      "INFO:bedrock_agentcore_starter_toolkit.services.codebuild:âœ… COMPLETED completed in 1.0s\n",
      "ğŸ‰ CodeBuild completed successfully in 1m 34s\n",
      "INFO:bedrock_agentcore_starter_toolkit.services.codebuild:ğŸ‰ CodeBuild completed successfully in 1m 34s\n",
      "CodeBuild completed successfully\n",
      "INFO:bedrock_agentcore_starter_toolkit.operations.runtime.launch:CodeBuild completed successfully\n",
      "âœ… CodeBuild project configuration saved\n",
      "INFO:bedrock_agentcore_starter_toolkit.operations.runtime.launch:âœ… CodeBuild project configuration saved\n",
      "Deploying to Bedrock AgentCore...\n",
      "INFO:bedrock_agentcore_starter_toolkit.operations.runtime.launch:Deploying to Bedrock AgentCore...\n",
      "âœ… Agent created/updated: arn:aws:bedrock-agentcore:us-west-2:654654304740:runtime/bedrock_manus_runtime-27tUDP5Ol2\n",
      "INFO:bedrock_agentcore_starter_toolkit.operations.runtime.launch:âœ… Agent created/updated: arn:aws:bedrock-agentcore:us-west-2:654654304740:runtime/bedrock_manus_runtime-27tUDP5Ol2\n",
      "Polling for endpoint to be ready...\n",
      "INFO:bedrock_agentcore_starter_toolkit.operations.runtime.launch:Polling for endpoint to be ready...\n",
      "Agent endpoint: arn:aws:bedrock-agentcore:us-west-2:654654304740:runtime/bedrock_manus_runtime-27tUDP5Ol2/runtime-endpoint/DEFAULT\n",
      "INFO:bedrock_agentcore_starter_toolkit.operations.runtime.launch:Agent endpoint: arn:aws:bedrock-agentcore:us-west-2:654654304740:runtime/bedrock_manus_runtime-27tUDP5Ol2/runtime-endpoint/DEFAULT\n",
      "Deployment completed successfully - Agent: arn:aws:bedrock-agentcore:us-west-2:654654304740:runtime/bedrock_manus_runtime-27tUDP5Ol2\n",
      "INFO:bedrock_agentcore_starter_toolkit.operations.runtime.launch:Deployment completed successfully - Agent: arn:aws:bedrock-agentcore:us-west-2:654654304740:runtime/bedrock_manus_runtime-27tUDP5Ol2\n",
      "Built with CodeBuild: bedrock-agentcore-bedrock_manus_runtime-builder:047084a3-cf99-4467-8680-a56098e2d65f\n",
      "INFO:bedrock_agentcore_starter_toolkit.notebook.runtime.bedrock_agentcore:Built with CodeBuild: bedrock-agentcore-bedrock_manus_runtime-builder:047084a3-cf99-4467-8680-a56098e2d65f\n",
      "Deployed to cloud: arn:aws:bedrock-agentcore:us-west-2:654654304740:runtime/bedrock_manus_runtime-27tUDP5Ol2\n",
      "INFO:bedrock_agentcore_starter_toolkit.notebook.runtime.bedrock_agentcore:Deployed to cloud: arn:aws:bedrock-agentcore:us-west-2:654654304740:runtime/bedrock_manus_runtime-27tUDP5Ol2\n",
      "ECR image: 654654304740.dkr.ecr.us-west-2.amazonaws.com/bedrock-agentcore-bedrock_manus_runtime\n",
      "INFO:bedrock_agentcore_starter_toolkit.notebook.runtime.bedrock_agentcore:ECR image: 654654304740.dkr.ecr.us-west-2.amazonaws.com/bedrock-agentcore-bedrock_manus_runtime\n",
      "ğŸ” Agent logs available at:\n",
      "INFO:bedrock_agentcore_starter_toolkit.notebook.runtime.bedrock_agentcore:ğŸ” Agent logs available at:\n",
      "   /aws/bedrock-agentcore/runtimes/bedrock_manus_runtime-27tUDP5Ol2-DEFAULT --log-stream-name-prefix \"2025/09/12/\\[runtime-logs]\"\n",
      "INFO:bedrock_agentcore_starter_toolkit.notebook.runtime.bedrock_agentcore:   /aws/bedrock-agentcore/runtimes/bedrock_manus_runtime-27tUDP5Ol2-DEFAULT --log-stream-name-prefix \"2025/09/12/\\[runtime-logs]\"\n",
      "   /aws/bedrock-agentcore/runtimes/bedrock_manus_runtime-27tUDP5Ol2-DEFAULT --log-stream-names \"otel-rt-logs\"\n",
      "INFO:bedrock_agentcore_starter_toolkit.notebook.runtime.bedrock_agentcore:   /aws/bedrock-agentcore/runtimes/bedrock_manus_runtime-27tUDP5Ol2-DEFAULT --log-stream-names \"otel-rt-logs\"\n",
      "ğŸ’¡ Tail logs with: aws logs tail /aws/bedrock-agentcore/runtimes/bedrock_manus_runtime-27tUDP5Ol2-DEFAULT --log-stream-name-prefix \"2025/09/12/\\[runtime-logs]\" --follow\n",
      "INFO:bedrock_agentcore_starter_toolkit.notebook.runtime.bedrock_agentcore:ğŸ’¡ Tail logs with: aws logs tail /aws/bedrock-agentcore/runtimes/bedrock_manus_runtime-27tUDP5Ol2-DEFAULT --log-stream-name-prefix \"2025/09/12/\\[runtime-logs]\" --follow\n",
      "ğŸ’¡ Or view recent logs: aws logs tail /aws/bedrock-agentcore/runtimes/bedrock_manus_runtime-27tUDP5Ol2-DEFAULT --log-stream-name-prefix \"2025/09/12/\\[runtime-logs]\" --since 1h\n",
      "INFO:bedrock_agentcore_starter_toolkit.notebook.runtime.bedrock_agentcore:ğŸ’¡ Or view recent logs: aws logs tail /aws/bedrock-agentcore/runtimes/bedrock_manus_runtime-27tUDP5Ol2-DEFAULT --log-stream-name-prefix \"2025/09/12/\\[runtime-logs]\" --since 1h\n"
     ]
    }
   ],
   "source": [
    "launch_result = agentcore_runtime.launch(auto_update_on_conflict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4491351a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Machine: x86_64\n",
      "Processor: x86_64\n",
      "Architecture: ('64bit', 'ELF')\n",
      "Platform: Linux-5.10.240-238.959.amzn2.x86_64-x86_64-with-glibc2.26\n"
     ]
    }
   ],
   "source": [
    "import platform\n",
    "print(f\"Machine: {platform.machine()}\")\n",
    "print(f\"Processor: {platform.processor()}\")\n",
    "print(f\"Architecture: {platform.architecture()}\")\n",
    "print(f\"Platform: {platform.platform()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0ae9c09-09db-4a76-871a-92eacd96b9c3",
   "metadata": {},
   "source": [
    "#### Step 3-a. ë°°í¬ ì™„ë£Œ í›„, ì˜ ë°°í¬ë˜ì—ˆëŠ”ì§€ ìƒíƒœ ì²´í¬: `agentcore_runtime.status()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "afa6ac09-9adb-4846-9fc1-4d12aeb74853",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrieved Bedrock AgentCore status for: bedrock_manus_runtime\n",
      "INFO:bedrock_agentcore_starter_toolkit.notebook.runtime.bedrock_agentcore:Retrieved Bedrock AgentCore status for: bedrock_manus_runtime\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'READY'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "status_response = agentcore_runtime.status()\n",
    "status = status_response.endpoint['status']\n",
    "end_status = ['READY', 'CREATE_FAILED', 'DELETE_FAILED', 'UPDATE_FAILED']\n",
    "while status not in end_status:\n",
    "    time.sleep(10)\n",
    "    status_response = agentcore_runtime.status()\n",
    "    status = status_response.endpoint['status']\n",
    "    print(status)\n",
    "status"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7f89c56-918a-4cab-beaa-c7ac43a2ba29",
   "metadata": {},
   "source": [
    "### Step 4. ë°°í¬í•œ ì—ì´ì „íŠ¸ í˜¸ì¶œ: `invoke_agent_runtime()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "78945203",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[97mhan\u001b[0m\u001b[97mdoff_to_\u001b[0m\u001b[97mplanner: I\u001b[0m\u001b[97m'll need to cons\u001b[0m\u001b[97mult our planning system\u001b[0m\u001b[97m for this request.\u001b[0m\u001b[97m\n",
      "\n",
      "This\u001b[0m\u001b[97m request\u001b[0m\u001b[97m involves\u001b[0m\u001b[97m complex\u001b[0m\u001b[97m data analysis tasks\u001b[0m\u001b[97m including\u001b[0m\u001b[97m:\u001b[0m\u001b[97m\n",
      "- Analyzing Amazon\u001b[0m\u001b[97m product sales data\u001b[0m\u001b[97m\n",
      "- Exploring\u001b[0m\u001b[97m data\u001b[0m\u001b[97m attributes\u001b[0m\u001b[97m\n",
      "- Analyzing\u001b[0m\u001b[97m sales\u001b[0m\u001b[97m trends\n",
      "-\u001b[0m\u001b[97m Examining\u001b[0m\u001b[97m variable relationships\n",
      "-\u001b[0m\u001b[97m Creating\u001b[0m\u001b[97m visual\u001b[0m\u001b[97mizations an\u001b[0m\u001b[97md charts\u001b[0m\u001b[97m\n",
      "- Generating\u001b[0m\u001b[97m insights\u001b[0m\u001b[97m\n",
      "- Producing\u001b[0m\u001b[97m a PDF\u001b[0m\u001b[97m report\n",
      "\n",
      "I\u001b[0m\u001b[97m'll\u001b[0m\u001b[97m pass\u001b[0m\u001b[97m this to\u001b[0m\u001b[97m our\u001b[0m\u001b[97m planning system\u001b[0m\u001b[97m to handle these\u001b[0m\u001b[97m analytical\u001b[0m\u001b[97m tasks properly\u001b[0m\u001b[97m.\u001b[0m\u001b[96mLet\u001b[0m\u001b[96m me analyze\u001b[0m\u001b[96m the user request carefully:\u001b[0m\u001b[96m\n",
      "\n",
      "The user wants to analyze\u001b[0m\u001b[96m Amazon product sales data in\u001b[0m\u001b[96m a file\u001b[0m\u001b[96m called \"./\u001b[0m\u001b[96mdata/\u001b[0m\u001b[96mDat-fresh\u001b[0m\u001b[96m-food-claude\u001b[0m\u001b[96m.csv\". They want:\u001b[0m\u001b[96m\n",
      "1. Basic\u001b[0m\u001b[96m data property\u001b[0m\u001b[96m exploration\u001b[0m\u001b[96m\n",
      "2. Product sales trend analysis\u001b[0m\u001b[96m\n",
      "3. Variable\u001b[0m\u001b[96m relationship analysis\n",
      "4. Various\u001b[0m\u001b[96m analysis techniques\n",
      "5. Further\u001b[0m\u001b[96m analysis for\u001b[0m\u001b[96m insights if needed\u001b[0m\u001b[96m\n",
      "6. A detailed report\u001b[0m\u001b[96m with images and charts\u001b[0m\u001b[96m\n",
      "7. Final\u001b[0m\u001b[96m report in PDF format\n",
      "\n",
      "This\u001b[0m\u001b[96m is a comprehensive\u001b[0m\u001b[96m data analysis task that requires multiple\u001b[0m\u001b[96m steps. Let me\u001b[0m\u001b[96m create a detaile\u001b[0m\u001b[96md plan following the require\u001b[0m\u001b[96md agent workflow.\n",
      "\n",
      "Since\u001b[0m\u001b[96m this involves data analysis\u001b[0m\u001b[96m with calculations and visualizations,\u001b[0m\u001b[96m we will definitely\u001b[0m\u001b[96m need the Validator\u001b[0m\u001b[96m step after the Coder\u001b[0m\u001b[96m does the analysis\u001b[0m\u001b[96m.\n",
      "\n",
      "Here\u001b[0m\u001b[96m's my\u001b[0m\u001b[96m planned workflow:\n",
      "\n",
      "1\u001b[0m\u001b[96m. Coder\u001b[0m\u001b[96m: Will\u001b[0m\u001b[96m handle all\u001b[0m\u001b[96m data processing and analysis\u001b[0m\u001b[96m tasks\n",
      "   - Load and explore\u001b[0m\u001b[96m the dataset\u001b[0m\u001b[96m\n",
      "   -\u001b[0m\u001b[96m Perform basic\u001b[0m\u001b[96m data analysis (\u001b[0m\u001b[96mstatistics, distributions\u001b[0m\u001b[96m, etc.)\u001b[0m\u001b[96m\n",
      "   - Analyze\u001b[0m\u001b[96m product sales trends\n",
      "   -\u001b[0m\u001b[96m Explore relationships between variables\n",
      "   \u001b[0m\u001b[96m- Create visualizations\u001b[0m\u001b[96m and charts\n",
      "   -\u001b[0m\u001b[96m Perform additional analyses for insights\n",
      "   -\u001b[0m\u001b[96m Generate calculation metadata for validation\u001b[0m\u001b[96m\n",
      "\n",
      "2. Validator: Will validate all\u001b[0m\u001b[96m calculations and analyses\u001b[0m\u001b[96m\n",
      "   - Verify\u001b[0m\u001b[96m the accuracy of all numerical\u001b[0m\u001b[96m calculations\n",
      "   -\u001b[0m\u001b[96m Validate statistical\u001b[0m\u001b[96m analyses and trends\u001b[0m\u001b[96m\n",
      "   -\u001b[0m\u001b[96m Confirm visualization\u001b[0m\u001b[96m accuracy\u001b[0m\u001b[96m\n",
      "   - Generate citation\u001b[0m\u001b[96m metadata for important findings\u001b[0m\u001b[96m\n",
      "   \u001b[0m\u001b[96m\n",
      "3. Reporter:\u001b[0m\u001b[96m Will create the\u001b[0m\u001b[96m final report with\u001b[0m\u001b[96m validated findings\n",
      "   -\u001b[0m\u001b[96m Compile all\u001b[0m\u001b[96m validated analyses\u001b[0m\u001b[96m into a comprehensive\u001b[0m\u001b[96m report\n",
      "   -\u001b[0m\u001b[96m Include all\u001b[0m\u001b[96m necessary\u001b[0m\u001b[96m visualizations and charts\u001b[0m\u001b[96m\n",
      "   - Structure the report with\u001b[0m\u001b[96m proper\u001b[0m\u001b[96m sections\n",
      "   - Generate\u001b[0m\u001b[96m insights an\u001b[0m\u001b[96md recommendations base\u001b[0m\u001b[96md on validated findings\u001b[0m\u001b[96m\n",
      "   - Create\u001b[0m\u001b[96m both a version\u001b[0m\u001b[96m with citations and a clean version\u001b[0m\u001b[96m\n",
      "   -\u001b[0m\u001b[96m Generate\u001b[0m\u001b[96m the final PDF report\n",
      "\n",
      "This\u001b[0m\u001b[96m follows the require\u001b[0m\u001b[96md Coder â†’\u001b[0m\u001b[96m Validator â†’ Reporter workflow for data analysis\u001b[0m\u001b[96m tasks.\u001b[0m\u001b[96m\u001b[0m\u001b[97m# Plan\u001b[0m\u001b[97m\n",
      "\n",
      "## thought\n",
      "This\u001b[0m\u001b[97m request requires comprehensive\u001b[0m\u001b[97m analysis\u001b[0m\u001b[97m of Amazon product sales\u001b[0m\u001b[97m data. We\u001b[0m\u001b[97m need to loa\u001b[0m\u001b[97md an\u001b[0m\u001b[97md explore data\u001b[0m\u001b[97m from\u001b[0m\u001b[97m \"./\u001b[0m\u001b[97mdata/Dat\u001b[0m\u001b[97m-fresh-foo\u001b[0m\u001b[97md-claude.csv\u001b[0m\u001b[97m\", perform various analyses\u001b[0m\u001b[97m including basic\u001b[0m\u001b[97m exploration, sales trends\u001b[0m\u001b[97m, variable relationships, an\u001b[0m\u001b[97md generate insights. All\u001b[0m\u001b[97m analyses must be validated, an\u001b[0m\u001b[97md a final\u001b[0m\u001b[97m detailed report with\u001b[0m\u001b[97m visualizations must be create\u001b[0m\u001b[97md in PDF format.\u001b[0m\u001b[97m\n",
      "\n",
      "## title:\n",
      "Amazon\u001b[0m\u001b[97m Product Sales Data Analysis\u001b[0m\u001b[97m and Marketing\u001b[0m\u001b[97m Insights\n",
      "\n",
      "## steps\u001b[0m\u001b[97m:\n",
      "###\u001b[0m\u001b[97m 1.\u001b[0m\u001b[97m Coder:\u001b[0m\u001b[97m Data Loading,\u001b[0m\u001b[97m Analysis an\u001b[0m\u001b[97md Visualization\n",
      "-\u001b[0m\u001b[97m [ ] Loa\u001b[0m\u001b[97md the dataset \"./data/\u001b[0m\u001b[97mDat-fresh\u001b[0m\u001b[97m-food-claude.csv\"\u001b[0m\u001b[97m\n",
      "- [ ]\u001b[0m\u001b[97m Perform exploratory data analysis (\u001b[0m\u001b[97mdata types, missing values,\u001b[0m\u001b[97m statistics)\n",
      "- [ ]\u001b[0m\u001b[97m Analyze basic\u001b[0m\u001b[97m metrics (sales\u001b[0m\u001b[97m volume, revenue\u001b[0m\u001b[97m, customer behavior\u001b[0m\u001b[97m)\n",
      "- [\u001b[0m\u001b[97m ] Identify an\u001b[0m\u001b[97md visualize product\u001b[0m\u001b[97m sales trends (temporal patterns\u001b[0m\u001b[97m, seasonality)\n",
      "-\u001b[0m\u001b[97m [ ] Examine\u001b[0m\u001b[97m variable relationships (correlation\u001b[0m\u001b[97m analysis, feature importance\u001b[0m\u001b[97m)\n",
      "- [\u001b[0m\u001b[97m ] Perform segmentation analysis\u001b[0m\u001b[97m (product categories, customer segments)\u001b[0m\u001b[97m\n",
      "- [ ] Create visualizations for\u001b[0m\u001b[97m key findings (charts, h\u001b[0m\u001b[97meatmaps, distribution\u001b[0m\u001b[97m plots)\n",
      "- [ ]\u001b[0m\u001b[97m Analyze variable combinations for potential\u001b[0m\u001b[97m insights\n",
      "- [ ]\u001b[0m\u001b[97m Conduct additional analyses based on preliminary\u001b[0m\u001b[97m findings\n",
      "- [ ] Generate\u001b[0m\u001b[97m calculation metadata for validation\n",
      "\n",
      "### 2\u001b[0m\u001b[97m. Validator: Validation of Analysis\u001b[0m\u001b[97m Results\n",
      "- [ ]\u001b[0m\u001b[97m Verify data loading\u001b[0m\u001b[97m integrity and preprocessing steps\u001b[0m\u001b[97m\n",
      "- [ ] Validate all\u001b[0m\u001b[97m numerical calculations an\u001b[0m\u001b[97md statistical analyses\n",
      "-\u001b[0m\u001b[97m [ ] Check\u001b[0m\u001b[97m accuracy\u001b[0m\u001b[97m of tren\u001b[0m\u001b[97md analyses\u001b[0m\u001b[97m and growth\u001b[0m\u001b[97m metrics\u001b[0m\u001b[97m\n",
      "- [ ]\u001b[0m\u001b[97m Confirm\u001b[0m\u001b[97m correlation\u001b[0m\u001b[97m values\u001b[0m\u001b[97m an\u001b[0m\u001b[97md relationship\u001b[0m\u001b[97m streng\u001b[0m\u001b[97mths\n",
      "- [\u001b[0m\u001b[97m ] Verify\u001b[0m\u001b[97m seg\u001b[0m\u001b[97mmentation methodology\u001b[0m\u001b[97m and results\u001b[0m\u001b[97m\n",
      "- [ ]\u001b[0m\u001b[97m Validate visualization\u001b[0m\u001b[97m accuracy and representation\u001b[0m\u001b[97m\n",
      "- [ ]\u001b[0m\u001b[97m Generate citation metadata for\u001b[0m\u001b[97m key findings\n",
      "- [\u001b[0m\u001b[97m ] Create reference\u001b[0m\u001b[97m points for important\u001b[0m\u001b[97m calculations\n",
      "- [ ]\u001b[0m\u001b[97m Document validation procedures for\u001b[0m\u001b[97m reproducibility\n",
      "\n",
      "### \u001b[0m\u001b[97m3. Reporter:\u001b[0m\u001b[97m Comprehensive Report Creation\n",
      "- [ ]\u001b[0m\u001b[97m Structure report with executive\u001b[0m\u001b[97m summary and methodology\n",
      "- [ ]\u001b[0m\u001b[97m Incorporate\u001b[0m\u001b[97m validated analysis results with\u001b[0m\u001b[97m proper\u001b[0m\u001b[97m citations\n",
      "- [ ]\u001b[0m\u001b[97m Include validate\u001b[0m\u001b[97md visualizations and charts\u001b[0m\u001b[97m with\u001b[0m\u001b[97m explanations\n",
      "- [\u001b[0m\u001b[97m ]\u001b[0m\u001b[97m Synthes\u001b[0m\u001b[97mize marketing\u001b[0m\u001b[97m insights from validate\u001b[0m\u001b[97md findings\n",
      "- [\u001b[0m\u001b[97m ] Provide action\u001b[0m\u001b[97mable recommendations base\u001b[0m\u001b[97md on data\u001b[0m\u001b[97m analysis\u001b[0m\u001b[97m\n",
      "- [ ]\u001b[0m\u001b[97m Include\u001b[0m\u001b[97m limitations\u001b[0m\u001b[97m and future\u001b[0m\u001b[97m research\u001b[0m\u001b[97m directions\u001b[0m\u001b[97m\n",
      "- [ ]\u001b[0m\u001b[97m Create reference\u001b[0m\u001b[97m section\u001b[0m\u001b[97m with\u001b[0m\u001b[97m calculation sources\n",
      "-\u001b[0m\u001b[97m [ ] Generate\u001b[0m\u001b[97m PDF with\u001b[0m\u001b[97m citations\u001b[0m\u001b[97m (final\u001b[0m\u001b[97m_report_with\u001b[0m\u001b[97m_citations.pdf\u001b[0m\u001b[97m)\n",
      "- [ ] Generate\u001b[0m\u001b[97m clean PDF\u001b[0m\u001b[97m version without\u001b[0m\u001b[97m citations (final_\u001b[0m\u001b[97mreport.pdf)\u001b[0m\u001b[97mI'll\u001b[0m\u001b[97m help coordinate the execution of this\u001b[0m\u001b[97m Amazon Product Sales Data Analysis project\u001b[0m\u001b[97m according to the provided plan. Base\u001b[0m\u001b[97md on the plan, we\u001b[0m\u001b[97m need to follow the sequence:\u001b[0m\u001b[97m Coder â†’ Validator â†’ Reporter\u001b[0m\u001b[97m.\n",
      "\n",
      "Looking at the current\u001b[0m\u001b[97m status, we need to start\u001b[0m\u001b[97m with the Coder agent\u001b[0m\u001b[97m to perform the data loading an\u001b[0m\u001b[97md analysis tasks. Let's begin\u001b[0m\u001b[97m by having the Coder\u001b[0m\u001b[97m agent load the dataset an\u001b[0m\u001b[97md perform the exploratory data analysis\u001b[0m\u001b[97m.\u001b[0m\u001b[97mI\u001b[0m\u001b[97m'll\u001b[0m\u001b[97m analyze\u001b[0m\u001b[97m the Amazon\u001b[0m\u001b[97m product sales data from\u001b[0m\u001b[97m \"./data/\u001b[0m\u001b[97mDat-fresh-\u001b[0m\u001b[97mfood-claude.\u001b[0m\u001b[97mcsv\" and perform\u001b[0m\u001b[97m comprehensive\u001b[0m\u001b[97m data\u001b[0m\u001b[97m analysis to\u001b[0m\u001b[97m extract\u001b[0m\u001b[97m marketing insights. Let\u001b[0m\u001b[97m me start\u001b[0m\u001b[97m by loading the data\u001b[0m\u001b[97m and exploring its\u001b[0m\u001b[97m structure.\u001b[0m\n",
      "[TOOL RESULT - bash_tool]\n",
      "\u001b[93mCMD:\n",
      "```bash\n",
      "ls -la ./data/\n",
      "```\n",
      "\u001b[0m\u001b[93mOutput:\n",
      "total 172\n",
      "drwxr-xr-x 2 root root   4096 Sep 12 06:35 .\n",
      "drwxr-xr-x 1 root root   4096 Sep 12 06:37 ..\n",
      "-rw-rw-r-- 1 root root 163633 Sep 12 02:36 Dat-fresh-food-claude.csv\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[97mGreat\u001b[0m\u001b[97m!\u001b[0m\u001b[97m I\u001b[0m\u001b[97m can see the\u001b[0m\u001b[97m data file exists\u001b[0m\u001b[97m.\u001b[0m\u001b[97m Let\u001b[0m\u001b[97m's loa\u001b[0m\u001b[97md the\u001b[0m\u001b[97m dataset\u001b[0m\u001b[97m an\u001b[0m\u001b[97md start\u001b[0m\u001b[97m our\u001b[0m\u001b[97m analysis.\u001b[0m\n",
      "[TOOL RESULT - python_repl_tool]\n",
      "\u001b[93mStatus: Successfully executed:\n",
      "\n",
      "\u001b[0m\u001b[93mCode:\n",
      "```python\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "import os\n",
      "import json\n",
      "from datetime import datetime\n",
      "import matplotlib.font_manager as fm\n",
      "\n",
      "# Create artifacts directory\n",
      "os.makedirs('./artifacts', exist_ok=True)\n",
      "\n",
      "# Load the dataset\n",
      "file_path = './data/Dat-fresh-food-claude.csv'\n",
      "df = pd.read_csv(file_path)\n",
      "\n",
      "# Display basic information about the dataset\n",
      "print(\"Dataset shape:\", df.shape)\n",
      "print(\"\\nFirst 5 rows of the dataset:\")\n",
      "print(df.head())\n",
      "\n",
      "# Display data types and missing values\n",
      "print(\"\\nData types and missing values:\")\n",
      "print(df.info())\n",
      "\n",
      "# Display summary statistics\n",
      "print(\"\\nSummary statistics:\")\n",
      "print(df.describe())\n",
      "\n",
      "# Initialize metadata tracking\n",
      "calculation_metadata = {\"calculations\": []}\n",
      "\n",
      "def track_calculation(calc_id, value, description, formula, source_file=\"\", source_columns=[], \n",
      "                     source_rows=\"\", importance=\"medium\", notes=\"\"):\n",
      "    \"\"\"Track calculation metadata for validation\"\"\"\n",
      "    calculation_metadata[\"calculations\"].append({\n",
      "        \"id\": calc_id,\n",
      "        \"value\": float(value) if isinstance(value, (int, float)) else str(value),\n",
      "        \"description\": description,\n",
      "        \"formula\": formula,\n",
      "        \"source_file\": source_file,\n",
      "        \"source_columns\": source_columns,\n",
      "        \"source_rows\": source_rows,\n",
      "        \"importance\": importance,\n",
      "        \"timestamp\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
      "        \"verification_notes\": notes\n",
      "    })\n",
      "\n",
      "def get_original_data_file():\n",
      "    \"\"\"Dynamically find the original data file\"\"\"\n",
      "    # Priority order for finding original data\n",
      "    priority_patterns = [\n",
      "        './data/Dat-fresh-food-claude.csv',  # Known primary file\n",
      "        './data/*.csv',                       # Any CSV in data directory\n",
      "        './data/*.xlsx',                      # Excel files\n",
      "        './data/*.json'                       # JSON files\n",
      "    ]\n",
      "    \n",
      "    import glob\n",
      "    for pattern in priority_patterns:\n",
      "        files = glob.glob(pattern)\n",
      "        if files:\n",
      "            return files[0]  # Return first match\n",
      "    return \"unknown_source\"  # Fallback\n",
      "\n",
      "# Use dynamic file detection\n",
      "original_data_file = get_original_data_file()\n",
      "print(f\"Using original data file: {original_data_file}\")\n",
      "```\n",
      "\u001b[0m\u001b[93mOutput:\n",
      "Dataset shape: (1220, 18)\n",
      "\n",
      "First 5 rows of the dataset:\n",
      "   index    Date   Status  ... promotion-ids Gender Age Group\n",
      "0      1  1/1/24  Shipped  ...   FRUITPROMO5      F       30ëŒ€\n",
      "1      2  1/1/24  Shipped  ...           NaN      M       40ëŒ€\n",
      "2      3  1/1/24  Shipped  ...    FRESH10OFF      F       50ëŒ€\n",
      "3      4  1/1/24  Shipped  ...   FRUITPROMO5      M       30ëŒ€\n",
      "4      5  1/1/24  Shipped  ...    VEGGIESALE      F       50ëŒ€\n",
      "\n",
      "[5 rows x 18 columns]\n",
      "\n",
      "Data types and missing values:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1220 entries, 0 to 1219\n",
      "Data columns (total 18 columns):\n",
      " #   Column              Non-Null Count  Dtype \n",
      "---  ------              --------------  ----- \n",
      " 0   index               1220 non-null   int64 \n",
      " 1   Date                1220 non-null   object\n",
      " 2   Status              1220 non-null   object\n",
      " 3   Fulfilment          1220 non-null   object\n",
      " 4   ship-service-level  1220 non-null   object\n",
      " 5   Style               1220 non-null   object\n",
      " 6   SKU                 1220 non-null   object\n",
      " 7   Category            1220 non-null   object\n",
      " 8   Size                1220 non-null   object\n",
      " 9   Courier Status      1220 non-null   object\n",
      " 10  Qty                 1220 non-null   int64 \n",
      " 11  Amount              1220 non-null   int64 \n",
      " 12  ship-city           1220 non-null   object\n",
      " 13  ship-state          1220 non-null   object\n",
      " 14  ship-postal-code    1220 non-null   int64 \n",
      " 15  promotion-ids       1117 non-null   object\n",
      " 16  Gender              1220 non-null   object\n",
      " 17  Age Group           1220 non-null   object\n",
      "dtypes: int64(4), object(14)\n",
      "memory usage: 171.7+ KB\n",
      "None\n",
      "\n",
      "Summary statistics:\n",
      "             index          Qty        Amount  ship-postal-code\n",
      "count  1220.000000  1220.000000   1220.000000       1220.000000\n",
      "mean    419.434426     1.794262   7064.877049      13582.980328\n",
      "std     280.003340     0.831773   3750.569930      21037.061393\n",
      "min       0.000000     1.000000   2900.000000       1000.000000\n",
      "25%     152.750000     1.000000   4500.000000       3000.000000\n",
      "50%     410.500000     2.000000   5990.000000       5000.000000\n",
      "75%     655.250000     3.000000   8500.000000      10300.000000\n",
      "max     935.000000     3.000000  23700.000000      99821.000000\n",
      "Using original data file: ./data/Dat-fresh-food-claude.csv\n",
      "\n",
      "\u001b[0m\u001b[97mNow\u001b[0m\u001b[97m let's examine the columns\u001b[0m\u001b[97m in more detail an\u001b[0m\u001b[97md check for missing values:\u001b[0m\n",
      "[TOOL RESULT - python_repl_tool]\n",
      "\u001b[93mStatus: Successfully executed:\n",
      "\n",
      "\u001b[0m\u001b[93mCode:\n",
      "```python\n",
      "# Check for missing values in each column\n",
      "missing_values = df.isnull().sum()\n",
      "print(\"Missing values in each column:\")\n",
      "print(missing_values)\n",
      "\n",
      "# Check unique values in categorical columns\n",
      "print(\"\\nUnique values in categorical columns:\")\n",
      "categorical_columns = ['Status', 'Fulfilment', 'ship-service-level', 'Category', 'Size', 'Courier Status', 'ship-city', 'ship-state', 'Gender', 'Age Group']\n",
      "for col in categorical_columns:\n",
      "    print(f\"\\n{col}: {df[col].nunique()} unique values\")\n",
      "    print(df[col].value_counts().head(10))\n",
      "\n",
      "# Check the date range\n",
      "print(\"\\nDate range:\")\n",
      "df['Date'] = pd.to_datetime(df['Date'], format='%m/%d/%y')\n",
      "print(f\"Start date: {df['Date'].min()}\")\n",
      "print(f\"End date: {df['Date'].max()}\")\n",
      "\n",
      "# Track calculation for total records\n",
      "track_calculation(\n",
      "    calc_id=\"calc_001\",\n",
      "    value=len(df),\n",
      "    description=\"Total number of records\",\n",
      "    formula=\"COUNT(*)\",\n",
      "    source_file=original_data_file,\n",
      "    source_columns=[\"*\"],\n",
      "    source_rows=\"all rows\",\n",
      "    importance=\"high\",\n",
      "    notes=\"Basic dataset size metric\"\n",
      ")\n",
      "```\n",
      "\u001b[0m\u001b[93mOutput:\n",
      "Error: Traceback (most recent call last):\n",
      "  File \"<string>\", line 2, in <module>\n",
      "NameError: name 'df' is not defined\n",
      "\n",
      "\u001b[0m\u001b[97mI\u001b[0m\u001b[97m apolog\u001b[0m\u001b[97mize for the error\u001b[0m\u001b[97m. Let\u001b[0m\u001b[97m's\u001b[0m\u001b[97m reloa\u001b[0m\u001b[97md the data and continue\u001b[0m\u001b[97m with our\u001b[0m\u001b[97m analysis:\u001b[0m\n",
      "[TOOL RESULT - python_repl_tool]\n",
      "\u001b[93mStatus: Successfully executed:\n",
      "\n",
      "\u001b[0m\u001b[93mCode:\n",
      "```python\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "import os\n",
      "import json\n",
      "from datetime import datetime\n",
      "import matplotlib.font_manager as fm\n",
      "\n",
      "# Create artifacts directory\n",
      "os.makedirs('./artifacts', exist_ok=True)\n",
      "\n",
      "# Load the dataset\n",
      "file_path = './data/Dat-fresh-food-claude.csv'\n",
      "df = pd.read_csv(file_path)\n",
      "\n",
      "# Initialize metadata tracking\n",
      "calculation_metadata = {\"calculations\": []}\n",
      "\n",
      "def track_calculation(calc_id, value, description, formula, source_file=\"\", source_columns=[], \n",
      "                     source_rows=\"\", importance=\"medium\", notes=\"\"):\n",
      "    \"\"\"Track calculation metadata for validation\"\"\"\n",
      "    calculation_metadata[\"calculations\"].append({\n",
      "        \"id\": calc_id,\n",
      "        \"value\": float(value) if isinstance(value, (int, float)) else str(value),\n",
      "        \"description\": description,\n",
      "        \"formula\": formula,\n",
      "        \"source_file\": source_file,\n",
      "        \"source_columns\": source_columns,\n",
      "        \"source_rows\": source_rows,\n",
      "        \"importance\": importance,\n",
      "        \"timestamp\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
      "        \"verification_notes\": notes\n",
      "    })\n",
      "\n",
      "def get_original_data_file():\n",
      "    \"\"\"Dynamically find the original data file\"\"\"\n",
      "    # Priority order for finding original data\n",
      "    priority_patterns = [\n",
      "        './data/Dat-fresh-food-claude.csv',  # Known primary file\n",
      "        './data/*.csv',                       # Any CSV in data directory\n",
      "        './data/*.xlsx',                      # Excel files\n",
      "        './data/*.json'                       # JSON files\n",
      "    ]\n",
      "    \n",
      "    import glob\n",
      "    for pattern in priority_patterns:\n",
      "        files = glob.glob(pattern)\n",
      "        if files:\n",
      "            return files[0]  # Return first match\n",
      "    return \"unknown_source\"  # Fallback\n",
      "\n",
      "# Use dynamic file detection\n",
      "original_data_file = get_original_data_file()\n",
      "print(f\"Using original data file: {original_data_file}\")\n",
      "\n",
      "# Check for missing values in each column\n",
      "missing_values = df.isnull().sum()\n",
      "print(\"Missing values in each column:\")\n",
      "print(missing_values)\n",
      "\n",
      "# Check unique values in categorical columns\n",
      "print(\"\\nUnique values in categorical columns:\")\n",
      "categorical_columns = ['Status', 'Fulfilment', 'ship-service-level', 'Category', 'Size', 'Courier Status', 'ship-city', 'ship-state', 'Gender', 'Age Group']\n",
      "for col in categorical_columns:\n",
      "    print(f\"\\n{col}: {df[col].nunique()} unique values\")\n",
      "    print(df[col].value_counts().head(10))\n",
      "\n",
      "# Check the date range\n",
      "print(\"\\nDate range:\")\n",
      "df['Date'] = pd.to_datetime(df['Date'], format='%m/%d/%y')\n",
      "print(f\"Start date: {df['Date'].min()}\")\n",
      "print(f\"End date: {df['Date'].max()}\")\n",
      "\n",
      "# Track calculation for total records\n",
      "track_calculation(\n",
      "    calc_id=\"calc_001\",\n",
      "    value=len(df),\n",
      "    description=\"Total number of records\",\n",
      "    formula=\"COUNT(*)\",\n",
      "    source_file=original_data_file,\n",
      "    source_columns=[\"*\"],\n",
      "    source_rows=\"all rows\",\n",
      "    importance=\"high\",\n",
      "    notes=\"Basic dataset size metric\")\n",
      "```\n",
      "\u001b[0m\u001b[93mOutput:\n",
      "Using original data file: ./data/Dat-fresh-food-claude.csv\n",
      "Missing values in each column:\n",
      "index                   0\n",
      "Date                    0\n",
      "Status                  0\n",
      "Fulfilment              0\n",
      "ship-service-level      0\n",
      "Style                   0\n",
      "SKU                     0\n",
      "Category                0\n",
      "Size                    0\n",
      "Courier Status          0\n",
      "Qty                     0\n",
      "Amount                  0\n",
      "ship-city               0\n",
      "ship-state              0\n",
      "ship-postal-code        0\n",
      "promotion-ids         103\n",
      "Gender                  0\n",
      "Age Group               0\n",
      "dtype: int64\n",
      "\n",
      "Unique values in categorical columns:\n",
      "\n",
      "Status: 1 unique values\n",
      "Status\n",
      "Shipped    1220\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Fulfilment: 4 unique values\n",
      "Fulfilment\n",
      "Coupang         312\n",
      "Market Kurly    307\n",
      "SSG.com         302\n",
      "Amazon          299\n",
      "Name: count, dtype: int64\n",
      "\n",
      "ship-service-level: 3 unique values\n",
      "ship-service-level\n",
      "Standard     588\n",
      "Expedited    403\n",
      "Priority     229\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Category: 3 unique values\n",
      "Category\n",
      "ì±„ì†Œ     458\n",
      "ê³¼ì¼     414\n",
      "ìœ ì œí’ˆ    348\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Size: 18 unique values\n",
      "Size\n",
      "300g    237\n",
      "500g    104\n",
      "200g    103\n",
      "1kg     100\n",
      "3ê°œì…      69\n",
      "1L       68\n",
      "400g     68\n",
      "1í†µ       68\n",
      "XL       54\n",
      "M        51\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Courier Status: 2 unique values\n",
      "Courier Status\n",
      "Delivered     1017\n",
      "In Transit     203\n",
      "Name: count, dtype: int64\n",
      "\n",
      "ship-city: 30 unique values\n",
      "ship-city\n",
      "ì¢…ë¡œêµ¬     59\n",
      "ì„±ë™êµ¬     58\n",
      "ì¤‘êµ¬      58\n",
      "ë¶„ë‹¹êµ¬     57\n",
      "ë…¸ì›êµ¬     57\n",
      "ë§ˆí¬êµ¬     56\n",
      "ì„±ë¶êµ¬     55\n",
      "ë™ëŒ€ë¬¸êµ¬    55\n",
      "ì€í‰êµ¬     54\n",
      "ë™ì‘êµ¬     53\n",
      "Name: count, dtype: int64\n",
      "\n",
      "ship-state: 2 unique values\n",
      "ship-state\n",
      "ì„œìš¸     976\n",
      "ê²½ê¸°ë„    244\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Gender: 2 unique values\n",
      "Gender\n",
      "F    612\n",
      "M    608\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Age Group: 6 unique values\n",
      "Age Group\n",
      "30ëŒ€    412\n",
      "50ëŒ€    301\n",
      "40ëŒ€    298\n",
      "20ëŒ€    112\n",
      "60ëŒ€     96\n",
      "40       1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Date range:\n",
      "Start date: 2024-01-01 00:00:00\n",
      "End date: 2024-06-30 00:00:00\n",
      "\n",
      "\u001b[0m\u001b[97mNow\u001b[0m\u001b[97m let's analyze basic metrics\u001b[0m\u001b[97m like sales volume\u001b[0m\u001b[97m, revenue, and customer\u001b[0m\u001b[97m behavior:\u001b[0m"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[30]\u001b[39m\u001b[32m, line 49\u001b[39m\n\u001b[32m     47\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mtext/event-stream\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m boto3_response.get(\u001b[33m\"\u001b[39m\u001b[33mcontentType\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m     48\u001b[39m     content = []\n\u001b[32m---> \u001b[39m\u001b[32m49\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mevent\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mboto3_response\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mresponse\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43miter_lines\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunk_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     50\u001b[39m \u001b[43m        \u001b[49m\u001b[43mevent\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mparse_sse_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevent\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     51\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mevent\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m:\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# None ì²´í¬ ì¶”ê°€\u001b[39;49;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/myenv/lib/python3.12/site-packages/botocore/response.py:157\u001b[39m, in \u001b[36mStreamingBody.iter_lines\u001b[39m\u001b[34m(self, chunk_size, keepends)\u001b[39m\n\u001b[32m    151\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Return an iterator to yield lines from the raw stream.\u001b[39;00m\n\u001b[32m    152\u001b[39m \n\u001b[32m    153\u001b[39m \u001b[33;03mThis is achieved by reading chunk of bytes (of size chunk_size) at a\u001b[39;00m\n\u001b[32m    154\u001b[39m \u001b[33;03mtime from the raw stream, and then yielding lines from there.\u001b[39;00m\n\u001b[32m    155\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    156\u001b[39m pending = \u001b[33mb\u001b[39m\u001b[33m'\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m157\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43miter_chunks\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunk_size\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    158\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlines\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mpending\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43msplitlines\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    159\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mline\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mlines\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m:\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/myenv/lib/python3.12/site-packages/botocore/response.py:170\u001b[39m, in \u001b[36mStreamingBody.iter_chunks\u001b[39m\u001b[34m(self, chunk_size)\u001b[39m\n\u001b[32m    166\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Return an iterator to yield chunks of chunk_size bytes from the raw\u001b[39;00m\n\u001b[32m    167\u001b[39m \u001b[33;03mstream.\u001b[39;00m\n\u001b[32m    168\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    169\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m170\u001b[39m     current_chunk = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunk_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    171\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m current_chunk == \u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    172\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/myenv/lib/python3.12/site-packages/botocore/response.py:98\u001b[39m, in \u001b[36mStreamingBody.read\u001b[39m\u001b[34m(self, amt)\u001b[39m\n\u001b[32m     93\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Read at most amt bytes from the stream.\u001b[39;00m\n\u001b[32m     94\u001b[39m \n\u001b[32m     95\u001b[39m \u001b[33;03mIf the amt argument is omitted, read all data.\u001b[39;00m\n\u001b[32m     96\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     97\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m98\u001b[39m     chunk = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_raw_stream\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     99\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m URLLib3ReadTimeoutError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    100\u001b[39m     \u001b[38;5;66;03m# TODO: the url will be None as urllib3 isn't setting it yet\u001b[39;00m\n\u001b[32m    101\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m ReadTimeoutError(endpoint_url=e.url, error=e)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/myenv/lib/python3.12/site-packages/urllib3/response.py:980\u001b[39m, in \u001b[36mHTTPResponse.read\u001b[39m\u001b[34m(self, amt, decode_content, cache_content)\u001b[39m\n\u001b[32m    977\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m._decoded_buffer) >= amt:\n\u001b[32m    978\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._decoded_buffer.get(amt)\n\u001b[32m--> \u001b[39m\u001b[32m980\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_raw_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    982\u001b[39m flush_decoder = amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m (amt != \u001b[32m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data)\n\u001b[32m    984\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m._decoded_buffer) == \u001b[32m0\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/myenv/lib/python3.12/site-packages/urllib3/response.py:904\u001b[39m, in \u001b[36mHTTPResponse._raw_read\u001b[39m\u001b[34m(self, amt, read1)\u001b[39m\n\u001b[32m    901\u001b[39m fp_closed = \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m._fp, \u001b[33m\"\u001b[39m\u001b[33mclosed\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m    903\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._error_catcher():\n\u001b[32m--> \u001b[39m\u001b[32m904\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_fp_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mread1\u001b[49m\u001b[43m=\u001b[49m\u001b[43mread1\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m fp_closed \u001b[38;5;28;01melse\u001b[39;00m \u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    905\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m amt != \u001b[32m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data:\n\u001b[32m    906\u001b[39m         \u001b[38;5;66;03m# Platform-specific: Buggy versions of Python.\u001b[39;00m\n\u001b[32m    907\u001b[39m         \u001b[38;5;66;03m# Close the connection when no data is returned\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    912\u001b[39m         \u001b[38;5;66;03m# not properly close the connection in all cases. There is\u001b[39;00m\n\u001b[32m    913\u001b[39m         \u001b[38;5;66;03m# no harm in redundantly calling close.\u001b[39;00m\n\u001b[32m    914\u001b[39m         \u001b[38;5;28mself\u001b[39m._fp.close()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/myenv/lib/python3.12/site-packages/urllib3/response.py:887\u001b[39m, in \u001b[36mHTTPResponse._fp_read\u001b[39m\u001b[34m(self, amt, read1)\u001b[39m\n\u001b[32m    884\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._fp.read1(amt) \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m._fp.read1()\n\u001b[32m    885\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    886\u001b[39m     \u001b[38;5;66;03m# StringIO doesn't like amt=None\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m887\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_fp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m._fp.read()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/myenv/lib/python3.12/http/client.py:473\u001b[39m, in \u001b[36mHTTPResponse.read\u001b[39m\u001b[34m(self, amt)\u001b[39m\n\u001b[32m    470\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    472\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.chunked:\n\u001b[32m--> \u001b[39m\u001b[32m473\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_read_chunked\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    475\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m amt >= \u001b[32m0\u001b[39m:\n\u001b[32m    476\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.length \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m amt > \u001b[38;5;28mself\u001b[39m.length:\n\u001b[32m    477\u001b[39m         \u001b[38;5;66;03m# clip the read to the \"end of response\"\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/myenv/lib/python3.12/http/client.py:597\u001b[39m, in \u001b[36mHTTPResponse._read_chunked\u001b[39m\u001b[34m(self, amt)\u001b[39m\n\u001b[32m    595\u001b[39m value = []\n\u001b[32m    596\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m597\u001b[39m     \u001b[38;5;28;01mwhile\u001b[39;00m (chunk_left := \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_chunk_left\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    598\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m amt <= chunk_left:\n\u001b[32m    599\u001b[39m             value.append(\u001b[38;5;28mself\u001b[39m._safe_read(amt))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/myenv/lib/python3.12/http/client.py:579\u001b[39m, in \u001b[36mHTTPResponse._get_chunk_left\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    577\u001b[39m     \u001b[38;5;28mself\u001b[39m._safe_read(\u001b[32m2\u001b[39m)  \u001b[38;5;66;03m# toss the CRLF at the end of the chunk\u001b[39;00m\n\u001b[32m    578\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m579\u001b[39m     chunk_left = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_read_next_chunk_size\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    580\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m:\n\u001b[32m    581\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m IncompleteRead(\u001b[33mb\u001b[39m\u001b[33m'\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/myenv/lib/python3.12/http/client.py:539\u001b[39m, in \u001b[36mHTTPResponse._read_next_chunk_size\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    537\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_read_next_chunk_size\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    538\u001b[39m     \u001b[38;5;66;03m# Read the next chunk size from the file\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m539\u001b[39m     line = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_MAXLINE\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    540\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(line) > _MAXLINE:\n\u001b[32m    541\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m LineTooLong(\u001b[33m\"\u001b[39m\u001b[33mchunk size\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/myenv/lib/python3.12/socket.py:720\u001b[39m, in \u001b[36mSocketIO.readinto\u001b[39m\u001b[34m(self, b)\u001b[39m\n\u001b[32m    718\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m    719\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m720\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sock\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    721\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[32m    722\u001b[39m         \u001b[38;5;28mself\u001b[39m._timeout_occurred = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/myenv/lib/python3.12/ssl.py:1251\u001b[39m, in \u001b[36mSSLSocket.recv_into\u001b[39m\u001b[34m(self, buffer, nbytes, flags)\u001b[39m\n\u001b[32m   1247\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m flags != \u001b[32m0\u001b[39m:\n\u001b[32m   1248\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1249\u001b[39m           \u001b[33m\"\u001b[39m\u001b[33mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m %\n\u001b[32m   1250\u001b[39m           \u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1251\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1252\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1253\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m().recv_into(buffer, nbytes, flags)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/myenv/lib/python3.12/ssl.py:1103\u001b[39m, in \u001b[36mSSLSocket.read\u001b[39m\u001b[34m(self, len, buffer)\u001b[39m\n\u001b[32m   1101\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1102\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1103\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sslobj\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1104\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1105\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sslobj.read(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import json\n",
    "import boto3\n",
    "from boto3.session import Session\n",
    "from src.utils.strands_sdk_utils import strands_utils\n",
    "\n",
    "boto_session = Session()\n",
    "region = boto_session.region_name\n",
    "\n",
    "def parse_sse_data(sse_bytes):\n",
    "    if not sse_bytes or len(sse_bytes) == 0:\n",
    "        return None\n",
    "\n",
    "    try:\n",
    "        text = sse_bytes.decode('utf-8').strip()\n",
    "        if not text or text == '': return None\n",
    "\n",
    "        if text.startswith('data: '):\n",
    "            json_text = text[6:].strip()\n",
    "            if json_text: return json.loads(json_text)\n",
    "        else:\n",
    "            return json.loads(text)\n",
    "\n",
    "    except Exception as e:\n",
    "        pass\n",
    "\n",
    "    return None\n",
    "\n",
    "agent_arn = launch_result.agent_arn\n",
    "from botocore.config import Config\n",
    "\n",
    "my_config = Config(\n",
    "    connect_timeout=60*100,\n",
    "    read_timeout=60*5,\n",
    ")\n",
    "\n",
    "agentcore_client = boto3.client(\n",
    "    'bedrock-agentcore',\n",
    "    region_name=region,\n",
    "    config=my_config,\n",
    ")\n",
    "\n",
    "boto3_response = agentcore_client.invoke_agent_runtime(\n",
    "    agentRuntimeArn=agent_arn,\n",
    "    qualifier=\"DEFAULT\",\n",
    "    payload=json.dumps({\"prompt\": 'ì´ê²ƒì€ ì•„ë§ˆì¡´ ìƒí’ˆíŒë§¤ ë°ì´í„°ë¥¼ ë¶„ì„í•˜ê³  ì‹¶ìŠµë‹ˆë‹¤. ë¶„ì„ëŒ€ìƒì€ \"./data/Dat-fresh-food-claude.csv\" íŒŒì¼ ì…ë‹ˆë‹¤. ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë§ˆì¼€íŒ… ì¸ì‚¬ì´íŠ¸ ì¶”ì¶œì„ ìœ„í•œ ë¶„ì„ì„ ì§„í–‰í•´ ì£¼ì„¸ìš”. ë¶„ì„ì€ ê¸°ë³¸ì ì¸ ë°ì´í„° ì†ì„± íƒìƒ‰ ë¶€í„°, ìƒí’ˆ íŒë§¤ íŠ¸ë Œë“œ, ë³€ìˆ˜ ê´€ê³„, ë³€ìˆ˜ ì¡°í•© ë“± ë‹¤ì–‘í•œ ë¶„ì„ ê¸°ë²•ì„ ìˆ˜í–‰í•´ ì£¼ì„¸ìš”. ë°ì´í„° ë¶„ì„ í›„ ì¸ì‚¬ì´íŠ¸ ì¶”ì¶œì— í•„ìš”í•œ ì‚¬í•­ì´ ìˆë‹¤ë©´ ê·¸ë¥¼ ìœ„í•œ ì¶”ê°€ ë¶„ì„ë„ ìˆ˜í–‰í•´ ì£¼ì„¸ìš”. ë¶„ì„ ë¦¬í¬íŠ¸ëŠ” ìƒì„¸ ë¶„ì„ê³¼ ê·¸ ê²ƒì„ ë’·ë°›ì¹¨ í•  ìˆ˜ ìˆëŠ” ì´ë¯¸ì§€ ë° ì°¨íŠ¸ë¥¼ í•¨ê»˜ ì‚½ì…í•´ ì£¼ì„¸ìš”. ìµœì¢… ë¦¬í¬íŠ¸ëŠ” pdf í˜•íƒœë¡œ ì €ì¥í•´ ì£¼ì„¸ìš”.'})\n",
    ")\n",
    "if \"text/event-stream\" in boto3_response.get(\"contentType\", \"\"):\n",
    "    content = []\n",
    "    for event in boto3_response[\"response\"].iter_lines(chunk_size=1):\n",
    "        event = parse_sse_data(event)\n",
    "        if event is None:  # None ì²´í¬ ì¶”ê°€\n",
    "            continue\n",
    "        else:\n",
    "            strands_utils.process_event_for_display(event)\n",
    "else:\n",
    "    try:\n",
    "        events = []\n",
    "        for event in boto3_response.get(\"response\", []):\n",
    "            print (\"6\", event)\n",
    "            events.append(event)\n",
    "    except Exception as e:\n",
    "        events = [f\"Error reading EventStream: {e}\"]\n",
    "    display(Markdown(json.loads(events[0].decode(\"utf-8\"))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fefa09f2-d25a-483f-aedb-11690bb8923a",
   "metadata": {},
   "source": [
    "í˜¸ì¶œ ê²°ê³¼ëŠ” ì•„ë˜ì™€ ê°™ì€ í˜•íƒœë¡œ ì• í”Œë¦¬ì¼€ì´ì…˜ì— í¬í•¨í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11249103-cfb3-47b5-970d-981a977a225a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import Markdown, display\n",
    "import json\n",
    "response_text = json.loads(invoke_response['response'][0].decode(\"utf-8\"))\n",
    "display(Markdown(response_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c1d2bce-be41-478c-8bed-b4037c385795",
   "metadata": {},
   "source": [
    "#### boto3ë¡œ AgentCore Runtime í˜¸ì¶œ\n",
    "\n",
    "AgentCore Runtimeì´ ìƒì„±ë˜ì—ˆìœ¼ë¯€ë¡œ ì´ì œ ëª¨ë“  AWS SDKë¡œ í˜¸ì¶œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, boto3ì˜ `invoke_agent_runtime` ë©”ì„œë“œë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7f84e68d-6c04-41b9-bf5b-60edc3fa0985",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "agent_arn = 'arn:aws:bedrock-agentcore:us-west-2:654654304740:runtime/manus_runtime_1147-aIOtxv3w50' #launch_result.agent_arn\n",
    "agentcore_client = boto3.client(\n",
    "    'bedrock-agentcore',\n",
    "    region_name='us-west-2'\n",
    ")\n",
    "\n",
    "boto3_response = agentcore_client.invoke_agent_runtime(\n",
    "    agentRuntimeArn=agent_arn,\n",
    "    qualifier=\"DEFAULT\",\n",
    "    payload=json.dumps({\"prompt\": \"ì•ˆë…• ë‚˜ëŠ” ì¥ë™ì§„ì´ë¼ê³  í•´. ë§Œë‚˜ì„œ ë°˜ê°€ì›Œ ë‚˜ëŠ” ë°ì´í„°ë¥¼ ì œê³µí•˜ê³  ê·¸ê²ƒìœ¼ë¡œ ë¶€í„° ì¸ì‚¬ì´íŠ¸ë¥¼ ì¶”ì¶œí•˜ê³  ì‹¶ì–´.\"})\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2dd4c970",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ResponseMetadata': {'RequestId': '2bc53297-2bfe-4d7c-b740-e68e0a02f319',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'date': 'Fri, 12 Sep 2025 03:08:55 GMT',\n",
       "   'content-type': 'text/event-stream; charset=utf-8',\n",
       "   'transfer-encoding': 'chunked',\n",
       "   'connection': 'keep-alive',\n",
       "   'x-amzn-requestid': '2bc53297-2bfe-4d7c-b740-e68e0a02f319',\n",
       "   'baggage': 'Self=1-68c38ec7-7ed904b13d57214b380ee867,session.id=8183b63b-8637-4d58-90df-21d645d5c2ff',\n",
       "   'x-amzn-bedrock-agentcore-runtime-session-id': '8183b63b-8637-4d58-90df-21d645d5c2ff',\n",
       "   'x-amzn-trace-id': 'Root=1-68c38ec7-67221e4e30d1e62812fb7db8;Self=1-68c38ec7-7ed904b13d57214b380ee867'},\n",
       "  'RetryAttempts': 0},\n",
       " 'runtimeSessionId': '8183b63b-8637-4d58-90df-21d645d5c2ff',\n",
       " 'traceId': 'Root=1-68c38ec7-67221e4e30d1e62812fb7db8;Self=1-68c38ec7-7ed904b13d57214b380ee867',\n",
       " 'baggage': 'Self=1-68c38ec7-7ed904b13d57214b380ee867,session.id=8183b63b-8637-4d58-90df-21d645d5c2ff',\n",
       " 'contentType': 'text/event-stream; charset=utf-8',\n",
       " 'statusCode': 200,\n",
       " 'response': <botocore.response.StreamingBody at 0x7fa30626b070>}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "boto3_response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d3fdfe404469632",
   "metadata": {},
   "source": [
    "### (Optional) Step 5. ë¦¬ì†ŒìŠ¤ ì •ë¦¬\n",
    "\n",
    "ì‹¤ìŠµì„ ìœ„í•´ ìƒì„±í–ˆë˜ ë¦¬ì†ŒìŠ¤ë¥¼ ì‚­ì œí•˜ëŠ” ê³¼ì •ì…ë‹ˆë‹¤. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08f86824-c775-4ad4-aaee-f18e8cf390b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "launch_result.ecr_uri, launch_result.agent_id, launch_result.ecr_uri.split('/')[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76a6cf1416830a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "agentcore_control_client = boto3.client(\n",
    "    'bedrock-agentcore-control',\n",
    "    region_name=region\n",
    ")\n",
    "ecr_client = boto3.client(\n",
    "    'ecr',\n",
    "    region_name=region\n",
    "    \n",
    ")\n",
    "\n",
    "iam_client = boto3.client('iam')\n",
    "\n",
    "runtime_delete_response = agentcore_control_client.delete_agent_runtime(\n",
    "    agentRuntimeId=launch_result.agent_id\n",
    ")\n",
    "\n",
    "response = ecr_client.delete_repository(\n",
    "    repositoryName=launch_result.ecr_uri.split('/')[1],\n",
    "    force=True\n",
    ")\n",
    "policies = iam_client.list_role_policies(\n",
    "    RoleName=agentcore_iam_role['Role']['RoleName'],\n",
    "    MaxItems=100\n",
    ")\n",
    "\n",
    "for policy_name in policies['PolicyNames']:\n",
    "    iam_client.delete_role_policy(\n",
    "        RoleName=agentcore_iam_role['Role']['RoleName'],\n",
    "        PolicyName=policy_name\n",
    "    )\n",
    "iam_response = iam_client.delete_role(\n",
    "    RoleName=agentcore_iam_role['Role']['RoleName']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b118ad38-feeb-4d1d-9d57-e5c845becc56",
   "metadata": {},
   "source": [
    "# Congratulations!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_myenv",
   "language": "python",
   "name": "conda_myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
