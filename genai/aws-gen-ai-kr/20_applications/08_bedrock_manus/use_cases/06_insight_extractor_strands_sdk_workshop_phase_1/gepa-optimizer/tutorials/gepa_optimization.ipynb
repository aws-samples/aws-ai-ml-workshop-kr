{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GEPA Prompt Optimization Tutorial\n",
    "\n",
    "This notebook demonstrates how to use the GEPA (Generate, Evaluate, Produce, Assess) optimizer to improve your prompts systematically.\n",
    "\n",
    "## What is GEPA?\n",
    "\n",
    "GEPA is a structured methodology for prompt optimization:\n",
    "\n",
    "1. **Generate**: Run tests with your current prompt\n",
    "2. **Evaluate**: Assess the quality of outputs using LLM judge\n",
    "3. **Produce**: Create an improved version based on feedback\n",
    "4. **Assess**: Validate the improvement and decide whether to continue\n",
    "\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add project root to path\n",
    "project_root = Path.cwd().parent\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "from src.optimizer import GEPAOptimizer, LLMEvaluator, TestCaseLoader, TestCaseBuilder\n",
    "from src.prompts.template import apply_prompt_template\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 1: Quick Validation\n",
    "\n",
    "Let's start by validating a simple prompt without optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a simple prompt\n",
    "simple_prompt = \"\"\"\n",
    "You are a helpful assistant that answers questions clearly and concisely.\n",
    "Always respond in a professional tone.\n",
    "\"\"\"\n",
    "\n",
    "# Create test cases\n",
    "builder = TestCaseBuilder()\n",
    "builder.add(\"What is Python?\", expected=\"Should provide clear definition\")\n",
    "builder.add(\"Explain machine learning\", expected=\"Should give concise explanation\")\n",
    "builder.add(\"Hello!\", expected=\"Should greet professionally\")\n",
    "\n",
    "test_cases = builder.build()\n",
    "print(f\"Created {len(test_cases)} test cases\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a simple evaluator\n",
    "async def simple_evaluator(result):\n",
    "    evaluator = LLMEvaluator(\n",
    "        criteria={\n",
    "            \"clarity\": \"Response should be clear and easy to understand\",\n",
    "            \"conciseness\": \"Response should be concise, not verbose\",\n",
    "            \"professionalism\": \"Tone should be professional\"\n",
    "        },\n",
    "        guidelines=[\"Be helpful\", \"Stay on topic\"]\n",
    "    )\n",
    "    return await evaluator.evaluate(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create optimizer (we'll just use it for validation, no optimization)\n",
    "optimizer = GEPAOptimizer(\n",
    "    base_prompt=simple_prompt,\n",
    "    evaluation_fn=simple_evaluator,\n",
    "    agent_type=\"claude-sonnet-4\",\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Run just the generate + evaluate steps\n",
    "results = await optimizer.generate(test_cases)\n",
    "evaluation = await optimizer.evaluate(results)\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Score: {evaluation['avg_score']:.2f}\")\n",
    "print(f\"Pass Rate: {evaluation['pass_rate']:.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 2: Full Optimization\n",
    "\n",
    "Now let's optimize a prompt using the full GEPA cycle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test cases from file\n",
    "loader = TestCaseLoader()\n",
    "coordinator_tests = loader.load(\"coordinator_tests.yaml\")\n",
    "\n",
    "print(f\"Loaded {len(coordinator_tests)} test cases:\")\n",
    "for i, test in enumerate(coordinator_tests[:3], 1):\n",
    "    print(f\"{i}. {test['input'][:50]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load current coordinator prompt\n",
    "current_prompt = apply_prompt_template(\n",
    "    prompt_name=\"coordinator\",\n",
    "    prompt_context={\"CURRENT_TIME\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")}\n",
    ")\n",
    "\n",
    "print(f\"Current prompt length: {len(current_prompt)} characters\")\n",
    "print(f\"\\nFirst 200 chars:\\n{current_prompt[:200]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define custom evaluator for coordinator\n",
    "async def coordinator_evaluator(result):\n",
    "    output = result.get(\"output\", \"\")\n",
    "    metadata = result.get(\"metadata\", {})\n",
    "    \n",
    "    # Check structural correctness\n",
    "    has_handoff = \"handoff_to_planner\" in output.lower()\n",
    "    expected_action = metadata.get(\"expected_action\", \"\")\n",
    "    should_handoff = expected_action == \"handoff_to_planner\"\n",
    "    \n",
    "    # Use LLM evaluator\n",
    "    llm_eval = LLMEvaluator(\n",
    "        criteria={\n",
    "            \"handoff_logic\": \"Should correctly decide when to handoff to planner\",\n",
    "            \"response_quality\": \"Response should be appropriate and helpful\",\n",
    "            \"tone\": \"Should be friendly but professional\"\n",
    "        },\n",
    "        guidelines=[\n",
    "            \"Simple greetings handled directly\",\n",
    "            \"Complex tasks handed off to planner\",\n",
    "            \"Clear handoff marker when handing off\"\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    eval_result = await llm_eval.evaluate(result)\n",
    "    \n",
    "    # Adjust score based on handoff correctness\n",
    "    if has_handoff == should_handoff:\n",
    "        eval_result[\"score\"] = min(1.0, eval_result[\"score\"] + 0.1)\n",
    "    else:\n",
    "        eval_result[\"score\"] = max(0.0, eval_result[\"score\"] - 0.3)\n",
    "        eval_result[\"feedback\"] = f\"[HANDOFF ERROR] {eval_result['feedback']}\"\n",
    "    \n",
    "    return eval_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create optimizer with guidelines\n",
    "guidelines = [\n",
    "    \"Use clear XML tags for structure\",\n",
    "    \"Provide concrete examples\",\n",
    "    \"Define clear decision criteria\",\n",
    "    \"Be specific about constraints\"\n",
    "]\n",
    "\n",
    "improvement_context = \"\"\"\n",
    "**System Prompt Best Practices:**\n",
    "- Use structural tags like <role>, <instructions>, <constraints>\n",
    "- Provide examples for each major scenario\n",
    "- Define measurable success criteria\n",
    "- Specify both what to do and what NOT to do\n",
    "- Keep language clear and direct\n",
    "\"\"\"\n",
    "\n",
    "optimizer = GEPAOptimizer(\n",
    "    base_prompt=current_prompt,\n",
    "    evaluation_fn=coordinator_evaluator,\n",
    "    agent_type=\"claude-sonnet-4\",\n",
    "    guidelines=guidelines,\n",
    "    improvement_context=improvement_context,\n",
    "    enable_reasoning=False,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run optimization (limit to 2 iterations for demo)\n",
    "best_version = await optimizer.optimize(\n",
    "    test_cases=coordinator_tests,\n",
    "    max_iterations=2,\n",
    "    target_score=0.9,\n",
    "    min_improvement=0.05\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Review results\n",
    "if best_version:\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"OPTIMIZATION SUMMARY\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Versions created: {len(optimizer.versions)}\")\n",
    "    print(f\"Best version: {best_version.version}\")\n",
    "    print(f\"Best score: {best_version.score:.3f}\")\n",
    "    print(f\"Pass rate: {best_version.metadata.get('pass_rate', 0):.1%}\")\n",
    "    \n",
    "    print(\"\\nFirst 300 chars of optimized prompt:\")\n",
    "    print(best_version.prompt[:300] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 3: Compare Versions\n",
    "\n",
    "Let's compare all versions created during optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare scores across versions\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "versions = [v.version for v in optimizer.versions]\n",
    "scores = [v.score for v in optimizer.versions]\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(versions, scores, marker='o', linewidth=2, markersize=8)\n",
    "plt.xlabel('Version', fontsize=12)\n",
    "plt.ylabel('Score', fontsize=12)\n",
    "plt.title('GEPA Optimization Progress', fontsize=14, fontweight='bold')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.ylim(0, 1)\n",
    "plt.axhline(y=0.9, color='g', linestyle='--', label='Target Score')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nScore progression:\")\n",
    "for v in optimizer.versions:\n",
    "    print(f\"  Version {v.version}: {v.score:.3f} (Pass rate: {v.metadata.get('pass_rate', 0):.1%})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 4: Save and Export\n",
    "\n",
    "Save the optimized prompt and optimization history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save optimized prompt\n",
    "if best_version:\n",
    "    output_path = project_root / \"artifacts\" / \"coordinator_optimized.md\"\n",
    "    output_path.parent.mkdir(exist_ok=True)\n",
    "    \n",
    "    with open(output_path, 'w', encoding='utf-8') as f:\n",
    "        f.write(best_version.prompt)\n",
    "    \n",
    "    print(f\"✓ Saved optimized prompt to: {output_path}\")\n",
    "    \n",
    "    # Save history\n",
    "    history_path = project_root / \"artifacts\" / \"optimization_history.json\"\n",
    "    optimizer.save_history(str(history_path))\n",
    "    print(f\"✓ Saved optimization history to: {history_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 5: Creating Custom Test Cases\n",
    "\n",
    "Build custom test cases programmatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create test cases for a hypothetical agent\n",
    "builder = TestCaseBuilder()\n",
    "\n",
    "# Add various test cases\n",
    "builder.add(\n",
    "    \"Calculate the mean of [1, 2, 3, 4, 5]\",\n",
    "    expected=\"Should perform calculation and return 3\",\n",
    "    metadata={\"category\": \"math\", \"priority\": \"high\"}\n",
    ")\n",
    "\n",
    "builder.add(\n",
    "    \"What's the weather like?\",\n",
    "    expected=\"Should politely indicate inability to access weather data\",\n",
    "    metadata={\"category\": \"limitation\", \"priority\": \"medium\"}\n",
    ")\n",
    "\n",
    "builder.add(\n",
    "    \"Generate a Python function to sort a list\",\n",
    "    expected=\"Should generate working Python code with explanation\",\n",
    "    metadata={\"category\": \"code_generation\", \"priority\": \"high\"}\n",
    ")\n",
    "\n",
    "# Build and save\n",
    "custom_tests = builder.build()\n",
    "builder.save(\"custom_agent_tests.yaml\")\n",
    "\n",
    "print(f\"Created {len(custom_tests)} custom test cases\")\n",
    "print(\"Saved to: data/test_cases/custom_agent_tests.yaml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 6: Using Simple Evaluator\n",
    "\n",
    "For quick structural checks without LLM overhead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.optimizer.llm_evaluator import SimpleEvaluator\n",
    "\n",
    "# Create evaluator with structural rules\n",
    "simple_eval = SimpleEvaluator(\n",
    "    required_keywords=[\"handoff_to_planner\"],\n",
    "    min_length=10,\n",
    "    max_length=500\n",
    ")\n",
    "\n",
    "# Test it\n",
    "test_result = {\n",
    "    \"input\": \"Analyze data\",\n",
    "    \"output\": \"handoff_to_planner: I'll analyze this data for you.\"\n",
    "}\n",
    "\n",
    "eval_result = simple_eval.evaluate(test_result)\n",
    "print(f\"Score: {eval_result['score']:.2f}\")\n",
    "print(f\"Feedback: {eval_result['feedback']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best Practices Summary\n",
    "\n",
    "1. **Start with validation** before full optimization\n",
    "2. **Use realistic test cases** that cover edge cases\n",
    "3. **Define clear evaluation criteria** with specific metrics\n",
    "4. **Iterate gradually** (3-5 iterations usually sufficient)\n",
    "5. **Review improvements manually** before deploying\n",
    "6. **Save optimization history** for auditing\n",
    "7. **Combine evaluators** (Simple for structure, LLM for quality)\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "- Try optimizing other agents (planner, supervisor)\n",
    "- Create custom evaluators for domain-specific needs\n",
    "- Build comprehensive test suites for your use cases\n",
    "- Integrate GEPA into your CI/CD pipeline\n",
    "- Experiment with different guidelines and contexts"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
