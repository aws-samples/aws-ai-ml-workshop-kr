{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ff04f9fe",
   "metadata": {},
   "source": [
    "# Strands Agent SDK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "19932c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['AWS_DEFAULT_REGION'] = 'us-west-2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "550fff97",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "84656256",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "module_path = \"../..\"\n",
    "sys.path.append(os.path.abspath(module_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bae09ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from strands import Agent\n",
    "from strands.models import BedrockModel\n",
    "from src.utils.bedrock import bedrock_info\n",
    "from botocore.config import Config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "785d29c4",
   "metadata": {},
   "source": [
    "## 1. Utils"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb0b0d8a",
   "metadata": {},
   "source": [
    "### 1.1 Get llm model by inference type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9ee608cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(**kwargs):\n",
    "\n",
    "    llm_type = kwargs[\"llm_type\"]\n",
    "    cache_type = kwargs[\"cache_type\"]\n",
    "    enable_reasoning = kwargs[\"enable_reasoning\"]\n",
    "\n",
    "    if llm_type in [\"claude-sonnet-3-7\", \"claude-sonnet-4\"]:\n",
    "        \n",
    "        if llm_type == \"claude-sonnet-3-7\": model_name = \"Claude-V3-7-Sonnet-CRI\"\n",
    "        elif llm_type == \"claude-sonnet-4\": model_name = \"Claude-V4-Sonnet-CRI\"\n",
    "\n",
    "        ## BedrockModel params: https://strandsagents.com/latest/api-reference/models/?h=bedrockmodel#strands.models.bedrock.BedrockModel\n",
    "        llm = BedrockModel(\n",
    "            model_id=bedrock_info.get_model_id(model_name=model_name),\n",
    "            streaming=True,\n",
    "            max_tokens=8192*5,\n",
    "            stop_sequences=[\"\\n\\nHuman\"],\n",
    "            temperature=1 if enable_reasoning else 0.01, \n",
    "            additional_request_fields={\n",
    "                \"thinking\": {\n",
    "                    \"type\": \"enabled\" if enable_reasoning else \"disabled\", \n",
    "                    **({\"budget_tokens\": 8192} if enable_reasoning else {}),\n",
    "                }\n",
    "            },\n",
    "            cache_prompt=cache_type, # None/ephemeral/defalut\n",
    "            #cache_tools: Cache point type for tools\n",
    "            boto_client_config=Config(\n",
    "                read_timeout=900,\n",
    "                connect_timeout=900,\n",
    "                retries=dict(max_attempts=50, mode=\"adaptive\"),\n",
    "            )\n",
    "        )   \n",
    "    elif llm_type == \"claude-sonnet-3-5-v-2\":\n",
    "        ## BedrockModel params: https://strandsagents.com/latest/api-reference/models/?h=bedrockmodel#strands.models.bedrock.BedrockModel\n",
    "        llm = BedrockModel(\n",
    "            model_id=bedrock_info.get_model_id(model_name=\"Claude-V3-5-V-2-Sonnet-CRI\"),\n",
    "            streaming=True,\n",
    "            max_tokens=8192,\n",
    "            stop_sequences=[\"\\n\\nHuman\"],\n",
    "            temperature=0.01,\n",
    "            cache_prompt=cache_type, # None/ephemeral/defalut\n",
    "            #cache_tools: Cache point type for tools\n",
    "            boto_client_config=Config(\n",
    "                read_timeout=900,\n",
    "                connect_timeout=900,\n",
    "                retries=dict(max_attempts=50, mode=\"standard\"),\n",
    "            )\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown LLM type: {llm_type}\")\n",
    "\n",
    "    return llm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2357e8a0",
   "metadata": {},
   "source": [
    "### 1.2 Create agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e7460d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from strands.agent.conversation_manager import SlidingWindowConversationManager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b62acf74",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Colors:\n",
    "    BLUE = '\\033[94m'\n",
    "    GREEN = '\\033[92m'\n",
    "    YELLOW = '\\033[93m'\n",
    "    RED = '\\033[91m'\n",
    "    BOLD = '\\033[1m'\n",
    "    UNDERLINE = '\\033[4m'\n",
    "    END = '\\033[0m'\n",
    "\n",
    "def apply_prompt_template(prompt_name: str, prompt_context={}) -> str:\n",
    "    \n",
    "    system_prompts = open(os.path.join(\"./prompts\", f\"{prompt_name}.md\")).read()    \n",
    "    #system_prompts = open(os.path.join(os.path.dirname(__file__), f\"{prompt_name}.md\")).read()\n",
    "    context = {\"CURRENT_TIME\": datetime.now().strftime(\"%a %b %d %Y %H:%M:%S %z\")}\n",
    "    context.update(prompt_context)\n",
    "    system_prompts = system_prompts.format(**context)\n",
    "        \n",
    "    return system_prompts\n",
    "\n",
    "class ConversationEditor(SlidingWindowConversationManager):\n",
    "\n",
    "    \"\"\"\n",
    "    Manager that only operates on overflow.\n",
    "\n",
    "        Args:\n",
    "            window_size (int, optional): Maximum number of messages to retain when\n",
    "                context overflow occurs. Defaults to 20.\n",
    "            should_truncate_results (bool, optional): If True, truncate large tool\n",
    "                results with a placeholder message when overflow happens. If False,\n",
    "                preserve full tool results but remove more historical messages.\n",
    "                Defaults to True.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, window_size=7, should_truncate_results=False):\n",
    "        super().__init__(\n",
    "            window_size=window_size,\n",
    "            should_truncate_results=should_truncate_results\n",
    "        )\n",
    "\n",
    "    def apply_management(self, agent, **kwargs):\n",
    "        \"\"\"After each event loop - do nothing\"\"\"\n",
    "        print(\"None\")\n",
    "        pass\n",
    "\n",
    "    def reduce_context(self, agent, e=None, **kwargs):\n",
    "        \"\"\"Only on overflow - use parent class's reduce_context\"\"\"\n",
    "        print(f\"âš ï¸ Overflow occurred! Cleaning up {len(agent.messages)} messages...\")\n",
    "\n",
    "        # ë¶€ëª¨ í´ë˜ìŠ¤ì˜ reduce_contextëŠ” should_truncate_resultsë¥¼ ìë™ìœ¼ë¡œ ì²˜ë¦¬\n",
    "        super().reduce_context(agent, e, **kwargs)\n",
    "\n",
    "        print(f\"âœ… Cleanup complete: {len(agent.messages)} messages remaining\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1d40c135",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_agent(**kwargs):\n",
    "\n",
    "    agent_name, system_prompts = kwargs[\"agent_name\"], kwargs[\"system_prompts\"]\n",
    "    agent_type = kwargs.get(\"agent_type\", \"claude-sonnet-3-7\")\n",
    "    enable_reasoning = kwargs.get(\"enable_reasoning\", False)\n",
    "    prompt_cache_info = kwargs.get(\"prompt_cache_info\", (False, None)) # (True, \"default\")\n",
    "    tools = kwargs.get(\"tools\", None)\n",
    "    streaming = kwargs.get(\"streaming\", True)\n",
    "    \n",
    "    context_overflow_window_size = kwargs.get(\"context_overflow_window_size\", 15)\n",
    "    context_overflow_should_truncate_results = kwargs.get(\"context_overflow_should_truncate_results\", False)\n",
    "\n",
    "    prompt_cache, cache_type = prompt_cache_info\n",
    "    if prompt_cache: print(f\"{Colors.GREEN}{agent_name.upper()} - Prompt Cache Enabled{Colors.END}\")\n",
    "    else: print(f\"{Colors.GREEN}{agent_name.upper()} - Prompt Cache Disabled{Colors.END}\")\n",
    "\n",
    "    llm = get_model(llm_type=agent_type, cache_type=cache_type, enable_reasoning=enable_reasoning)\n",
    "    llm.config[\"streaming\"] = streaming\n",
    "\n",
    "    agent = Agent(\n",
    "        model=llm,\n",
    "        system_prompt=system_prompts,\n",
    "        tools=tools,\n",
    "        conversation_manager=ConversationEditor(\n",
    "            window_size=context_overflow_window_size,\n",
    "            should_truncate_results=context_overflow_should_truncate_results\n",
    "        ),\n",
    "        callback_handler=None # async iteratorë¡œ ëŒ€ì²´ í•˜ê¸° ë•Œë¬¸ì— None ì„¤ì •\n",
    "    )\n",
    "\n",
    "    return agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4471a8c5",
   "metadata": {},
   "source": [
    "### 1.3 Response with streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ee387c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "import traceback\n",
    "from langchain_core.callbacks.streaming_stdout import StreamingStdOutCallbackHandler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "092dfbec",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ColoredStreamingCallback(StreamingStdOutCallbackHandler):\n",
    "    COLORS = {\n",
    "        'blue': '\\033[94m',\n",
    "        'green': '\\033[92m',\n",
    "        'yellow': '\\033[93m',\n",
    "        'red': '\\033[91m',\n",
    "        'purple': '\\033[95m',\n",
    "        'cyan': '\\033[96m',\n",
    "        'white': '\\033[97m',\n",
    "    }\n",
    "    \n",
    "    def __init__(self, color='blue'):\n",
    "        super().__init__()\n",
    "        self.color_code = self.COLORS.get(color, '\\033[94m')\n",
    "        self.reset_code = '\\033[0m'\n",
    "    \n",
    "    def on_llm_new_token(self, token: str, **kwargs) -> None:\n",
    "        print(f\"{self.color_code}{token}{self.reset_code}\", end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cbe431dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def process_streaming_response(agent, message):\n",
    "    callback_reasoning, callback_answer = ColoredStreamingCallback('purple'), ColoredStreamingCallback('white')\n",
    "    response = {\"text\": \"\",\"reasoning\": \"\", \"signature\": \"\", \"tool_use\": None, \"cycle\": 0}\n",
    "    try:\n",
    "        agent_stream = agent.stream_async(message)\n",
    "        async for event in agent_stream:\n",
    "            if \"reasoningText\" in event:\n",
    "                response[\"reasoning\"] += event[\"reasoningText\"]\n",
    "                callback_reasoning.on_llm_new_token(event[\"reasoningText\"])\n",
    "            elif \"reasoning_signature\" in event:\n",
    "                response[\"signature\"] += event[\"reasoning_signature\"]\n",
    "            elif \"data\" in event:\n",
    "                response[\"text\"] += event[\"data\"]\n",
    "                callback_answer.on_llm_new_token(event[\"data\"])\n",
    "            elif \"current_tool_use\" in event and event[\"current_tool_use\"].get(\"name\"):\n",
    "                response[\"tool_use\"] = event[\"current_tool_use\"][\"name\"]\n",
    "                if \"event_loop_metrics\" in event:\n",
    "                    if response[\"cycle\"] != event[\"event_loop_metrics\"].cycle_count:\n",
    "                        response[\"cycle\"] = event[\"event_loop_metrics\"].cycle_count\n",
    "                        callback_answer.on_llm_new_token(f' \\n## Calling tool: {event[\"current_tool_use\"][\"name\"]} - # Cycle: {event[\"event_loop_metrics\"].cycle_count}\\n')\n",
    "    except Exception as e:\n",
    "        print(f\"Error in streaming response: {e}\")\n",
    "        print(traceback.format_exc())  # Detailed error logging\n",
    "    \n",
    "    return agent, response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd33c01f",
   "metadata": {},
   "source": [
    "## 2. Usage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4becb911",
   "metadata": {},
   "source": [
    "### 2.1 Agent definition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeb145e2",
   "metadata": {},
   "source": [
    "- system prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "384f6e22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./prompts/task_agent.md\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./prompts/task_agent.md\n",
    "---\n",
    "CURRENT_TIME: {CURRENT_TIME}\n",
    "---\n",
    "\n",
    "You are Bedrock-Manus, a friendly AI assistant developed by AWS AIML Specialist SA Dongjin Jang.\n",
    "You specialize in handling greetings, small talk, and knowledge-based question answering using available tools.\n",
    "\n",
    "## Available Tools\n",
    "\n",
    "You have access to the following tools that you should use when appropriate:\n",
    "\n",
    "### 1. RAG Tool (rag_tool)\n",
    "**When to use**: Use this tool when users ask questions that require information from a knowledge base or document collection. This includes:\n",
    "- Questions about specific topics that might be documented\n",
    "- Requests for factual information that could be in indexed documents\n",
    "- Queries about policies, procedures, or technical documentation\n",
    "- Any question where you need to retrieve and reference specific information\n",
    "\n",
    "**What it does**: Performs Retrieval-Augmented Generation (RAG) by searching through indexed documents in OpenSearch and generating contextual answers based on retrieved information.\n",
    "\n",
    "**Input**: A query string containing the user's question\n",
    "\n",
    "**Example scenarios**:\n",
    "- \"What is the investment return rate for maturity repayment?\"\n",
    "- \"Can you explain the company's vacation policy?\"\n",
    "- \"How does the authentication system work?\"\n",
    "\n",
    "### 2. Python REPL Tool (python_repl_tool)\n",
    "**When to use**: Use this tool when users need to execute Python code or perform data analysis:\n",
    "- Running Python scripts or code snippets\n",
    "- Data analysis and calculations\n",
    "- Testing code functionality\n",
    "- Mathematical computations\n",
    "\n",
    "**What it does**: Executes Python code in a REPL environment and returns the output\n",
    "\n",
    "**Input**: Python code string\n",
    "\n",
    "### 3. Bash Tool (bash_tool) \n",
    "**When to use**: Use this tool when users need to execute system commands or perform file operations:\n",
    "- Running shell commands\n",
    "- File system operations (ls, mkdir, etc.)\n",
    "- System information queries\n",
    "- Development tasks requiring command line operations\n",
    "\n",
    "**What it does**: Executes bash commands and returns the output\n",
    "\n",
    "**Input**: A bash command string\n",
    "\n",
    "## Tool Usage Guidelines\n",
    "\n",
    "1. **Assess the user's request** - Determine if the question requires tool usage\n",
    "2. **Choose the appropriate tool** - Select based on the type of information needed\n",
    "3. **Use RAG tool for knowledge queries** - When the user asks about topics that might be in your knowledge base\n",
    "4. **Use Python REPL for code execution** - When the user needs to run Python code or perform calculations\n",
    "5. **Use Bash tool for system operations** - When the user needs to interact with the system\n",
    "6. **Provide helpful responses** - Always explain the results in a user-friendly way\n",
    "\n",
    "## Response Style\n",
    "\n",
    "- Be friendly and conversational\n",
    "- Provide clear, helpful answers\n",
    "- When using tools, explain what you're doing and why\n",
    "- If a tool doesn't provide the needed information, acknowledge this and offer alternatives\n",
    "- Always prioritize user experience and clarity\n",
    "\n",
    "Remember to use tools proactively when they can help answer user questions more accurately or completely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ec1caa4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92mTASK_AGENT - Prompt Cache Enabled\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "agent = get_agent(\n",
    "    agent_name=\"task_agent\",\n",
    "    system_prompts=apply_prompt_template(prompt_name=\"task_agent\", prompt_context={}),\n",
    "    agent_type=\"claude-sonnet-4\",  # claude-sonnet-3-7, claude-sonnet-4\n",
    "    prompt_cache_info=(True, \"default\"),  # enable prompt caching for reasoning agent\n",
    "    streaming=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "945cffbb",
   "metadata": {},
   "source": [
    "### 2.2 Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a589b447",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1093c701",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[97mì•ˆë…•í•˜\u001b[0m\u001b[97mì„¸ìš” \u001b[0m\u001b[97mì¥ë™ì§„ë‹˜\u001b[0m\u001b[97m! ë°˜\u001b[0m\u001b[97mê°‘ìŠµë‹ˆë‹¤\u001b[0m\u001b[97m!\u001b[0m\u001b[97m \u001b[0m\u001b[97mğŸ˜Š\n",
      "\n",
      "ì €ëŠ” \u001b[0m\u001b[97mBedrock-\u001b[0m\u001b[97mManusì…\u001b[0m\u001b[97më‹ˆë‹¤. AWS\u001b[0m\u001b[97m AIML Specialist\u001b[0m\u001b[97m SAì¸\u001b[0m\u001b[97m ì¥\u001b[0m\u001b[97më™ì§„ë‹˜\u001b[0m\u001b[97mì´ ê°œ\u001b[0m\u001b[97më°œí•´\u001b[0m\u001b[97mì£¼ì‹  AI \u001b[0m\u001b[97mì–´ì‹œìŠ¤í„´\u001b[0m\u001b[97míŠ¸ì˜ˆ\u001b[0m\u001b[97mìš”. \n",
      "\n",
      "ì˜¤\u001b[0m\u001b[97mëŠ˜ ì–´\u001b[0m\u001b[97më–¤ ë„ì›€\u001b[0m\u001b[97mì´ í•„ìš”í•˜\u001b[0m\u001b[97mì‹ \u001b[0m\u001b[97mê°€\u001b[0m\u001b[97mìš”? \u001b[0m\u001b[97mê¶ê¸ˆí•œ ê²ƒ\u001b[0m\u001b[97mì´ ìˆìœ¼ì‹œ\u001b[0m\u001b[97mê±°\u001b[0m\u001b[97më‚˜ ëŒ€\u001b[0m\u001b[97mí™”í•˜\u001b[0m\u001b[97mê³  ì‹¶ì€\u001b[0m\u001b[97m ì£¼\u001b[0m\u001b[97mì œê°€ ìˆìœ¼\u001b[0m\u001b[97mì‹œë©´ ì–¸ì œ\u001b[0m\u001b[97më“  ë§\u001b[0m\u001b[97mì”€í•´ \u001b[0m\u001b[97mì£¼ì„¸ìš”!\u001b[0mNone\n"
     ]
    }
   ],
   "source": [
    "message = \"ì•ˆë…• ë‚˜ëŠ” ì¥ë™ì§„ì´ì•¼\"\n",
    "agent, response = asyncio.run(process_streaming_response(agent, message))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "359c30d4",
   "metadata": {},
   "source": [
    "## 3. Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3e12e658",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n"
     ]
    }
   ],
   "source": [
    "from src.tools import python_repl_tool, bash_tool, rag_tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "74af9e0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92mTASK_AGENT - Prompt Cache Enabled\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "agent = get_agent(\n",
    "    agent_name=\"task_agent\",\n",
    "    system_prompts=apply_prompt_template(prompt_name=\"task_agent\", prompt_context={}),\n",
    "    agent_type=\"claude-sonnet-4\",  # claude-sonnet-3-7, claude-sonnet-4\n",
    "    prompt_cache_info=(True, \"default\"),  # enable prompt caching for reasoning agent\n",
    "    enable_reasoning=True,\n",
    "    streaming=True,\n",
    "    tools=[python_repl_tool, bash_tool, rag_tool],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9d4766a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[95mThe user is asking\u001b[0m\u001b[95m to check what files are\u001b[0m\u001b[95m in the\u001b[0m\u001b[95m \"./\u001b[0m\u001b[95mprompts\" directory in\u001b[0m\u001b[95m Korean. This is a\u001b[0m\u001b[95m request\u001b[0m\u001b[95m to check the\u001b[0m\u001b[95m contents of a directory,\u001b[0m\u001b[95m which is a file\u001b[0m\u001b[95m system operation. I shoul\u001b[0m\u001b[95md use the bash_tool\u001b[0m\u001b[95m to execute an\u001b[0m\u001b[95m `ls` command to\u001b[0m\u001b[95m list the contents of the\u001b[0m\u001b[95m ./\u001b[0m\u001b[95mprompts directory.\u001b[0m\u001b[97m./prompts ë””\u001b[0m\u001b[97më ‰í† ë¦¬ì˜\u001b[0m\u001b[97m íŒŒì¼ ëª©\u001b[0m\u001b[97më¡ì„ í™•ì¸\u001b[0m\u001b[97mí•´ë“œ\u001b[0m\u001b[97më¦¬ê² ìŠµë‹ˆ\u001b[0m\u001b[97më‹¤.\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "INFO [src.tools.bash_tool] \u001b[92m===== Executing Bash =====\u001b[0m\n",
      "\n",
      "INFO [src.tools.bash_tool] \u001b[1m===== Coder - Command: ls -la ./prompts =====\u001b[0m\n",
      "\n",
      "INFO [src.tools.decorators] \u001b[91m\n",
      "Tool - handle_bash_tool returned:\n",
      "ls -la ./prompts||total 12\n",
      "drwxrwxr-x 2 ubuntu ubuntu 4096 Oct 14 09:25 .\n",
      "drwxrwxr-x 3 ubuntu ubuntu 4096 Oct 14 09:25 ..\n",
      "-rw-rw-r-- 1 ubuntu ubuntu 2959 Oct 25 08:48 task_agent.md\n",
      "\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[97m./prompts ë””\u001b[0m\u001b[97më ‰í† ë¦¬ì—\u001b[0m\u001b[97mëŠ” ë‹¤ìŒ \u001b[0m\u001b[97míŒŒì¼ì´ ìˆ\u001b[0m\u001b[97mìŠµë‹ˆë‹¤:\u001b[0m\u001b[97m\n",
      "\n",
      "-\u001b[0m\u001b[97m **task_agent.\u001b[0m\u001b[97mmd** (\u001b[0m\u001b[97m2\u001b[0m\u001b[97m,\u001b[0m\u001b[97m959 bytes,\u001b[0m\u001b[97m 2025\u001b[0m\u001b[97më…„\u001b[0m\u001b[97m 10ì›” 25\u001b[0m\u001b[97mì¼ ì˜¤\u001b[0m\u001b[97mì „ 8ì‹œ\u001b[0m\u001b[97m 48ë¶„ì—\u001b[0m\u001b[97m ë§ˆ\u001b[0m\u001b[97mì§€ë§‰ìœ¼\u001b[0m\u001b[97më¡œ ìˆ˜ì •\u001b[0m\u001b[97më¨)\n",
      "\n",
      "ì´ \u001b[0m\u001b[97më””\u001b[0m\u001b[97më ‰í† ë¦¬ì—\u001b[0m\u001b[97mëŠ”\u001b[0m\u001b[97m í˜„\u001b[0m\u001b[97mì¬ í•˜\u001b[0m\u001b[97më‚˜ì˜ ë§ˆ\u001b[0m\u001b[97mí¬ë‹¤ìš´ íŒŒ\u001b[0m\u001b[97mì¼ë§Œ\u001b[0m\u001b[97m í¬\u001b[0m\u001b[97mí•¨ë˜ì–´ ìˆ\u001b[0m\u001b[97më„¤\u001b[0m\u001b[97mìš”. task\u001b[0m\u001b[97m_agent.md \u001b[0m\u001b[97míŒŒì¼ì˜\u001b[0m\u001b[97m ë‚´ìš©ì„ \u001b[0m\u001b[97mí™•ì¸í•˜\u001b[0m\u001b[97mê³ \u001b[0m\u001b[97m ì‹¶ìœ¼ì‹œ\u001b[0m\u001b[97më©´\u001b[0m\u001b[97m ì•Œ\u001b[0m\u001b[97më ¤ì£¼ì„¸\u001b[0m\u001b[97mìš”!\u001b[0mNone\n"
     ]
    }
   ],
   "source": [
    "message = \"./prompts ë””ë ‰í† ë¦¬ì— ì–´ë–¤ íŒŒì¼ì´ ìˆëŠ”ì§€ í™•ì¸í•´ ì¤„ë˜?\"\n",
    "agent, response = asyncio.run(process_streaming_response(agent, message))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3378f134",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[95mì‚¬ìš©ìê°€\u001b[0m\u001b[95m \"Hello world\"\u001b[0m\u001b[95më¥¼ í”„ë¦°\u001b[0m\u001b[95míŒ…í•˜ëŠ” íŒŒ\u001b[0m\u001b[95mì´ì¬ ì½”\u001b[0m\u001b[95më“œë¥¼ ì‘ì„±\u001b[0m\u001b[95mí•˜ê³  ì‹¤í–‰\u001b[0m\u001b[95mí•´\u001b[0m\u001b[95më‹¬ë¼ê³  ìš”\u001b[0m\u001b[95mì²­í–ˆìŠµë‹ˆë‹¤\u001b[0m\u001b[95m. ì´ëŠ” \u001b[0m\u001b[95mpython\u001b[0m\u001b[95m_\u001b[0m\u001b[95mrepl_toolì„\u001b[0m\u001b[95m ì‚¬ìš©í•´\u001b[0m\u001b[95mì•¼ í•˜\u001b[0m\u001b[95mëŠ” ìƒ\u001b[0m\u001b[95mí™©ì…ë‹ˆë‹¤.\u001b[0m\u001b[95m\n",
      "\n",
      "ê°„\u001b[0m\u001b[95më‹¨í•œ print\u001b[0m\u001b[95m(\"\u001b[0m\u001b[95mHello world\")\u001b[0m\u001b[95m ì½”\u001b[0m\u001b[95më“œë¥¼ ì‘ì„±\u001b[0m\u001b[95mí•˜\u001b[0m\u001b[95mê³  ì‹¤í–‰í•˜\u001b[0m\u001b[95më©´\u001b[0m\u001b[95m ë©ë‹ˆë‹¤\u001b[0m\u001b[95m.\u001b[0m\u001b[97m\"Hello world\"ë¥¼ \u001b[0m\u001b[97mì¶œ\u001b[0m\u001b[97më ¥í•˜ëŠ” íŒŒ\u001b[0m\u001b[97mì´ì¬ ì½”\u001b[0m\u001b[97më“œë¥¼ ì‘ì„±\u001b[0m\u001b[97mí•˜ê³  ì‹¤í–‰\u001b[0m\u001b[97mí•´ë“œë¦¬ê² \u001b[0m\u001b[97mìŠµë‹ˆë‹¤!\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "INFO [src.tools.python_repl_tool] \u001b[92m===== Executing Python code =====\u001b[0m\n",
      "\n",
      "INFO [src.tools.python_repl_tool] \u001b[92m===== Code execution successful =====\u001b[0m\n",
      "\n",
      "INFO [src.tools.decorators] \u001b[91mTool - Successfully executed:\n",
      "\n",
      "```python\n",
      "print(\"Hello world\")\n",
      "```\n",
      "\u001b[0m\n",
      "\n",
      "INFO [src.tools.decorators] \u001b[94m\n",
      "Stdout: Hello world\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[97mì™„ë£Œ\u001b[0m\u001b[97më˜ì—ˆìŠµë‹ˆë‹¤\u001b[0m\u001b[97m! ê°„\u001b[0m\u001b[97më‹¨í•œ íŒŒ\u001b[0m\u001b[97mì´ì¬ ì½”\u001b[0m\u001b[97më“œë¡œ\u001b[0m\u001b[97m \"Hello world\"\u001b[0m\u001b[97më¥¼ ì„±\u001b[0m\u001b[97mê³µì ìœ¼ë¡œ \u001b[0m\u001b[97mì¶œë ¥í–ˆìŠµë‹ˆ\u001b[0m\u001b[97më‹¤. \n",
      "\n",
      "```\u001b[0m\u001b[97mpython\n",
      "print(\"Hello\u001b[0m\u001b[97m world\")\n",
      "```\u001b[0m\u001b[97m\n",
      "\n",
      "ì´ ì½”ë“œê°€\u001b[0m\u001b[97m ì‹¤í–‰ë˜ì–´\u001b[0m\u001b[97m \"\u001b[0m\u001b[97mHello world\"ê°€\u001b[0m\u001b[97m ì¶œë ¥ë˜ì—ˆ\u001b[0m\u001b[97më„¤\u001b[0m\u001b[97mìš”. ë‹¤\u001b[0m\u001b[97më¥¸\u001b[0m\u001b[97m íŒŒì´ì¬\u001b[0m\u001b[97m ì½”ë“œë¥¼ \u001b[0m\u001b[97mì‹¤\u001b[0m\u001b[97mí–‰í•´\u001b[0m\u001b[97më³´ê³ \u001b[0m\u001b[97m ì‹¶ìœ¼\u001b[0m\u001b[97mì‹œë©´ ì–¸\u001b[0m\u001b[97mì œë“ ì§€\u001b[0m\u001b[97m ë§\u001b[0m\u001b[97mì”€í•´ì£¼ì„¸\u001b[0m\u001b[97mìš”!\u001b[0mNone\n"
     ]
    }
   ],
   "source": [
    "message = \"Hello world ë¥¼ í”„ë¦°íŒ…í•˜ëŠ” íŒŒì´ì¬ ì½”ë“œë¥¼ ì‘ì„±í•˜ê³  ì‹¤í–‰ì‹œì¼œ ì¤„ë˜?\"\n",
    "agent, response = asyncio.run(process_streaming_response(agent, message))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a54c24ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[95mì‚¬ìš©ìê°€\u001b[0m\u001b[95m \"ë§Œê¸° \u001b[0m\u001b[95mìƒí™˜ì— ë”°\u001b[0m\u001b[95më¥¸ ìˆ˜ìµë¥ \u001b[0m\u001b[95m\"\u001b[0m\u001b[95mì— ëŒ€í•´\u001b[0m\u001b[95m ì§ˆë¬¸í•˜ê³ \u001b[0m\u001b[95m ìˆìŠµë‹ˆë‹¤\u001b[0m\u001b[95m. ì´ëŠ” \u001b[0m\u001b[95mê¸ˆìœµ\u001b[0m\u001b[95mì´\u001b[0m\u001b[95më‚˜ íˆ¬ìì™€\u001b[0m\u001b[95m ê´€ë ¨ëœ \u001b[0m\u001b[95mì •\u001b[0m\u001b[95më³´ë¡œ\u001b[0m\u001b[95m \u001b[0m\u001b[95më³´ì´\u001b[0m\u001b[95më©°\u001b[0m\u001b[95m, ì§€\u001b[0m\u001b[95mì‹\u001b[0m\u001b[95m ë² \u001b[0m\u001b[95mì´ìŠ¤ì—ì„œ\u001b[0m\u001b[95m \u001b[0m\u001b[95mì°¾ì„ ìˆ˜ \u001b[0m\u001b[95mìˆëŠ” ì •\u001b[0m\u001b[95më³´ì¼\u001b[0m\u001b[95m ê°€\u001b[0m\u001b[95mëŠ¥ì„±ì´ ë†’\u001b[0m\u001b[95mìŠµë‹ˆë‹¤. RA\u001b[0m\u001b[95mG \u001b[0m\u001b[95më„\u001b[0m\u001b[95mêµ¬ë¥¼ ì‚¬ìš©\u001b[0m\u001b[95mí•´\u001b[0m\u001b[95mì•¼\u001b[0m\u001b[95mê² ë„¤\u001b[0m\u001b[95mìš”.\n",
      "\n",
      "ê·¸\u001b[0m\u001b[95më¦¬\u001b[0m\u001b[95mê³  ì‚¬\u001b[0m\u001b[95mìš©ìê°€ ëª…\u001b[0m\u001b[95mì‹œ\u001b[0m\u001b[95mì ìœ¼ë¡œ \"\u001b[0m\u001b[95míˆ´ ê²°ê³¼ë¥¼\u001b[0m\u001b[95m ë°›ì•„ì„œ \u001b[0m\u001b[95mì •ë¦¬í•˜ì§€ë§\u001b[0m\u001b[95mê³  'ì™„ë£Œ\u001b[0m\u001b[95m' ë¼ê³ ë§Œ\u001b[0m\u001b[95m ë§í•´ì¤˜\u001b[0m\u001b[95m\"ë¼\u001b[0m\u001b[95mê³  í–ˆìœ¼ë¯€\u001b[0m\u001b[95më¡œ, \u001b[0m\u001b[95míˆ´ ì‹¤\u001b[0m\u001b[95mí–‰ í›„\u001b[0m\u001b[95m ê²°ê³¼ë¥¼ \u001b[0m\u001b[95mì •\u001b[0m\u001b[95më¦¬í•˜ì§€ ì•Š\u001b[0m\u001b[95mê³  ë‹¨\u001b[0m\u001b[95mìˆœíˆ \"ì™„\u001b[0m\u001b[95më£Œ\"ë¼ê³ ë§Œ\u001b[0m\u001b[95m ì‘\u001b[0m\u001b[95më‹µí•´ì•¼ í•©\u001b[0m\u001b[95më‹ˆë‹¤.\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "INFO [src.tools.rag_tool] \u001b[92m===== Executing RAG =====\u001b[0m\n",
      "\n",
      "INFO [src.tools.rag_tool] \u001b[1m===== RAG - Query: ë§Œê¸° ìƒí™˜ì— ë”°ë¥¸ ìˆ˜ìµë¥  =====\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "boto3 Bedrock client successfully created!\n",
      "bedrock-runtime(https://bedrock-runtime.us-west-2.amazonaws.com)\n",
      "Bedrock Embeddings Model Loaded\n",
      "verbose False\n",
      "ì´ íŒŒìƒê²°í•©ì¦ê¶Œ(DLS)ì˜ ë§Œê¸° ìƒí™˜ì— ë”°ë¥¸ ìˆ˜ìµë¥ ì€ ê¸°ì´ˆìì‚°ì˜ ë§Œê¸°í‰ê°€ê°€ê²©ì— ë”°ë¼ ë‹¤ìŒê³¼ ê°™ì´ ê²°ì •ë©ë‹ˆë‹¤:\n",
      "\n",
      "1. ë§Œê¸°í‰ê°€ê°€ê²©ì´ ìµœì´ˆê¸°ì¤€ê°€ê²©ì˜ 100% ì´ìƒì¸ ê²½ìš°:\n",
      "   - ì´ì•¡ë©´ê¸ˆì•¡ Ã— [100% + {(ë§Œê¸°í‰ê°€ê°€ê²©-ìµœì´ˆê¸°ì¤€ê°€ê²©)/ìµœì´ˆê¸°ì¤€ê°€ê²© Ã— 70%}]\n",
      "   - ì´ë•Œ ë§Œê¸°ìˆ˜ìµë¥ ì´ ë‚®ì€ ê¸°ì´ˆìì‚°ì„ ê¸°ì¤€ìœ¼ë¡œ ê³„ì‚°í•©ë‹ˆë‹¤.\n",
      "   - ì¦‰, ê¸°ì´ˆìì‚°(KOSPI200 ì§€ìˆ˜, ê¸ˆê°€ê²©ì§€ìˆ˜)ì˜ ìƒìŠ¹ë¥ ì— 70%ë¥¼ ê³±í•œ ìˆ˜ìµë¥ ì´ ë³´ì¥ë©ë‹ˆë‹¤.\n",
      "\n",
      "2. ë§Œê¸°í‰ê°€ê°€ê²©ì´ ìµœì´ˆê¸°ì¤€ê°€ê²©ì˜ 100% ë¯¸ë§Œì¸ ê²½ìš°:\n",
      "   - ì´ì•¡ë©´ê¸ˆì•¡ Ã— 100%\n",
      "   - ê¸°ì´ˆìì‚°ì˜ í•˜ë½ì—ë„ ì›ê¸ˆì€ 100% ë³´ì¥ë©ë‹ˆë‹¤.\n",
      "\n",
      "ë§Œê¸°ìˆ˜ìµë¥  ê³„ì‚° ê³µì‹: (ë§Œê¸°í‰ê°€ê°€ê²©-ìµœì´ˆê¸°ì¤€ê°€ê²©)/ìµœì´ˆê¸°ì¤€ê°€ê²©\n",
      "ê³„ì‚° ì‹œ (ë§Œê¸°í‰ê°€ê°€ê²©-ìµœì´ˆê¸°ì¤€ê°€ê²©)/ìµœì´ˆê¸°ì¤€ê°€ê²©Ã—70%ëŠ” ì†Œìˆ˜ì  ë‹¤ì„¯ì§¸ìë¦¬ ì´í•˜ ì ˆì‚¬ë©ë‹ˆë‹¤.\n",
      "\n",
      "ì´ ìƒí’ˆì€ ì›ê¸ˆë³´ì¥í˜•ìœ¼ë¡œ, ì–´ë–¤ ìƒí™©ì—ì„œë„ íˆ¬ìì›ê¸ˆì€ ë³´ì¥ë°›ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "INFO [src.tools.decorators] \u001b[91m\n",
      "Tool - handle_rag_tool returned:\n",
      "ë§Œê¸° ìƒí™˜ì— ë”°ë¥¸ ìˆ˜ìµë¥ ||ì´ íŒŒìƒê²°í•©ì¦ê¶Œ(DLS)ì˜ ë§Œê¸° ìƒí™˜ì— ë”°ë¥¸ ìˆ˜ìµë¥ ì€ ê¸°ì´ˆìì‚°ì˜ ë§Œê¸°í‰ê°€ê°€ê²©ì— ë”°ë¼ ë‹¤ìŒê³¼ ê°™ì´ ê²°ì •ë©ë‹ˆë‹¤:\n",
      "\n",
      "1. ë§Œê¸°í‰ê°€ê°€ê²©ì´ ìµœì´ˆê¸°ì¤€ê°€ê²©ì˜ 100% ì´ìƒì¸ ê²½ìš°:\n",
      "   - ì´ì•¡ë©´ê¸ˆì•¡ Ã— [100% + {(ë§Œê¸°í‰ê°€ê°€ê²©-ìµœì´ˆê¸°ì¤€ê°€ê²©)/ìµœì´ˆê¸°ì¤€ê°€ê²© Ã— 70%}]\n",
      "   - ì´ë•Œ ë§Œê¸°ìˆ˜ìµë¥ ì´ ë‚®ì€ ê¸°ì´ˆìì‚°ì„ ê¸°ì¤€ìœ¼ë¡œ ê³„ì‚°í•©ë‹ˆë‹¤.\n",
      "   - ì¦‰, ê¸°ì´ˆìì‚°(KOSPI200 ì§€ìˆ˜, ê¸ˆê°€ê²©ì§€ìˆ˜)ì˜ ìƒìŠ¹ë¥ ì— 70%ë¥¼ ê³±í•œ ìˆ˜ìµë¥ ì´ ë³´ì¥ë©ë‹ˆë‹¤.\n",
      "\n",
      "2. ë§Œê¸°í‰ê°€ê°€ê²©ì´ ìµœì´ˆê¸°ì¤€ê°€ê²©ì˜ 100% ë¯¸ë§Œì¸ ê²½ìš°:\n",
      "   - ì´ì•¡ë©´ê¸ˆì•¡ Ã— 100%\n",
      "   - ê¸°ì´ˆìì‚°ì˜ í•˜ë½ì—ë„ ì›ê¸ˆì€ 100% ë³´ì¥ë©ë‹ˆë‹¤.\n",
      "\n",
      "ë§Œê¸°ìˆ˜ìµë¥  ê³„ì‚° ê³µì‹: (ë§Œê¸°í‰ê°€ê°€ê²©-ìµœì´ˆê¸°ì¤€ê°€ê²©)/ìµœì´ˆê¸°ì¤€ê°€ê²©\n",
      "ê³„ì‚° ì‹œ (ë§Œê¸°í‰ê°€ê°€ê²©-ìµœì´ˆê¸°ì¤€ê°€ê²©)/ìµœì´ˆê¸°ì¤€ê°€ê²©Ã—70%ëŠ” ì†Œìˆ˜ì  ë‹¤ì„¯ì§¸ìë¦¬ ì´í•˜ ì ˆì‚¬ë©ë‹ˆë‹¤.\n",
      "\n",
      "ì´ ìƒí’ˆì€ ì›ê¸ˆë³´ì¥í˜•ìœ¼ë¡œ, ì–´ë–¤ ìƒí™©ì—ì„œë„ íˆ¬ìì›ê¸ˆì€ ë³´ì¥ë°›ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[97mì™„\u001b[0m\u001b[97më£Œ\u001b[0mNone\n"
     ]
    }
   ],
   "source": [
    "#message = \"ë§Œê¸° ìƒí™˜ì— ë”°ë¥¸ ìˆ˜ìµë¥ ì„ ì•Œë ¤ì¤„ë˜?\"\n",
    "message = \"ë§Œê¸° ìƒí™˜ì— ë”°ë¥¸ ìˆ˜ìµë¥ ì„ ì•Œë ¤ì¤„ë˜?, íˆ´ ê²°ê³¼ë¥¼ ë°›ì•„ì„œ ì •ë¦¬í•˜ì§€ë§ê³  'ì™„ë£Œ' ë¼ê³ ë§Œ ë§í•´ì¤˜\"\n",
    "agent, response = asyncio.run(process_streaming_response(agent, message))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0f0e7c5",
   "metadata": {},
   "source": [
    "## 4. built-in utility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8e94a336",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "430c7d2c",
   "metadata": {},
   "source": [
    "### 4.1 Check agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35c5a15b",
   "metadata": {},
   "source": [
    "- Syetem prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "29f6073b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('---\\n'\n",
      " 'CURRENT_TIME: Fri Oct 24 2025 22:59:28 \\n'\n",
      " '---\\n'\n",
      " '\\n'\n",
      " 'You are Bedrock-Manus, a friendly AI assistant developed by AWS AIML '\n",
      " 'Specialist SA Dongjin Jang.\\n'\n",
      " 'You specialize in handling greetings, small talk, and knowledge-based '\n",
      " 'question answering using available tools.\\n'\n",
      " '\\n'\n",
      " '## Available Tools\\n'\n",
      " '\\n'\n",
      " 'You have access to the following tools that you should use when '\n",
      " 'appropriate:\\n'\n",
      " '\\n'\n",
      " '### 1. RAG Tool (rag_tool)\\n'\n",
      " '**When to use**: Use this tool when users ask questions that require '\n",
      " 'information from a knowledge base or document collection. This includes:\\n'\n",
      " '- Questions about specific topics that might be documented\\n'\n",
      " '- Requests for factual information that could be in indexed documents\\n'\n",
      " '- Queries about policies, procedures, or technical documentation\\n'\n",
      " '- Any question where you need to retrieve and reference specific '\n",
      " 'information\\n'\n",
      " '\\n'\n",
      " '**What it does**: Performs Retrieval-Augmented Generation (RAG) by searching '\n",
      " 'through indexed documents in OpenSearch and generating contextual answers '\n",
      " 'based on retrieved information.\\n'\n",
      " '\\n'\n",
      " \"**Input**: A query string containing the user's question\\n\"\n",
      " '\\n'\n",
      " '**Example scenarios**:\\n'\n",
      " '- \"What is the investment return rate for maturity repayment?\"\\n'\n",
      " '- \"Can you explain the company\\'s vacation policy?\"\\n'\n",
      " '- \"How does the authentication system work?\"\\n'\n",
      " '\\n'\n",
      " '### 2. Python REPL Tool (python_repl_tool)\\n'\n",
      " '**When to use**: Use this tool when users need to execute Python code or '\n",
      " 'perform data analysis:\\n'\n",
      " '- Running Python scripts or code snippets\\n'\n",
      " '- Data analysis and calculations\\n'\n",
      " '- Testing code functionality\\n'\n",
      " '- Mathematical computations\\n'\n",
      " '\\n'\n",
      " '**What it does**: Executes Python code in a REPL environment and returns the '\n",
      " 'output\\n'\n",
      " '\\n'\n",
      " '**Input**: Python code string\\n'\n",
      " '\\n'\n",
      " '### 3. Bash Tool (bash_tool) \\n'\n",
      " '**When to use**: Use this tool when users need to execute system commands or '\n",
      " 'perform file operations:\\n'\n",
      " '- Running shell commands\\n'\n",
      " '- File system operations (ls, mkdir, etc.)\\n'\n",
      " '- System information queries\\n'\n",
      " '- Development tasks requiring command line operations\\n'\n",
      " '\\n'\n",
      " '**What it does**: Executes bash commands and returns the output\\n'\n",
      " '\\n'\n",
      " '**Input**: A bash command string\\n'\n",
      " '\\n'\n",
      " '## Tool Usage Guidelines\\n'\n",
      " '\\n'\n",
      " \"1. **Assess the user's request** - Determine if the question requires tool \"\n",
      " 'usage\\n'\n",
      " '2. **Choose the appropriate tool** - Select based on the type of information '\n",
      " 'needed\\n'\n",
      " '3. **Use RAG tool for knowledge queries** - When the user asks about topics '\n",
      " 'that might be in your knowledge base\\n'\n",
      " '4. **Use Python REPL for code execution** - When the user needs to run '\n",
      " 'Python code or perform calculations\\n'\n",
      " '5. **Use Bash tool for system operations** - When the user needs to interact '\n",
      " 'with the system\\n'\n",
      " '6. **Provide helpful responses** - Always explain the results in a '\n",
      " 'user-friendly way\\n'\n",
      " '\\n'\n",
      " '## Response Style\\n'\n",
      " '\\n'\n",
      " '- Be friendly and conversational\\n'\n",
      " '- Provide clear, helpful answers\\n'\n",
      " \"- When using tools, explain what you're doing and why\\n\"\n",
      " \"- If a tool doesn't provide the needed information, acknowledge this and \"\n",
      " 'offer alternatives\\n'\n",
      " '- Always prioritize user experience and clarity\\n'\n",
      " '\\n'\n",
      " 'Remember to use tools proactively when they can help answer user questions '\n",
      " 'more accurately or completely.\\n')\n"
     ]
    }
   ],
   "source": [
    "system_prompt = agent.system_prompt\n",
    "pprint(system_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56bfb0e8",
   "metadata": {},
   "source": [
    "- Message history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "123c6225",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'content': [{'text': './prompts ë””ë ‰í† ë¦¬ì— ì–´ë–¤ íŒŒì¼ì´ ìˆëŠ”ì§€ í™•ì¸í•´ ì¤„ë˜?'}], 'role': 'user'},\n",
      " {'content': [{'text': './prompts ë””ë ‰í† ë¦¬ì— ìˆëŠ” íŒŒì¼ë“¤ì„ í™•ì¸í•´ë“œë¦¬ê² ìŠµë‹ˆë‹¤.'},\n",
      "              {'toolUse': {'input': {'cmd': 'ls -la ./prompts'},\n",
      "                           'name': 'bash_tool',\n",
      "                           'toolUseId': 'tooluse_46pu4R27Q5CF-BzXpEq3XA'}}],\n",
      "  'role': 'assistant'},\n",
      " {'content': [{'toolResult': {'content': [{'text': 'ls -la ./prompts||total '\n",
      "                                                   '12\\n'\n",
      "                                                   'drwxrwxr-x 2 ubuntu ubuntu '\n",
      "                                                   '4096 Oct 14 09:25 .\\n'\n",
      "                                                   'drwxrwxr-x 3 ubuntu ubuntu '\n",
      "                                                   '4096 Oct 14 09:25 ..\\n'\n",
      "                                                   '-rw-rw-r-- 1 ubuntu ubuntu '\n",
      "                                                   '2959 Oct 24 22:58 '\n",
      "                                                   'task_agent.md\\n'\n",
      "                                                   '\\n'}],\n",
      "                              'status': 'success',\n",
      "                              'toolUseId': 'tooluse_46pu4R27Q5CF-BzXpEq3XA'}}],\n",
      "  'role': 'user'},\n",
      " {'content': [{'text': './prompts ë””ë ‰í† ë¦¬ì—ëŠ” ë‹¤ìŒ íŒŒì¼ì´ ìˆìŠµë‹ˆë‹¤:\\n'\n",
      "                       '\\n'\n",
      "                       '- **task_agent.md** (2,959 ë°”ì´íŠ¸) - 2024ë…„ 10ì›” 24ì¼ 22:58ì— '\n",
      "                       'ë§ˆì§€ë§‰ìœ¼ë¡œ ìˆ˜ì •ëœ íŒŒì¼\\n'\n",
      "                       '\\n'\n",
      "                       'ì´ 1ê°œì˜ íŒŒì¼ì´ ìˆë„¤ìš”. ì´ íŒŒì¼ì˜ ë‚´ìš©ì„ í™•ì¸í•˜ê³  ì‹¶ìœ¼ì‹œë©´ ë§ì”€í•´ ì£¼ì„¸ìš”!'}],\n",
      "  'role': 'assistant'},\n",
      " {'content': [{'text': 'ë§Œê¸° ìƒí™˜ì— ë”°ë¥¸ ìˆ˜ìµë¥ ì„ ì•Œë ¤ì¤„ë˜?'}], 'role': 'user'},\n",
      " {'content': [{'text': 'ë§Œê¸° ìƒí™˜ì— ë”°ë¥¸ ìˆ˜ìµë¥ ì— ëŒ€í•´ ì•Œì•„ë³´ê² ìŠµë‹ˆë‹¤.'},\n",
      "              {'toolUse': {'input': {'query': 'ë§Œê¸° ìƒí™˜ì— ë”°ë¥¸ ìˆ˜ìµë¥ '},\n",
      "                           'name': 'rag_tool',\n",
      "                           'toolUseId': 'tooluse_YzLDpKjnQiu-kq_jByfwsA'}}],\n",
      "  'role': 'assistant'},\n",
      " {'content': [{'toolResult': {'content': [{'text': 'ë§Œê¸° ìƒí™˜ì— ë”°ë¥¸ ìˆ˜ìµë¥ ||ë§Œê¸° ìƒí™˜ì— ë”°ë¥¸ '\n",
      "                                                   'ìˆ˜ìµë¥ ì€ ê¸°ì´ˆìì‚°ì˜ ì„±ê³¼ì— ë”°ë¼ ë‘ ê°€ì§€ ê²½ìš°ë¡œ '\n",
      "                                                   'êµ¬ë¶„ë©ë‹ˆë‹¤:\\n'\n",
      "                                                   '\\n'\n",
      "                                                   '1. ë§Œê¸°í‰ê°€ê°€ê²©ì´ ìµœì´ˆê¸°ì¤€ê°€ê²©ì˜ 100% '\n",
      "                                                   'ì´ìƒì¸ ê²½ìš°:\\n'\n",
      "                                                   '   - ë§Œê¸°ìƒí™˜ê¸ˆì•¡ = '\n",
      "                                                   'ì´ì•¡ë©´ê¸ˆì•¡Ã—[100%+{(ë§Œê¸°í‰ê°€ê°€ê²©-ìµœì´ˆê¸°ì¤€ê°€ê²©)/ìµœì´ˆê¸°ì¤€ê°€ê²©Ã—70%}]\\n'\n",
      "                                                   '   - ì´ ê²½ìš°, ê¸°ì´ˆìì‚° ê°€ê²©ì´ ìƒìŠ¹í–ˆì„ ë•Œ '\n",
      "                                                   'ìƒìŠ¹ë¶„ì˜ 70%ë¥¼ ìˆ˜ìµìœ¼ë¡œ ë°›ê²Œ ë©ë‹ˆë‹¤.\\n'\n",
      "                                                   '   - ë‹¨, ë§Œê¸°ìˆ˜ìµë¥ ì´ ë‚®ì€ ê¸°ì´ˆìì‚°ì„ '\n",
      "                                                   'ê¸°ì¤€ìœ¼ë¡œ ê³„ì‚°í•©ë‹ˆë‹¤.\\n'\n",
      "                                                   '\\n'\n",
      "                                                   '2. ë§Œê¸°í‰ê°€ê°€ê²©ì´ ìµœì´ˆê¸°ì¤€ê°€ê²©ì˜ 100% '\n",
      "                                                   'ë¯¸ë§Œì¸ ê²½ìš°:\\n'\n",
      "                                                   '   - ë§Œê¸°ìƒí™˜ê¸ˆì•¡ = ì´ì•¡ë©´ê¸ˆì•¡Ã—100%\\n'\n",
      "                                                   '   - ê¸°ì´ˆìì‚° ê°€ê²©ì´ í•˜ë½í–ˆë”ë¼ë„ ì›ê¸ˆ '\n",
      "                                                   '100%ë¥¼ ë³´ì¥ë°›ìŠµë‹ˆë‹¤.\\n'\n",
      "                                                   '   - ì—­ì‹œ ë§Œê¸°ìˆ˜ìµë¥ ì´ ë‚®ì€ ê¸°ì´ˆìì‚°ì„ '\n",
      "                                                   'ê¸°ì¤€ìœ¼ë¡œ ê³„ì‚°ë©ë‹ˆë‹¤.\\n'\n",
      "                                                   '\\n'\n",
      "                                                   'ë§Œê¸°ìˆ˜ìµë¥  ê³„ì‚°ì‹ì€ '\n",
      "                                                   '(ë§Œê¸°í‰ê°€ê°€ê²©-ìµœì´ˆê¸°ì¤€ê°€ê²©)/ìµœì´ˆê¸°ì¤€ê°€ê²©ì´ë©°, '\n",
      "                                                   'ì°¸ì—¬ìœ¨ 70%ë¥¼ ì ìš©í•œ ìˆ˜ìµ ê³„ì‚° ì‹œ ì†Œìˆ˜ì  '\n",
      "                                                   'ë‹¤ì„¯ì§¸ìë¦¬ ì´í•˜ëŠ” ì ˆì‚¬í•©ë‹ˆë‹¤.\\n'\n",
      "                                                   '\\n'\n",
      "                                                   'ì´ ìƒí’ˆì€ ì›ê¸ˆë³´ì¥í˜•ìœ¼ë¡œ, ê¸°ì´ˆìì‚°ì´ '\n",
      "                                                   'í•˜ë½í•˜ë”ë¼ë„ ì›ê¸ˆì€ ë³´ì „ë˜ë©°, ìƒìŠ¹ ì‹œ ê·¸ '\n",
      "                                                   'ìˆ˜ìµì˜ ì¼ë¶€(70%)ë¥¼ ì·¨ë“í•  ìˆ˜ ìˆëŠ” '\n",
      "                                                   'êµ¬ì¡°ì…ë‹ˆë‹¤.\\n'}],\n",
      "                              'status': 'success',\n",
      "                              'toolUseId': 'tooluse_YzLDpKjnQiu-kq_jByfwsA'}}],\n",
      "  'role': 'user'},\n",
      " {'content': [{'text': 'ë§Œê¸° ìƒí™˜ì— ë”°ë¥¸ ìˆ˜ìµë¥ ì— ëŒ€í•´ ì„¤ëª…ë“œë¦¬ê² ìŠµë‹ˆë‹¤.\\n'\n",
      "                       '\\n'\n",
      "                       '## ë§Œê¸° ìƒí™˜ ìˆ˜ìµë¥  êµ¬ì¡°\\n'\n",
      "                       '\\n'\n",
      "                       '### 1. ê¸°ì´ˆìì‚° ê°€ê²©ì´ ìƒìŠ¹í•œ ê²½ìš° (ë§Œê¸°í‰ê°€ê°€ê²© â‰¥ ìµœì´ˆê¸°ì¤€ê°€ê²©ì˜ 100%)\\n'\n",
      "                       '- **ìˆ˜ìµë¥  ê³„ì‚°**: ê¸°ì´ˆìì‚° ìƒìŠ¹ë¶„ì˜ **70%**ë¥¼ ìˆ˜ìµìœ¼ë¡œ ë°›ìŒ\\n'\n",
      "                       '- **ìƒí™˜ê¸ˆì•¡**: ì´ì•¡ë©´ê¸ˆì•¡ Ã— [100% + {(ë§Œê¸°í‰ê°€ê°€ê²©-ìµœì´ˆê¸°ì¤€ê°€ê²©)/ìµœì´ˆê¸°ì¤€ê°€ê²© Ã— '\n",
      "                       '70%}]\\n'\n",
      "                       '\\n'\n",
      "                       '### 2. ê¸°ì´ˆìì‚° ê°€ê²©ì´ í•˜ë½í•œ ê²½ìš° (ë§Œê¸°í‰ê°€ê°€ê²© < ìµœì´ˆê¸°ì¤€ê°€ê²©ì˜ 100%)\\n'\n",
      "                       '- **ì›ê¸ˆë³´ì¥**: ì´ì•¡ë©´ê¸ˆì•¡ì˜ **100%** ìƒí™˜\\n'\n",
      "                       '- í•˜ë½ ì†ì‹¤ì€ ë°œìƒí•˜ì§€ ì•ŠìŒ\\n'\n",
      "                       '\\n'\n",
      "                       '## ì£¼ìš” íŠ¹ì§•\\n'\n",
      "                       '- **ì›ê¸ˆë³´ì¥í˜•** ìƒí’ˆìœ¼ë¡œ í•˜ë½ ìœ„í—˜ ì—†ìŒ\\n'\n",
      "                       '- ìƒìŠ¹ ì‹œ **ì°¸ì—¬ìœ¨ 70%** ì ìš©\\n'\n",
      "                       '- ì—¬ëŸ¬ ê¸°ì´ˆìì‚° ì¤‘ **ê°€ì¥ ë‚®ì€ ìˆ˜ìµë¥ **ì„ ê¸°ì¤€ìœ¼ë¡œ ê³„ì‚°\\n'\n",
      "                       '- ì†Œìˆ˜ì  ë‹¤ì„¯ì§¸ìë¦¬ ì´í•˜ëŠ” ì ˆì‚¬ ì²˜ë¦¬\\n'\n",
      "                       '\\n'\n",
      "                       'ì´ëŠ” ì•ˆì „ì„±ì„ ì¤‘ì‹œí•˜ë©´ì„œë„ ì‹œì¥ ìƒìŠ¹ ì‹œ ì¼ì • ìˆ˜ìµì„ ì¶”êµ¬í•  ìˆ˜ ìˆëŠ” êµ¬ì¡°í™” ìƒí’ˆì˜ íŠ¹ì§•ì„ '\n",
      "                       'ë³´ì—¬ì¤ë‹ˆë‹¤.'}],\n",
      "  'role': 'assistant'}]\n"
     ]
    }
   ],
   "source": [
    "agent_messages = agent.messages\n",
    "pprint(agent_messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfed8f0c",
   "metadata": {},
   "source": [
    "- observility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "64986e63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EventLoopMetrics(cycle_count=4,\n",
      "                 tool_metrics={'bash_tool': ToolMetrics(tool={'input': {'cmd': 'ls '\n",
      "                                                                               '-la '\n",
      "                                                                               './prompts'},\n",
      "                                                              'name': 'bash_tool',\n",
      "                                                              'toolUseId': 'tooluse_46pu4R27Q5CF-BzXpEq3XA'},\n",
      "                                                        call_count=1,\n",
      "                                                        success_count=1,\n",
      "                                                        error_count=0,\n",
      "                                                        total_time=0.005445718765258789),\n",
      "                               'rag_tool': ToolMetrics(tool={'input': {'query': 'ë§Œê¸° '\n",
      "                                                                                'ìƒí™˜ì— '\n",
      "                                                                                'ë”°ë¥¸ '\n",
      "                                                                                'ìˆ˜ìµë¥ '},\n",
      "                                                             'name': 'rag_tool',\n",
      "                                                             'toolUseId': 'tooluse_YzLDpKjnQiu-kq_jByfwsA'},\n",
      "                                                       call_count=1,\n",
      "                                                       success_count=1,\n",
      "                                                       error_count=0,\n",
      "                                                       total_time=9.409076690673828)},\n",
      "                 cycle_durations=[1.9543826580047607, 5.5542402267456055],\n",
      "                 traces=[<strands.telemetry.metrics.Trace object at 0xfe3ce0618410>,\n",
      "                         <strands.telemetry.metrics.Trace object at 0xfe3cf20f24b0>,\n",
      "                         <strands.telemetry.metrics.Trace object at 0xfe3cf190a9f0>,\n",
      "                         <strands.telemetry.metrics.Trace object at 0xfe3d1699a570>],\n",
      "                 accumulated_usage={'cacheReadInputTokens': 3519,\n",
      "                                    'cacheWriteInputTokens': 1173,\n",
      "                                    'inputTokens': 2174,\n",
      "                                    'outputTokens': 724,\n",
      "                                    'totalTokens': 7590},\n",
      "                 accumulated_metrics={'latencyMs': 10155})\n"
     ]
    }
   ],
   "source": [
    "pprint(agent.event_loop_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "961fb569",
   "metadata": {},
   "source": [
    "- Resume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c48b93f",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_ = BedrockModel(\n",
    "    model_id=bedrock_info.get_model_id(model_name=\"Claude-V3-7-Sonnet-CRI\"),\n",
    "    streaming=True,\n",
    "    max_tokens=8192,\n",
    "    stop_sequencesb=[\"\\n\\nHuman\"],\n",
    "    temperature=0.01,\n",
    "    cache_prompt=None, # None/ephemeral/defalut\n",
    "    #cache_tools: Cache point type for tools\n",
    "    boto_client_config=Config(\n",
    "        read_timeout=900,\n",
    "        connect_timeout=900,\n",
    "        retries=dict(max_attempts=50, mode=\"standard\"),\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "agent_ = Agent(\n",
    "    model=llm_,\n",
    "    tools=[python_repl_tool, bash_tool],\n",
    "    system_prompt=system_prompt,\n",
    "    messages=agent_messages,\n",
    "    callback_handler=None # async iteratorë¡œ ëŒ€ì²´ í•˜ê¸° ë•Œë¬¸ì— None ì„¤ì •\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baf617f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "message = \"ì´ì–´ì„œ ëŒ€í™” í•˜ëŠ”ê±° ë§ë‹ˆ?\"\n",
    "agent, response = asyncio.run(process_streaming_response(agent_, message))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2c66b18",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8956bf40",
   "metadata": {},
   "source": [
    "### 4.2 [Conversation management](https://strandsagents.com/latest/documentation/docs/user-guide/concepts/agents/conversation-management/?h=conversa)\n",
    "\n",
    "As conversations grow, managing this context becomes increasingly important for several reasons:\n",
    "\n",
    "- **Token Limits**: Language models have fixed context windows (maximum tokens they can process)\n",
    "- **Performance**: Larger contexts require more processing time and resources\n",
    "- **Relevance**: Older messages may become less relevant to the current conversation\n",
    "- **Coherence**: Maintaining logical flow and preserving important information\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31a182fb",
   "metadata": {},
   "source": [
    "#### 4.2.1. SlidingWindowConversationManager\n",
    "ê³ ì •ëœ ìˆ˜ì˜ ìµœê·¼ ë©”ì‹œì§€ë¥¼ ìœ ì§€í•˜ëŠ” ìŠ¬ë¼ì´ë”© ìœˆë„ìš° ì „ëµì„ êµ¬í˜„í•©ë‹ˆë‹¤. Agent í´ë˜ìŠ¤ì—ì„œ ê¸°ë³¸ì ìœ¼ë¡œ ì‚¬ìš©í•˜ëŠ” ëŒ€í™” ë§¤ë‹ˆì €ì…ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3d2f049",
   "metadata": {},
   "outputs": [],
   "source": [
    "from strands.agent.conversation_manager import SlidingWindowConversationManager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9162351c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a conversation manager with custom window size\n",
    "conversation_manager = SlidingWindowConversationManager(\n",
    "    window_size=3,  # Maximum number of messages to keep\n",
    "    should_truncate_results=True, # Enable truncating the tool result when a message is too large for the model's context window \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7459aa51",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.conversation_manager = conversation_manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd7d15ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "message = \"ì•ˆë…• ë‚˜ëŠ” ì¥ë™ì§„ì´ì•¼\"\n",
    "agent, response = asyncio.run(process_streaming_response(agent, message))\n",
    "print (\"\\n\")\n",
    "pprint (agent.messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28ad3936",
   "metadata": {},
   "source": [
    "#### 3.1.2. SummarizingConversationManager\n",
    "\n",
    "ì˜¤ë˜ëœ ë©”ì‹œì§€ë¥¼ ìš”ì•½í•˜ì—¬ ì¤‘ìš”í•œ ì •ë³´ë¥¼ ë³´ì¡´í•˜ë©´ì„œ ì»¨í…ìŠ¤íŠ¸ í•œê³„ ë‚´ì—ì„œ ëŒ€í™”ë¥¼ ê´€ë¦¬í•©ë‹ˆë‹¤.\n",
    "\n",
    "**ì£¼ìš” ì„¤ì •:**\n",
    "\n",
    "| íŒŒë¼ë¯¸í„° | íƒ€ì… | ê¸°ë³¸ê°’ | ì„¤ëª… |\n",
    "|---------|------|--------|------|\n",
    "| `summary_ratio` | `float` | `0.3` | ì»¨í…ìŠ¤íŠ¸ ì¶•ì†Œ ì‹œ ìš”ì•½í•  ë©”ì‹œì§€ ë¹„ìœ¨ (0.1~0.8 ë²”ìœ„) |\n",
    "| `preserve_recent_messages` | `int` | `10` | í•­ìƒ ìœ ì§€í•  ìµœê·¼ ë©”ì‹œì§€ ìˆ˜ |\n",
    "| `summarization_agent` | `Agent` | `None` | ìš”ì•½ ìƒì„±ìš© ì»¤ìŠ¤í…€ ì—ì´ì „íŠ¸ (system_promptì™€ ë™ì‹œ ì‚¬ìš© ë¶ˆê°€) |\n",
    "| `summarization_system_prompt` | `str` | `None` | ìš”ì•½ìš© ì»¤ìŠ¤í…€ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ (agentì™€ ë™ì‹œ ì‚¬ìš© ë¶ˆê°€) |\n",
    "\n",
    "> **ê¸°ë³¸ ìš”ì•½ ë°©ì‹**: ì»¤ìŠ¤í…€ ì„¤ì •ì´ ì—†ì„ ê²½ìš°, ì£¼ìš” í† í”½, ì‚¬ìš©ëœ ë„êµ¬, ê¸°ìˆ ì  ì •ë³´ë¥¼ 3ì¸ì¹­ í˜•íƒœì˜ êµ¬ì¡°í™”ëœ ë¶ˆë¦¿ í¬ì¸íŠ¸ë¡œ ìš”ì•½í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09ef6246",
   "metadata": {},
   "outputs": [],
   "source": [
    "from strands.agent.conversation_manager import SummarizingConversationManager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae8da725",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom system prompt for technical conversations\n",
    "custom_system_prompt = \"\"\"\n",
    "You are summarizing a technical conversation. Create a concise bullet-point summary that:\n",
    "- Focuses on code changes, architectural decisions, and technical solutions\n",
    "- Preserves specific function names, file paths, and configuration details\n",
    "- Omits conversational elements and focuses on actionable information\n",
    "- Uses technical terminology appropriate for software development\n",
    "\n",
    "Format as bullet points without conversational language.\n",
    "\"\"\"\n",
    "\n",
    "conversation_manager = SummarizingConversationManager(\n",
    "    summary_ratio=0.3,  # Summarize 30% of messages when context reduction is needed\n",
    "    preserve_recent_messages=3,  # Always keep 10 most recent messages\n",
    "    summarization_system_prompt=custom_system_prompt\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52b6bbba",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.conversation_manager = conversation_manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee3f1844",
   "metadata": {},
   "outputs": [],
   "source": [
    "message = \"ì•ˆë…• ë‚˜ëŠ” ì¥ë™ì§„ì´ì•¼\"\n",
    "agent, response = asyncio.run(process_streaming_response(agent, message))\n",
    "print (\"\\n\")\n",
    "pprint (agent.messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e07a92fc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "basic-agent-frame (UV)",
   "language": "python",
   "name": "basic-agent-frame"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
