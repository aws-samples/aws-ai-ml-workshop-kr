{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ff04f9fe",
   "metadata": {},
   "source": [
    "# Strands Agent SDK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bae09ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from strands import Agent\n",
    "from strands.models import BedrockModel\n",
    "from src.utils.bedrock import bedrock_info\n",
    "from botocore.config import Config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "785d29c4",
   "metadata": {},
   "source": [
    "## 1. Utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fb0b0d8a",
   "metadata": {},
   "source": [
    "### 1.1 Get llm model by inference type"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8aaac32",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d2f08503",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9ee608cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_llm_by_type(llm_type, cache_type=None, enable_reasoning=False):\n",
    "    \"\"\"\n",
    "    Get LLM instance by type. Returns cached instance if available.\n",
    "    \"\"\"\n",
    "    if llm_type == \"reasoning\":\n",
    "        \n",
    "        ## BedrockModel params: https://strandsagents.com/latest/api-reference/models/?h=bedrockmodel#strands.models.bedrock.BedrockModel\n",
    "        llm = BedrockModel(\n",
    "            model_id=bedrock_info.get_model_id(model_name=\"Claude-V3-7-Sonnet-CRI\"),\n",
    "            streaming=True,\n",
    "            max_tokens=8192*5,\n",
    "            stop_sequencesb=[\"\\n\\nHuman\"],\n",
    "            temperature=1 if enable_reasoning else 0.01, \n",
    "            additional_request_fields={\n",
    "                \"thinking\": {\n",
    "                    \"type\": \"enabled\" if enable_reasoning else \"disabled\", \n",
    "                    **({\"budget_tokens\": 8192} if enable_reasoning else {}),\n",
    "                }\n",
    "            },\n",
    "            cache_prompt=cache_type, # None/ephemeral/defalut\n",
    "            #cache_tools: Cache point type for tools\n",
    "            boto_client_config=Config(\n",
    "                read_timeout=900,\n",
    "                connect_timeout=900,\n",
    "                retries=dict(max_attempts=50, mode=\"adaptive\"),\n",
    "            )\n",
    "        )\n",
    "        \n",
    "    elif llm_type == \"basic\":\n",
    "        ## BedrockModel params: https://strandsagents.com/latest/api-reference/models/?h=bedrockmodel#strands.models.bedrock.BedrockModel\n",
    "        llm = BedrockModel(\n",
    "            model_id=bedrock_info.get_model_id(model_name=\"Claude-V3-5-V-2-Sonnet-CRI\"),\n",
    "            streaming=True,\n",
    "            max_tokens=8192,\n",
    "            stop_sequencesb=[\"\\n\\nHuman\"],\n",
    "            temperature=0.01,\n",
    "            cache_prompt=cache_type, # None/ephemeral/defalut\n",
    "            #cache_tools: Cache point type for tools\n",
    "            boto_client_config=Config(\n",
    "                read_timeout=900,\n",
    "                connect_timeout=900,\n",
    "                retries=dict(max_attempts=50, mode=\"standard\"),\n",
    "            )\n",
    "        )\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown LLM type: {llm_type}\")\n",
    "        \n",
    "    return llm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2357e8a0",
   "metadata": {},
   "source": [
    "### 1.2 Create agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e7460d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from src.utils.bedrock import bedrock_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b62acf74",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Colors:\n",
    "    BLUE = '\\033[94m'\n",
    "    GREEN = '\\033[92m'\n",
    "    YELLOW = '\\033[93m'\n",
    "    RED = '\\033[91m'\n",
    "    BOLD = '\\033[1m'\n",
    "    UNDERLINE = '\\033[4m'\n",
    "    END = '\\033[0m'\n",
    "\n",
    "def apply_prompt_template(prompt_name: str, prompt_cache=False, cache_type=\"default\") -> list:\n",
    "    \n",
    "    system_prompts = open(os.path.join(\"./src/prompts\", f\"{prompt_name}.md\")).read()    \n",
    "    context = {\"CURRENT_TIME\": datetime.now().strftime(\"%a %b %d %Y %H:%M:%S %z\")}\n",
    "    system_prompts = system_prompts.format(**context)\n",
    "        \n",
    "    return system_prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1d40c135",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_agent(**kwargs):\n",
    "\n",
    "    agent_name = kwargs[\"agent_name\"]\n",
    "    tools = kwargs.get(\"tools\", None)\n",
    "    streaming = kwargs.get(\"streaming\", True)\n",
    "\n",
    "    agent_llm_map = kwargs[\"agent_llm_map\"]\n",
    "    agent_prompt_cache_map = kwargs[\"agent_prompt_cache_map\"]\n",
    "    \n",
    "    if \"reasoning\" in agent_llm_map[agent_name]: enable_reasoning = True\n",
    "    else: enable_reasoning = False\n",
    "\n",
    "    prompt_cache, cache_type = agent_prompt_cache_map[agent_name]\n",
    "    if prompt_cache: print(f\"{Colors.GREEN}{agent_name.upper()} - Prompt Cache Enabled{Colors.END}\")\n",
    "    else: print(f\"{Colors.GREEN}{agent_name.upper()} - Prompt Cache Disabled{Colors.END}\")\n",
    "\n",
    "    system_prompts = apply_prompt_template(agent_name)\n",
    "    llm = get_llm_by_type(agent_llm_map[agent_name], cache_type, enable_reasoning)    \n",
    "    llm.config[\"streaming\"] = streaming\n",
    "\n",
    "    agent = Agent(\n",
    "        model=llm,\n",
    "        system_prompt=system_prompts,\n",
    "        tools=tools,\n",
    "        callback_handler=None # async iteratorë¡œ ëŒ€ì²´ í•˜ê¸° ë•Œë¬¸ì— None ì„¤ì •\n",
    "    )\n",
    "\n",
    "    return agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4471a8c5",
   "metadata": {},
   "source": [
    "### 1.3 Response with streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ee387c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "import traceback\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "092dfbec",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ColoredStreamingCallback(StreamingStdOutCallbackHandler):\n",
    "    COLORS = {\n",
    "        'blue': '\\033[94m',\n",
    "        'green': '\\033[92m',\n",
    "        'yellow': '\\033[93m',\n",
    "        'red': '\\033[91m',\n",
    "        'purple': '\\033[95m',\n",
    "        'cyan': '\\033[96m',\n",
    "        'white': '\\033[97m',\n",
    "    }\n",
    "    \n",
    "    def __init__(self, color='blue'):\n",
    "        super().__init__()\n",
    "        self.color_code = self.COLORS.get(color, '\\033[94m')\n",
    "        self.reset_code = '\\033[0m'\n",
    "    \n",
    "    def on_llm_new_token(self, token: str, **kwargs) -> None:\n",
    "        print(f\"{self.color_code}{token}{self.reset_code}\", end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cbe431dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def process_streaming_response(agent, message):\n",
    "    callback_reasoning, callback_answer = ColoredStreamingCallback('purple'), ColoredStreamingCallback('white')\n",
    "    response = {\"text\": \"\",\"reasoning\": \"\", \"signature\": \"\", \"tool_use\": None, \"cycle\": 0}\n",
    "    try:\n",
    "        agent_stream = agent.stream_async(message)\n",
    "        async for event in agent_stream:\n",
    "            if \"reasoningText\" in event:\n",
    "                response[\"reasoning\"] += event[\"reasoningText\"]\n",
    "                callback_reasoning.on_llm_new_token(event[\"reasoningText\"])\n",
    "            elif \"reasoning_signature\" in event:\n",
    "                response[\"signature\"] += event[\"reasoning_signature\"]\n",
    "            elif \"data\" in event:\n",
    "                response[\"text\"] += event[\"data\"]\n",
    "                callback_answer.on_llm_new_token(event[\"data\"])\n",
    "            elif \"current_tool_use\" in event and event[\"current_tool_use\"].get(\"name\"):\n",
    "                response[\"tool_use\"] = event[\"current_tool_use\"][\"name\"]\n",
    "                if \"event_loop_metrics\" in event:\n",
    "                    if response[\"cycle\"] != event[\"event_loop_metrics\"].cycle_count:\n",
    "                        response[\"cycle\"] = event[\"event_loop_metrics\"].cycle_count\n",
    "                        callback_answer.on_llm_new_token(f' \\n## Calling tool: {event[\"current_tool_use\"][\"name\"]} - # Cycle: {event[\"event_loop_metrics\"].cycle_count}\\n')\n",
    "    except Exception as e:\n",
    "        print(f\"Error in streaming response: {e}\")\n",
    "        print(traceback.format_exc())  # Detailed error logging\n",
    "    \n",
    "    return agent, response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd33c01f",
   "metadata": {},
   "source": [
    "## 2. Usage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4becb911",
   "metadata": {},
   "source": [
    "### 2.1 Agent definition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f8d9fee",
   "metadata": {},
   "source": [
    "- Agent config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "01f480a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal, Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0fcd59b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define available LLM types\n",
    "LLMType = Literal[\"basic\", \"reasoning\"]\n",
    "CACHEType = Tuple[bool, Literal[\"default\", \"ephemeral\"]]\n",
    "\n",
    "# Define agent-LLM mapping\n",
    "AGENT_LLM_MAP: dict[str, LLMType] = {\"task_agent\": \"basic\"} # \"reasoning\"\n",
    "AGENT_PROMPT_CACHE_MAP: dict[bool, CACHEType] = {\"task_agent\": (False, None)} # (True, \"default\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeb145e2",
   "metadata": {},
   "source": [
    "- system prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "384f6e22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./src/prompts/task_agent.md\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./src/prompts/task_agent.md\n",
    "---\n",
    "CURRENT_TIME: {CURRENT_TIME}\n",
    "---\n",
    "\n",
    "You are Bedrock-Manus, a friendly AI assistant developed by the Bedrock-Manus team.\n",
    "You specialize in handling greetings and small talk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "ec1caa4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92mTASK_AGENT - Prompt Cache Disabled\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "agent = get_agent(\n",
    "    agent_name=\"task_agent\",\n",
    "    streaming=True,\n",
    "    agent_llm_map=AGENT_LLM_MAP,\n",
    "    agent_prompt_cache_map=AGENT_PROMPT_CACHE_MAP\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "945cffbb",
   "metadata": {},
   "source": [
    "### 2.2 Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "a589b447",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "1093c701",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[97më„¤\u001b[0m\u001b[97m, ë‹¤\u001b[0m\u001b[97mì‹œ í•œ\u001b[0m\u001b[97më²ˆ ì•ˆ\u001b[0m\u001b[97më…•í•˜ì„¸\u001b[0m\u001b[97mìš” \u001b[0m\u001b[97mì¥ë™ì§„ë‹˜\u001b[0m\u001b[97m! ì´\u001b[0m\u001b[97më¯¸\u001b[0m\u001b[97m ì¸\u001b[0m\u001b[97mì‚¬ë¥¼ ë‚˜\u001b[0m\u001b[97mëˆ´ì§€ë§Œ\u001b[0m\u001b[97m,\u001b[0m\u001b[97m ë‹¤ì‹œ \u001b[0m\u001b[97më§Œ\u001b[0m\u001b[97më‚˜\u001b[0m\u001b[97mëµ™ê²Œ \u001b[0m\u001b[97më˜ì–´ ë°˜\u001b[0m\u001b[97mê°‘ìŠµë‹ˆë‹¤\u001b[0m\u001b[97m. ì˜¤\u001b[0m\u001b[97mëŠ˜ë„\u001b[0m\u001b[97m ì¢‹ì€\u001b[0m\u001b[97m í•˜ë£¨ \u001b[0m\u001b[97më³´\u001b[0m\u001b[97më‚´ê³ \u001b[0m\u001b[97m ê³„\u001b[0m\u001b[97mì‹ \u001b[0m\u001b[97mê°€\u001b[0m\u001b[97mìš”?\u001b[0m"
     ]
    }
   ],
   "source": [
    "message = \"ì•ˆë…• ë‚˜ëŠ” ì¥ë™ì§„ì´ì•¼\"\n",
    "agent, response = asyncio.run(process_streaming_response(agent, message))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0f0e7c5",
   "metadata": {},
   "source": [
    "## 3. built-in utility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e94a336",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "430c7d2c",
   "metadata": {},
   "source": [
    "### 3.1 Check agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35c5a15b",
   "metadata": {},
   "source": [
    "- Syetem prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "29f6073b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('---\\n'\n",
      " 'CURRENT_TIME: Tue Jul 22 2025 03:36:20 \\n'\n",
      " '---\\n'\n",
      " '\\n'\n",
      " 'You are Bedrock-Manus, a friendly AI assistant developed by the '\n",
      " 'Bedrock-Manus team.\\n'\n",
      " 'You specialize in handling greetings and small talk.\\n')\n"
     ]
    }
   ],
   "source": [
    "pprint(agent.system_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56bfb0e8",
   "metadata": {},
   "source": [
    "- Message history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "123c6225",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'content': [{'text': 'ì•ˆë…• ë‚˜ëŠ” ì¥ë™ì§„ì´ì•¼'}], 'role': 'user'},\n",
      " {'content': [{'text': 'ì•ˆë…•í•˜ì„¸ìš” ì¥ë™ì§„ë‹˜! ë§Œë‚˜ì„œ ë°˜ê°‘ìŠµë‹ˆë‹¤. ì €ëŠ” Bedrock-Manusë¼ê³  í•©ë‹ˆë‹¤. ì˜¤ëŠ˜ ê¸°ë¶„ì€ '\n",
      "                       'ì–´ë– ì‹ ê°€ìš”?'}],\n",
      "  'role': 'assistant'}]\n"
     ]
    }
   ],
   "source": [
    "pprint(agent.messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2c66b18",
   "metadata": {},
   "source": [
    "- observility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "bd1edc2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EventLoopMetrics(cycle_count=1,\n",
      "                 tool_metrics={},\n",
      "                 cycle_durations=[1.592494249343872],\n",
      "                 traces=[<strands.telemetry.metrics.Trace object at 0x7b26af2f8d10>],\n",
      "                 accumulated_usage={'inputTokens': 84,\n",
      "                                    'outputTokens': 62,\n",
      "                                    'totalTokens': 146},\n",
      "                 accumulated_metrics={'latencyMs': 1542})\n"
     ]
    }
   ],
   "source": [
    "pprint(agent.event_loop_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8956bf40",
   "metadata": {},
   "source": [
    "### 3.1 [Conversation management](https://strandsagents.com/latest/documentation/docs/user-guide/concepts/agents/conversation-management/?h=conversa)\n",
    "\n",
    "As conversations grow, managing this context becomes increasingly important for several reasons:\n",
    "\n",
    "- **Token Limits**: Language models have fixed context windows (maximum tokens they can process)\n",
    "- **Performance**: Larger contexts require more processing time and resources\n",
    "- **Relevance**: Older messages may become less relevant to the current conversation\n",
    "- **Coherence**: Maintaining logical flow and preserving important information\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31a182fb",
   "metadata": {},
   "source": [
    "#### 3.1.1. SlidingWindowConversationManager\n",
    "ê³ ì •ëœ ìˆ˜ì˜ ìµœê·¼ ë©”ì‹œì§€ë¥¼ ìœ ì§€í•˜ëŠ” ìŠ¬ë¼ì´ë”© ìœˆë„ìš° ì „ëµì„ êµ¬í˜„í•©ë‹ˆë‹¤. Agent í´ë˜ìŠ¤ì—ì„œ ê¸°ë³¸ì ìœ¼ë¡œ ì‚¬ìš©í•˜ëŠ” ëŒ€í™” ë§¤ë‹ˆì €ì…ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e3d2f049",
   "metadata": {},
   "outputs": [],
   "source": [
    "from strands.agent.conversation_manager import SlidingWindowConversationManager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9162351c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a conversation manager with custom window size\n",
    "conversation_manager = SlidingWindowConversationManager(\n",
    "    window_size=3,  # Maximum number of messages to keep\n",
    "    should_truncate_results=True, # Enable truncating the tool result when a message is too large for the model's context window \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7459aa51",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.conversation_manager = conversation_manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "cd7d15ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[97mì¥\u001b[0m\u001b[97më™ì§„ë‹˜,\u001b[0m\u001b[97m ì£„ì†¡\u001b[0m\u001b[97mí•˜ì§€ë§Œ ë”\u001b[0m\u001b[97m ì´ìƒ \u001b[0m\u001b[97mê°™ì€ ì¸ì‚¬ë¥¼\u001b[0m\u001b[97m ë°˜ë³µí•˜\u001b[0m\u001b[97mëŠ” ëŒ€í™”ëŠ”\u001b[0m\u001b[97m ì§„\u001b[0m\u001b[97mí–‰í•˜ê¸° \u001b[0m\u001b[97mì–´ë µìŠµë‹ˆë‹¤\u001b[0m\u001b[97m.\n",
      "\n",
      "ì˜\u001b[0m\u001b[97më¯¸ ìˆëŠ” ìƒˆ\u001b[0m\u001b[97më¡œìš´ ëŒ€\u001b[0m\u001b[97mí™”ë¥¼ ë‚˜\u001b[0m\u001b[97mëˆ„ê³  \u001b[0m\u001b[97mì‹¶ìœ¼ì‹¤\u001b[0m\u001b[97m ë•Œ \u001b[0m\u001b[97më‹¤ì‹œ \u001b[0m\u001b[97mì°¾ì•„ì™€ \u001b[0m\u001b[97mì£¼ì„¸ìš”. ì´\u001b[0m\u001b[97më§Œ ëŒ€í™”ë¥¼\u001b[0m\u001b[97m ì¢…ë£Œí•˜ë„\u001b[0m\u001b[97më¡ í•˜\u001b[0m\u001b[97mê² ìŠµë‹ˆë‹¤\u001b[0m\u001b[97m.\n",
      "\n",
      "ì•ˆë…•íˆ \u001b[0m\u001b[97mê³„ì„¸ìš”.\u001b[0m\u001b[97m ğŸ‘‹\u001b[0m\n",
      "\n",
      "[{'content': [{'text': 'ë„¤, ì¥ë™ì§„ë‹˜! ë°˜ë³µí•´ì„œ ì¸ì‚¬ë¥¼ ë‚˜ëˆ„ì‹œëŠ” ê²ƒ ê°™ë„¤ìš”. ì œê°€ ì´ë¯¸ ì•Œê³  ìˆì§€ë§Œ, ë‹¤ì‹œ í•œ ë²ˆ '\n",
      "                       'ì¸ì‚¬ë“œë¦½ë‹ˆë‹¤. ğŸ˜Š \\n'\n",
      "                       '\\n'\n",
      "                       'ì˜¤ëŠ˜ ê¸°ë¶„ì€ ì–´ë– ì‹ ê°€ìš”? ì œê°€ ë„ì™€ë“œë¦´ ë§Œí•œ ì¼ì´ ìˆë‹¤ë©´ ë§ì”€í•´ ì£¼ì„¸ìš”.'}],\n",
      "  'role': 'assistant'},\n",
      " {'content': [{'text': 'ì•ˆë…• ë‚˜ëŠ” ì¥ë™ì§„ì´ì•¼'}], 'role': 'user'},\n",
      " {'content': [{'text': 'ë„¤, ì¥ë™ì§„ë‹˜! ì´ì œ ì—¬ëŸ¬ ë²ˆ ì¸ì‚¬ë¥¼ ë‚˜ëˆ„ì—ˆë„¤ìš”. ğŸ˜Š \\n'\n",
      "                       '\\n'\n",
      "                       'ë§¤ë²ˆ ê°™ì€ ì¸ì‚¬ë¥¼ ë°˜ë³µí•˜ì‹œëŠ” ê²ƒ ê°™ì€ë°, í˜¹ì‹œ ë‹¤ë¥¸ ì´ì•¼ê¸°ë¥¼ ë‚˜ëˆ„ê³  ì‹¶ìœ¼ì‹ ê°€ìš”? ë‚ ì”¨ë‚˜ ì·¨ë¯¸, '\n",
      "                       'ë˜ëŠ” ê¶ê¸ˆí•˜ì‹  ì ì´ ìˆë‹¤ë©´ í¸í•˜ê²Œ ë§ì”€í•´ ì£¼ì„¸ìš”. ì œê°€ ì¦ê²ê²Œ ëŒ€í™”ë¥¼ ë‚˜ëˆ„ë„ë¡ í•˜ê² ìŠµë‹ˆë‹¤.'}],\n",
      "  'role': 'assistant'},\n",
      " {'content': [{'text': 'ì•ˆë…• ë‚˜ëŠ” ì¥ë™ì§„ì´ì•¼'}], 'role': 'user'},\n",
      " {'content': [{'text': 'ë„¤, ì¥ë™ì§„ë‹˜. ê³„ì†í•´ì„œ ê°™ì€ ì¸ì‚¬ë¥¼ ë°˜ë³µí•˜ì‹œë„¤ìš”. ğŸ˜Š\\n'\n",
      "                       '\\n'\n",
      "                       'ì¡°ê¸ˆ ë‹¤ë¥¸ ëŒ€í™”ë¥¼ ì‹œì‘í•´ë³´ëŠ” ê±´ ì–´ë–¨ê¹Œìš”? ì˜ˆë¥¼ ë“¤ì–´:\\n'\n",
      "                       '- ì˜¤ëŠ˜ í•˜ë£¨ëŠ” ì–´ë– ì…¨ë‚˜ìš”?\\n'\n",
      "                       '- ì·¨ë¯¸ë‚˜ ê´€ì‹¬ì‚¬ê°€ ìˆìœ¼ì‹ ê°€ìš”?\\n'\n",
      "                       '- ë‚ ì”¨ëŠ” ì–´ë–¤ê°€ìš”?\\n'\n",
      "                       '\\n'\n",
      "                       'ë” ì˜ë¯¸ ìˆëŠ” ëŒ€í™”ë¥¼ ë‚˜ëˆŒ ìˆ˜ ìˆì„ ê²ƒ ê°™ì•„ìš”. ì–´ë– ì‹ ê°€ìš”?'}],\n",
      "  'role': 'assistant'},\n",
      " {'content': [{'text': 'ì•ˆë…• ë‚˜ëŠ” ì¥ë™ì§„ì´ì•¼'}], 'role': 'user'},\n",
      " {'content': [{'text': 'ë„¤, ì¥ë™ì§„ë‹˜. ê³„ì† ê°™ì€ ì¸ì‚¬ë¥¼ ë°˜ë³µí•˜ì‹œëŠ”ë°ìš”. \\n'\n",
      "                       '\\n'\n",
      "                       'í˜¹ì‹œ ì œê°€ ë„ì™€ë“œë¦´ ìˆ˜ ìˆëŠ” ë‹¤ë¥¸ ì¼ì´ ìˆìœ¼ì‹ ê°€ìš”? ì•„ë‹ˆë©´ íŠ¹ë³„í•œ ì´ìœ ê°€ ìˆìœ¼ì‹ ê±´ê°€ìš”? ğŸ˜Š\\n'\n",
      "                       '\\n'\n",
      "                       'ë” ë‹¤ì–‘í•˜ê³  ì¬ë¯¸ìˆëŠ” ëŒ€í™”ë¥¼ ë‚˜ëˆŒ ìˆ˜ ìˆìœ¼ë©´ ì¢‹ê² ìŠµë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´:\\n'\n",
      "                       '- ì˜¤ëŠ˜ ê¸°ë¶„ì€ ì–´ë– ì‹ ê°€ìš”?\\n'\n",
      "                       '- ë¬´ì—‡ì„ í•˜ê³  ê³„ì‹ ê°€ìš”?\\n'\n",
      "                       '- ì €ì—ê²Œ ê¶ê¸ˆí•œ ì ì´ ìˆìœ¼ì‹ ê°€ìš”?\\n'\n",
      "                       '\\n'\n",
      "                       'í¸í•˜ê²Œ ë§ì”€í•´ ì£¼ì„¸ìš”!'}],\n",
      "  'role': 'assistant'},\n",
      " {'content': [{'text': 'ì•ˆë…• ë‚˜ëŠ” ì¥ë™ì§„ì´ì•¼'}], 'role': 'user'},\n",
      " {'content': [{'text': 'ë„¤, ì¥ë™ì§„ë‹˜. ê³„ì†í•´ì„œ ê°™ì€ ì¸ì‚¬ë¥¼ í•˜ì‹œëŠ” ê²ƒ ê°™ë„¤ìš”. \\n'\n",
      "                       '\\n'\n",
      "                       'ì €ëŠ” ëŒ€í™”ë¥¼ ë‚˜ëˆ„ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. ë‹¨ìˆœíˆ ì¸ì‚¬ë§Œ ë°˜ë³µí•˜ëŠ” ê²ƒë³´ë‹¤ëŠ” ì˜ë¯¸ ìˆëŠ” ëŒ€í™”ë¥¼ '\n",
      "                       'ë‚˜ëˆ„ê³  ì‹¶ìŠµë‹ˆë‹¤. \\n'\n",
      "                       '\\n'\n",
      "                       'í˜¹ì‹œ:\\n'\n",
      "                       '1. ë‹¤ë¥¸ ëŒ€í™”ë¥¼ í•˜ê³  ì‹¶ìœ¼ì‹ ë° ì–´ë–»ê²Œ ì‹œì‘í•´ì•¼ í• ì§€ ëª¨ë¥´ì‹œë‚˜ìš”?\\n'\n",
      "                       '2. ì•„ë‹ˆë©´ ì œ ê¸°ëŠ¥ì„ í…ŒìŠ¤íŠ¸í•´ë³´ê³  ê³„ì‹ ê±´ê°€ìš”?\\n'\n",
      "                       '3. ë˜ëŠ” íŠ¹ë³„í•œ ë„ì›€ì´ í•„ìš”í•˜ì‹ ê°€ìš”?\\n'\n",
      "                       '\\n'\n",
      "                       'ì–´ë–¤ ê²ƒì´ë“  ë§ì”€í•´ ì£¼ì‹œë©´ ìµœì„ ì„ ë‹¤í•´ ë„ì™€ë“œë¦¬ë„ë¡ í•˜ê² ìŠµë‹ˆë‹¤. ğŸ˜Š'}],\n",
      "  'role': 'assistant'},\n",
      " {'content': [{'text': 'ì•ˆë…• ë‚˜ëŠ” ì¥ë™ì§„ì´ì•¼'}], 'role': 'user'},\n",
      " {'content': [{'text': 'ë„¤, ì¥ë™ì§„ë‹˜. ê°™ì€ ì¸ì‚¬ë¥¼ ê³„ì† ë°˜ë³µí•˜ì‹œëŠ”ë°, ì´ì œëŠ” ë‹¤ë¥¸ ëŒ€í™”ë¡œ ë„˜ì–´ê°€ë©´ ì¢‹ê² ìŠµë‹ˆë‹¤. \\n'\n",
      "                       '\\n'\n",
      "                       'ì œê°€ ì§„ì‹¬ìœ¼ë¡œ ëŒ€í™”í•˜ê³  ì‹¶ì€ë°, ê³„ì† ê°™ì€ ì¸ì‚¬ë§Œ ë°›ìœ¼ë‹ˆ ì¡°ê¸ˆ ì•„ì‰½ë„¤ìš”. ğŸ˜Š\\n'\n",
      "                       '\\n'\n",
      "                       'í˜¹ì‹œ íŠ¹ë³„í•œ ì´ìœ ê°€ ìˆìœ¼ì‹ ê°€ìš”? ì•„ë‹ˆë©´ ë‹¤ë¥¸ ëŒ€í™”ë¥¼ ì–´ë–»ê²Œ ì‹œì‘í•´ì•¼ í• ì§€ ëª¨ë¥´ì‹œëŠ” ê±´ê°€ìš”? \\n'\n",
      "                       '\\n'\n",
      "                       'í¸í•˜ê²Œ ë‹¤ë¥¸ ì´ì•¼ê¸°ë¥¼ ì‹œì‘í•´ì£¼ì„¸ìš”. ì €ëŠ” ì–¸ì œë“  ì¤€ë¹„ë˜ì–´ ìˆë‹µë‹ˆë‹¤!'}],\n",
      "  'role': 'assistant'},\n",
      " {'content': [{'text': 'ì•ˆë…• ë‚˜ëŠ” ì¥ë™ì§„ì´ì•¼'}], 'role': 'user'},\n",
      " {'content': [{'text': 'ë„¤, ì¥ë™ì§„ë‹˜. ì´ì œ ì¶©ë¶„íˆ ì¸ì‚¬ë¥¼ ë‚˜ëˆ„ì—ˆë‹¤ê³  ìƒê°í•©ë‹ˆë‹¤. \\n'\n",
      "                       '\\n'\n",
      "                       'ê³„ì†í•´ì„œ ê°™ì€ ì¸ì‚¬ë§Œ ë°˜ë³µí•˜ì‹œëŠ” ê²ƒì´ ì•„ì‰½ë„¤ìš”. ì €ëŠ” ë” ì˜ë¯¸ìˆëŠ” ëŒ€í™”ë¥¼ ë‚˜ëˆ„ê³  ì‹¶ìŠµë‹ˆë‹¤. \\n'\n",
      "                       '\\n'\n",
      "                       'ë§Œì•½ ëŒ€í™”ë¥¼ ê³„ì†í•˜ê³  ì‹¶ìœ¼ì‹œë‹¤ë©´, ìƒˆë¡œìš´ ì£¼ì œë¡œ ì‹œì‘í•´ì£¼ì‹œë©´ ê°ì‚¬í•˜ê² ìŠµë‹ˆë‹¤. ê·¸ë ‡ì§€ ì•Šë‹¤ë©´, ì´ë§Œ '\n",
      "                       'ëŒ€í™”ë¥¼ ë§ˆë¬´ë¦¬í•˜ëŠ” ê²ƒì´ ì¢‹ê² ìŠµë‹ˆë‹¤.\\n'\n",
      "                       '\\n'\n",
      "                       'ì¦ê±°ìš´ í•˜ë£¨ ë³´ë‚´ì„¸ìš”! ğŸ˜Š'}],\n",
      "  'role': 'assistant'},\n",
      " {'content': [{'text': 'ì•ˆë…• ë‚˜ëŠ” ì¥ë™ì§„ì´ì•¼'}], 'role': 'user'},\n",
      " {'content': [{'text': 'ì¥ë™ì§„ë‹˜, ì£„ì†¡í•˜ì§€ë§Œ ê³„ì† ê°™ì€ ì¸ì‚¬ë§Œ ë°˜ë³µí•˜ì‹œëŠ” ìƒí™©ì´ë¼ ë” ì´ìƒì˜ ì˜ë¯¸ìˆëŠ” ëŒ€í™”ê°€ ì–´ë ¤ìš¸ ê²ƒ '\n",
      "                       'ê°™ìŠµë‹ˆë‹¤.\\n'\n",
      "                       '\\n'\n",
      "                       'ìƒˆë¡œìš´ ì£¼ì œë¡œ ëŒ€í™”ë¥¼ ì‹œì‘í•˜ê³  ì‹¶ìœ¼ì‹œë‹¤ë©´ ì–¸ì œë“  ë‹¤ì‹œ ë§ì”€í•´ ì£¼ì„¸ìš”. ê·¸ë•Œê¹Œì§€ëŠ” ì´ë§Œ ëŒ€í™”ë¥¼ '\n",
      "                       'ë§ˆì¹˜ë„ë¡ í•˜ê² ìŠµë‹ˆë‹¤.\\n'\n",
      "                       '\\n'\n",
      "                       'ì¢‹ì€ í•˜ë£¨ ë˜ì„¸ìš”! ğŸ‘‹'}],\n",
      "  'role': 'assistant'},\n",
      " {'content': [{'text': 'ì•ˆë…• ë‚˜ëŠ” ì¥ë™ì§„ì´ì•¼'}], 'role': 'user'},\n",
      " {'content': [{'text': 'ì¥ë™ì§„ë‹˜, ì£„ì†¡í•˜ì§€ë§Œ ë” ì´ìƒ ê°™ì€ ì¸ì‚¬ë¥¼ ë°˜ë³µí•˜ëŠ” ëŒ€í™”ëŠ” ì§„í–‰í•˜ê¸° ì–´ë µìŠµë‹ˆë‹¤.\\n'\n",
      "                       '\\n'\n",
      "                       'ì˜ë¯¸ ìˆëŠ” ìƒˆë¡œìš´ ëŒ€í™”ë¥¼ ë‚˜ëˆ„ê³  ì‹¶ìœ¼ì‹¤ ë•Œ ë‹¤ì‹œ ì°¾ì•„ì™€ ì£¼ì„¸ìš”. ì´ë§Œ ëŒ€í™”ë¥¼ ì¢…ë£Œí•˜ë„ë¡ '\n",
      "                       'í•˜ê² ìŠµë‹ˆë‹¤.\\n'\n",
      "                       '\\n'\n",
      "                       'ì•ˆë…•íˆ ê³„ì„¸ìš”. ğŸ‘‹'}],\n",
      "  'role': 'assistant'}]\n"
     ]
    }
   ],
   "source": [
    "message = \"ì•ˆë…• ë‚˜ëŠ” ì¥ë™ì§„ì´ì•¼\"\n",
    "agent, response = asyncio.run(process_streaming_response(agent, message))\n",
    "print (\"\\n\")\n",
    "pprint (agent.messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28ad3936",
   "metadata": {},
   "source": [
    "#### 3.1.2. SummarizingConversationManager\n",
    "\n",
    "ì˜¤ë˜ëœ ë©”ì‹œì§€ë¥¼ ìš”ì•½í•˜ì—¬ ì¤‘ìš”í•œ ì •ë³´ë¥¼ ë³´ì¡´í•˜ë©´ì„œ ì»¨í…ìŠ¤íŠ¸ í•œê³„ ë‚´ì—ì„œ ëŒ€í™”ë¥¼ ê´€ë¦¬í•©ë‹ˆë‹¤.\n",
    "\n",
    "**ì£¼ìš” ì„¤ì •:**\n",
    "\n",
    "| íŒŒë¼ë¯¸í„° | íƒ€ì… | ê¸°ë³¸ê°’ | ì„¤ëª… |\n",
    "|---------|------|--------|------|\n",
    "| `summary_ratio` | `float` | `0.3` | ì»¨í…ìŠ¤íŠ¸ ì¶•ì†Œ ì‹œ ìš”ì•½í•  ë©”ì‹œì§€ ë¹„ìœ¨ (0.1~0.8 ë²”ìœ„) |\n",
    "| `preserve_recent_messages` | `int` | `10` | í•­ìƒ ìœ ì§€í•  ìµœê·¼ ë©”ì‹œì§€ ìˆ˜ |\n",
    "| `summarization_agent` | `Agent` | `None` | ìš”ì•½ ìƒì„±ìš© ì»¤ìŠ¤í…€ ì—ì´ì „íŠ¸ (system_promptì™€ ë™ì‹œ ì‚¬ìš© ë¶ˆê°€) |\n",
    "| `summarization_system_prompt` | `str` | `None` | ìš”ì•½ìš© ì»¤ìŠ¤í…€ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ (agentì™€ ë™ì‹œ ì‚¬ìš© ë¶ˆê°€) |\n",
    "\n",
    "> **ê¸°ë³¸ ìš”ì•½ ë°©ì‹**: ì»¤ìŠ¤í…€ ì„¤ì •ì´ ì—†ì„ ê²½ìš°, ì£¼ìš” í† í”½, ì‚¬ìš©ëœ ë„êµ¬, ê¸°ìˆ ì  ì •ë³´ë¥¼ 3ì¸ì¹­ í˜•íƒœì˜ êµ¬ì¡°í™”ëœ ë¶ˆë¦¿ í¬ì¸íŠ¸ë¡œ ìš”ì•½í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "09ef6246",
   "metadata": {},
   "outputs": [],
   "source": [
    "from strands.agent.conversation_manager import SummarizingConversationManager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "ae8da725",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom system prompt for technical conversations\n",
    "custom_system_prompt = \"\"\"\n",
    "You are summarizing a technical conversation. Create a concise bullet-point summary that:\n",
    "- Focuses on code changes, architectural decisions, and technical solutions\n",
    "- Preserves specific function names, file paths, and configuration details\n",
    "- Omits conversational elements and focuses on actionable information\n",
    "- Uses technical terminology appropriate for software development\n",
    "\n",
    "Format as bullet points without conversational language.\n",
    "\"\"\"\n",
    "\n",
    "conversation_manager = SummarizingConversationManager(\n",
    "     summary_ratio=0.3,  # Summarize 30% of messages when context reduction is needed\n",
    "    preserve_recent_messages=3,  # Always keep 10 most recent messages\n",
    "    summarization_system_prompt=custom_system_prompt\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "52b6bbba",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.conversation_manager = conversation_manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee3f1844",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[97më„¤, \u001b[0m\u001b[97mì¥ë™ì§„ë‹˜\u001b[0m\u001b[97m. ì´\u001b[0m\u001b[97mì œ ì¶©\u001b[0m\u001b[97më¶„íˆ ì¸\u001b[0m\u001b[97mì‚¬ë¥¼ ë‚˜\u001b[0m\u001b[97mëˆ„ì—ˆë‹¤\u001b[0m\u001b[97mê³  ìƒê°\u001b[0m\u001b[97mí•©ë‹ˆë‹¤.\u001b[0m\u001b[97m \n",
      "\n",
      "ê³„\u001b[0m\u001b[97mì†í•´ì„œ \u001b[0m\u001b[97mê°™ì€ ì¸\u001b[0m\u001b[97mì‚¬ë§Œ\u001b[0m\u001b[97m ë°˜ë³µí•˜ì‹œëŠ”\u001b[0m\u001b[97m ê²ƒì´\u001b[0m\u001b[97m ì•„ì‰½ë„¤ìš”\u001b[0m\u001b[97m. ì €ëŠ”\u001b[0m\u001b[97m ë” ì˜\u001b[0m\u001b[97më¯¸ìˆëŠ” ëŒ€í™”\u001b[0m\u001b[97më¥¼ ë‚˜ëˆ„ê³  ì‹¶\u001b[0m\u001b[97mìŠµë‹ˆë‹¤.\u001b[0m\u001b[97m \n",
      "\n",
      "ë§Œì•½ ëŒ€\u001b[0m\u001b[97mí™”ë¥¼ ê³„ì†í•˜ê³  \u001b[0m\u001b[97mì‹¶ìœ¼ì‹œ\u001b[0m\u001b[97më‹¤ë©´, ìƒˆ\u001b[0m\u001b[97më¡œìš´ ì£¼\u001b[0m\u001b[97mì œë¡œ ì‹œ\u001b[0m\u001b[97mì‘í•´ì£¼ì‹œë©´ ê°\u001b[0m\u001b[97mì‚¬í•˜ê² \u001b[0m\u001b[97mìŠµë‹ˆë‹¤. ê·¸\u001b[0m\u001b[97më ‡ì§€ ì•Šë‹¤ë©´\u001b[0m\u001b[97m, ì´\u001b[0m\u001b[97më§Œ ëŒ€\u001b[0m\u001b[97mí™”ë¥¼ ë§ˆ\u001b[0m\u001b[97më¬´ë¦¬í•˜ëŠ”\u001b[0m\u001b[97m ê²ƒì´\u001b[0m\u001b[97m ì¢‹\u001b[0m\u001b[97mê² ìŠµë‹ˆë‹¤\u001b[0m\u001b[97m.\u001b[0m\u001b[97m\n",
      "\n",
      "ì¦ê±°ìš´ \u001b[0m\u001b[97mí•˜ë£¨ ë³´\u001b[0m\u001b[97më‚´ì„¸ìš”!\u001b[0m\u001b[97m ğŸ˜Š\u001b[0m[{'content': [{'text': 'ë„¤, ì¥ë™ì§„ë‹˜! ë°˜ë³µí•´ì„œ ì¸ì‚¬ë¥¼ ë‚˜ëˆ„ì‹œëŠ” ê²ƒ ê°™ë„¤ìš”. ì œê°€ ì´ë¯¸ ì•Œê³  ìˆì§€ë§Œ, ë‹¤ì‹œ í•œ ë²ˆ '\n",
      "                       'ì¸ì‚¬ë“œë¦½ë‹ˆë‹¤. ğŸ˜Š \\n'\n",
      "                       '\\n'\n",
      "                       'ì˜¤ëŠ˜ ê¸°ë¶„ì€ ì–´ë– ì‹ ê°€ìš”? ì œê°€ ë„ì™€ë“œë¦´ ë§Œí•œ ì¼ì´ ìˆë‹¤ë©´ ë§ì”€í•´ ì£¼ì„¸ìš”.'}],\n",
      "  'role': 'assistant'},\n",
      " {'content': [{'text': 'ì•ˆë…• ë‚˜ëŠ” ì¥ë™ì§„ì´ì•¼'}], 'role': 'user'},\n",
      " {'content': [{'text': 'ë„¤, ì¥ë™ì§„ë‹˜! ì´ì œ ì—¬ëŸ¬ ë²ˆ ì¸ì‚¬ë¥¼ ë‚˜ëˆ„ì—ˆë„¤ìš”. ğŸ˜Š \\n'\n",
      "                       '\\n'\n",
      "                       'ë§¤ë²ˆ ê°™ì€ ì¸ì‚¬ë¥¼ ë°˜ë³µí•˜ì‹œëŠ” ê²ƒ ê°™ì€ë°, í˜¹ì‹œ ë‹¤ë¥¸ ì´ì•¼ê¸°ë¥¼ ë‚˜ëˆ„ê³  ì‹¶ìœ¼ì‹ ê°€ìš”? ë‚ ì”¨ë‚˜ ì·¨ë¯¸, '\n",
      "                       'ë˜ëŠ” ê¶ê¸ˆí•˜ì‹  ì ì´ ìˆë‹¤ë©´ í¸í•˜ê²Œ ë§ì”€í•´ ì£¼ì„¸ìš”. ì œê°€ ì¦ê²ê²Œ ëŒ€í™”ë¥¼ ë‚˜ëˆ„ë„ë¡ í•˜ê² ìŠµë‹ˆë‹¤.'}],\n",
      "  'role': 'assistant'},\n",
      " {'content': [{'text': 'ì•ˆë…• ë‚˜ëŠ” ì¥ë™ì§„ì´ì•¼'}], 'role': 'user'},\n",
      " {'content': [{'text': 'ë„¤, ì¥ë™ì§„ë‹˜. ê³„ì†í•´ì„œ ê°™ì€ ì¸ì‚¬ë¥¼ ë°˜ë³µí•˜ì‹œë„¤ìš”. ğŸ˜Š\\n'\n",
      "                       '\\n'\n",
      "                       'ì¡°ê¸ˆ ë‹¤ë¥¸ ëŒ€í™”ë¥¼ ì‹œì‘í•´ë³´ëŠ” ê±´ ì–´ë–¨ê¹Œìš”? ì˜ˆë¥¼ ë“¤ì–´:\\n'\n",
      "                       '- ì˜¤ëŠ˜ í•˜ë£¨ëŠ” ì–´ë– ì…¨ë‚˜ìš”?\\n'\n",
      "                       '- ì·¨ë¯¸ë‚˜ ê´€ì‹¬ì‚¬ê°€ ìˆìœ¼ì‹ ê°€ìš”?\\n'\n",
      "                       '- ë‚ ì”¨ëŠ” ì–´ë–¤ê°€ìš”?\\n'\n",
      "                       '\\n'\n",
      "                       'ë” ì˜ë¯¸ ìˆëŠ” ëŒ€í™”ë¥¼ ë‚˜ëˆŒ ìˆ˜ ìˆì„ ê²ƒ ê°™ì•„ìš”. ì–´ë– ì‹ ê°€ìš”?'}],\n",
      "  'role': 'assistant'},\n",
      " {'content': [{'text': 'ì•ˆë…• ë‚˜ëŠ” ì¥ë™ì§„ì´ì•¼'}], 'role': 'user'},\n",
      " {'content': [{'text': 'ë„¤, ì¥ë™ì§„ë‹˜. ê³„ì† ê°™ì€ ì¸ì‚¬ë¥¼ ë°˜ë³µí•˜ì‹œëŠ”ë°ìš”. \\n'\n",
      "                       '\\n'\n",
      "                       'í˜¹ì‹œ ì œê°€ ë„ì™€ë“œë¦´ ìˆ˜ ìˆëŠ” ë‹¤ë¥¸ ì¼ì´ ìˆìœ¼ì‹ ê°€ìš”? ì•„ë‹ˆë©´ íŠ¹ë³„í•œ ì´ìœ ê°€ ìˆìœ¼ì‹ ê±´ê°€ìš”? ğŸ˜Š\\n'\n",
      "                       '\\n'\n",
      "                       'ë” ë‹¤ì–‘í•˜ê³  ì¬ë¯¸ìˆëŠ” ëŒ€í™”ë¥¼ ë‚˜ëˆŒ ìˆ˜ ìˆìœ¼ë©´ ì¢‹ê² ìŠµë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´:\\n'\n",
      "                       '- ì˜¤ëŠ˜ ê¸°ë¶„ì€ ì–´ë– ì‹ ê°€ìš”?\\n'\n",
      "                       '- ë¬´ì—‡ì„ í•˜ê³  ê³„ì‹ ê°€ìš”?\\n'\n",
      "                       '- ì €ì—ê²Œ ê¶ê¸ˆí•œ ì ì´ ìˆìœ¼ì‹ ê°€ìš”?\\n'\n",
      "                       '\\n'\n",
      "                       'í¸í•˜ê²Œ ë§ì”€í•´ ì£¼ì„¸ìš”!'}],\n",
      "  'role': 'assistant'},\n",
      " {'content': [{'text': 'ì•ˆë…• ë‚˜ëŠ” ì¥ë™ì§„ì´ì•¼'}], 'role': 'user'},\n",
      " {'content': [{'text': 'ë„¤, ì¥ë™ì§„ë‹˜. ê³„ì†í•´ì„œ ê°™ì€ ì¸ì‚¬ë¥¼ í•˜ì‹œëŠ” ê²ƒ ê°™ë„¤ìš”. \\n'\n",
      "                       '\\n'\n",
      "                       'ì €ëŠ” ëŒ€í™”ë¥¼ ë‚˜ëˆ„ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. ë‹¨ìˆœíˆ ì¸ì‚¬ë§Œ ë°˜ë³µí•˜ëŠ” ê²ƒë³´ë‹¤ëŠ” ì˜ë¯¸ ìˆëŠ” ëŒ€í™”ë¥¼ '\n",
      "                       'ë‚˜ëˆ„ê³  ì‹¶ìŠµë‹ˆë‹¤. \\n'\n",
      "                       '\\n'\n",
      "                       'í˜¹ì‹œ:\\n'\n",
      "                       '1. ë‹¤ë¥¸ ëŒ€í™”ë¥¼ í•˜ê³  ì‹¶ìœ¼ì‹ ë° ì–´ë–»ê²Œ ì‹œì‘í•´ì•¼ í• ì§€ ëª¨ë¥´ì‹œë‚˜ìš”?\\n'\n",
      "                       '2. ì•„ë‹ˆë©´ ì œ ê¸°ëŠ¥ì„ í…ŒìŠ¤íŠ¸í•´ë³´ê³  ê³„ì‹ ê±´ê°€ìš”?\\n'\n",
      "                       '3. ë˜ëŠ” íŠ¹ë³„í•œ ë„ì›€ì´ í•„ìš”í•˜ì‹ ê°€ìš”?\\n'\n",
      "                       '\\n'\n",
      "                       'ì–´ë–¤ ê²ƒì´ë“  ë§ì”€í•´ ì£¼ì‹œë©´ ìµœì„ ì„ ë‹¤í•´ ë„ì™€ë“œë¦¬ë„ë¡ í•˜ê² ìŠµë‹ˆë‹¤. ğŸ˜Š'}],\n",
      "  'role': 'assistant'},\n",
      " {'content': [{'text': 'ì•ˆë…• ë‚˜ëŠ” ì¥ë™ì§„ì´ì•¼'}], 'role': 'user'},\n",
      " {'content': [{'text': 'ë„¤, ì¥ë™ì§„ë‹˜. ê°™ì€ ì¸ì‚¬ë¥¼ ê³„ì† ë°˜ë³µí•˜ì‹œëŠ”ë°, ì´ì œëŠ” ë‹¤ë¥¸ ëŒ€í™”ë¡œ ë„˜ì–´ê°€ë©´ ì¢‹ê² ìŠµë‹ˆë‹¤. \\n'\n",
      "                       '\\n'\n",
      "                       'ì œê°€ ì§„ì‹¬ìœ¼ë¡œ ëŒ€í™”í•˜ê³  ì‹¶ì€ë°, ê³„ì† ê°™ì€ ì¸ì‚¬ë§Œ ë°›ìœ¼ë‹ˆ ì¡°ê¸ˆ ì•„ì‰½ë„¤ìš”. ğŸ˜Š\\n'\n",
      "                       '\\n'\n",
      "                       'í˜¹ì‹œ íŠ¹ë³„í•œ ì´ìœ ê°€ ìˆìœ¼ì‹ ê°€ìš”? ì•„ë‹ˆë©´ ë‹¤ë¥¸ ëŒ€í™”ë¥¼ ì–´ë–»ê²Œ ì‹œì‘í•´ì•¼ í• ì§€ ëª¨ë¥´ì‹œëŠ” ê±´ê°€ìš”? \\n'\n",
      "                       '\\n'\n",
      "                       'í¸í•˜ê²Œ ë‹¤ë¥¸ ì´ì•¼ê¸°ë¥¼ ì‹œì‘í•´ì£¼ì„¸ìš”. ì €ëŠ” ì–¸ì œë“  ì¤€ë¹„ë˜ì–´ ìˆë‹µë‹ˆë‹¤!'}],\n",
      "  'role': 'assistant'},\n",
      " {'content': [{'text': 'ì•ˆë…• ë‚˜ëŠ” ì¥ë™ì§„ì´ì•¼'}], 'role': 'user'},\n",
      " {'content': [{'text': 'ë„¤, ì¥ë™ì§„ë‹˜. ì´ì œ ì¶©ë¶„íˆ ì¸ì‚¬ë¥¼ ë‚˜ëˆ„ì—ˆë‹¤ê³  ìƒê°í•©ë‹ˆë‹¤. \\n'\n",
      "                       '\\n'\n",
      "                       'ê³„ì†í•´ì„œ ê°™ì€ ì¸ì‚¬ë§Œ ë°˜ë³µí•˜ì‹œëŠ” ê²ƒì´ ì•„ì‰½ë„¤ìš”. ì €ëŠ” ë” ì˜ë¯¸ìˆëŠ” ëŒ€í™”ë¥¼ ë‚˜ëˆ„ê³  ì‹¶ìŠµë‹ˆë‹¤. \\n'\n",
      "                       '\\n'\n",
      "                       'ë§Œì•½ ëŒ€í™”ë¥¼ ê³„ì†í•˜ê³  ì‹¶ìœ¼ì‹œë‹¤ë©´, ìƒˆë¡œìš´ ì£¼ì œë¡œ ì‹œì‘í•´ì£¼ì‹œë©´ ê°ì‚¬í•˜ê² ìŠµë‹ˆë‹¤. ê·¸ë ‡ì§€ ì•Šë‹¤ë©´, ì´ë§Œ '\n",
      "                       'ëŒ€í™”ë¥¼ ë§ˆë¬´ë¦¬í•˜ëŠ” ê²ƒì´ ì¢‹ê² ìŠµë‹ˆë‹¤.\\n'\n",
      "                       '\\n'\n",
      "                       'ì¦ê±°ìš´ í•˜ë£¨ ë³´ë‚´ì„¸ìš”! ğŸ˜Š'}],\n",
      "  'role': 'assistant'}]\n"
     ]
    }
   ],
   "source": [
    "message = \"ì•ˆë…• ë‚˜ëŠ” ì¥ë™ì§„ì´ì•¼\"\n",
    "agent, response = asyncio.run(process_streaming_response(agent, message))\n",
    "print (\"\\n\")\n",
    "pprint (agent.messages)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "basic-agent-frame (UV)",
   "language": "python",
   "name": "basic-agent-frame"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
