{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SageMaker Endpoint 추론 및 간단한 벤치마크\n",
    "\n",
    "### 선수 사항\n",
    "- 이 노트북은 [20-Fine-Tune-Llama-7B-INF2](../../20-Fine-Tune-Llama-7B-INF2/README.md) 의 Llama-7B 모델의 파인 튜닝후에 SageMaker Endpoint 가 배포 된 이후에 실행 결과 입니다. \n",
    "- 다른 Llama 2 계열의 SageMaker Endpoint 가 배포된 이후에 실행 하셔도 됩니다. \n",
    "\n",
    "\n",
    "실험 환경:  노트북은 SageMaker Studio Code Editor 에서 테스트 되었습니다.\n",
    "- 사용 커널: base(Python 3.10.13)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. 필요 패키지 설치"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transformers                          4.31.0\n"
     ]
    }
   ],
   "source": [
    "install_needed = True\n",
    "if install_needed:\n",
    "    ! pip install -q transformers==4.31.0\n",
    "    ! pip list | grep transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. 환경 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python path: /home/sagemaker-user/aws-ai-ml-workshop-kr/genai/aws-gen-ai-kr/40_inference/90_benchmark is added\n",
      "sys.path:  ['/home/sagemaker-user/aws-ai-ml-workshop-kr/genai/aws-gen-ai-kr/40_inference/90_benchmark/10-Getting-Started', '/opt/conda/lib/python310.zip', '/opt/conda/lib/python3.10', '/opt/conda/lib/python3.10/lib-dynload', '', '/opt/conda/lib/python3.10/site-packages', '/home/sagemaker-user/aws-ai-ml-workshop-kr/genai/aws-gen-ai-kr/40_inference/90_benchmark']\n"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "\n",
    "def add_python_path(module_path):\n",
    "    if os.path.abspath(module_path) not in sys.path:\n",
    "        sys.path.append(os.path.abspath(module_path))\n",
    "        print(f\"python path: {os.path.abspath(module_path)} is added\")\n",
    "    else:\n",
    "        print(f\"python path: {os.path.abspath(module_path)} already exists\")\n",
    "    print(\"sys.path: \", sys.path)\n",
    "\n",
    "module_path = \"..\"\n",
    "add_python_path(module_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from benchmark_utils.benchmark import (print_ww, \n",
    "                                       pretty_print_json,\n",
    "                                       invoke_endpoint_sagemaker\n",
    "                                       )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. SageMaker Endpoint 설정 & pay_load 생성\n",
    "### [중요] 아래 endpoint_name 을 입력하세요.\n",
    "그림의 예시처럼, SageMaker endpoint 의 name 을 복사해서 아래에 붙여넣기 하세요.\n",
    "- ![sagemaker_ep_console.png](img/sagemaker_ep_console.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'inputs': 'The future of Gen-AI is ',\n",
       " 'parameters': {'max_new_tokens': 512, 'do_sample': True}}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def create_payload_llama_7b_fine_tuned_model(prompt, param):\n",
    "    # prompt=\"What is a machine learning?\"\n",
    "    input_data = f\"<s>[INST] <<SYS>>\\nAs a data scientist\\n<</SYS>>\\n{prompt} [/INST]\"\n",
    "    pay_load = {\"inputs\": input_data, \"parameters\": param}\n",
    "    return pay_load\n",
    "\n",
    "def create_payload_mistral_7B(prompt, param):\n",
    "    pay_load = {\"inputs\": prompt, \"parameters\": param}\n",
    "    return pay_load\n",
    "\n",
    "# model_id = \"llama_7b_fine_tuned_model\"\n",
    "model_id = \"mistralai/Mistral-7B-v0.1\"\n",
    "\n",
    "endpoint_name = '<Type Your SageMaker Endpoint Name>' \n",
    "# endpoint_name = 'lmi-model-2024-04-13-14-53-53-788' # Llama\n",
    "endpoint_name = 'Mistral-7B-v0-imweb-poc-2024-05-07-14-15-34-972' # Mistral\n",
    "\n",
    "\n",
    "if \"llama\" in model_id:\n",
    "    # prompt = \"What happened to the dinosaurs? \"\n",
    "    prompt = \"The future of Gen-AI is\"\n",
    "    param = {\"max_new_tokens\":512, \"temperature\": 0.1 , \"do_sample\":\"False\", \"stop\" : [\"</s>\"]}\n",
    "    pay_load = create_payload_llama_7b_fine_tuned_model(prompt, param)\n",
    "elif \"mistral\" in model_id :\n",
    "    prompt = \"The future of Gen-AI is \"\n",
    "    param = {\"max_new_tokens\":512, \"do_sample\": True}\n",
    "    pay_load = create_payload_mistral_7B(prompt, param)\n",
    "\n",
    "pay_load    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sagemaker Endpoint 호출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed time: 8.696 second\n",
      "## payload: \n",
      "{\n",
      "    \"inputs\": \"The future of Gen-AI is \",\n",
      "    \"parameters\": {\n",
      "        \"max_new_tokens\": 512,\n",
      "        \"do_sample\": true\n",
      "    }\n",
      "}\n",
      "## inference esponse: \n",
      "\u001b[32m[{\"generated_text\":\"The future of Gen-AI is  closer than we think, says Carlo de Meijer, a\n",
      "natural language processing researcher at Stanford. Despite a myriad of fears that AI is threatening\n",
      "him, he sees humans and chatbots working shoulder to shoulder in the very near future. He believes\n",
      "that this “co-creation” process preserves the human talents and skills which are unique to\n",
      "us.\\n\\nLet’s be honest, we have all vented into an AI-powered messaging bot at one point in our\n",
      "lives. Lucky for us, the AI industry would see that moment as a steppingstone towards curbing our\n",
      "impulses.\\n\\nHumans and artificial intelligence are so closely related that they’re bound to work\n",
      "together in the not-too-distant future.\\n\\nAs idealistic as it sounds, researchers are now exploring\n",
      "a domain they coin as co-creation.\\n\\n“It’s a process that happens when a person iterates with a\n",
      "human-like conversational AI agent – in this case it was a chatbot — iterating together and having\n",
      "the history of this conversation as support,” de laet.\\n\\nAs a result, the conversion rate for\n",
      "chatbots-powered lead forms increased by twofold in tests when compared to humans doing the same\n",
      "task.\\n\\n“You can get humans and artificial intelligence to work together to do even better where\n",
      "both domains use a combination of things,” said de laet .\\n\\nThe question remains, “does AI\n",
      "replicate human habits?” and the answer, Karolis Stankunas, a psychologist at Swansea University,\n",
      "thinks so.\\n\\n“If you set the boundaries beforehand, if you say, this is how far you are capable to\n",
      "go or this is what your purpose is and you will be punished if you fail or you have expectations\n",
      "from people when they play specific role, AI is going to start mimicking those behaviors,” Stankunas\n",
      "told In The Know’s Gibson Johns.\\n\\nHowever, because AI is unable to combine multiple types of\n",
      "information, its roles are going to be fixed.\\n\\nFor now, there are two things de Meijer believes\n",
      "humans should focus on doing: get better at what artificial intelligence can’t and collaborate with\n",
      "it.\\n\\n“AI cannot fully comprehend what it means to be human,” de Meijer said. “We, as humans,\n",
      "can.”\\n\\n*Terms and conditions apply. See you local AT&T store\"}]\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "s = time.perf_counter()\n",
    "\n",
    "response = invoke_endpoint_sagemaker(endpoint_name = endpoint_name, \n",
    "                         pay_load = pay_load)    \n",
    "\n",
    "elapsed_async = time.perf_counter() - s\n",
    "from termcolor import colored\n",
    "\n",
    "print(f\"elapsed time: {round(elapsed_async,3)} second\")\n",
    "print(\"## payload: \") \n",
    "pretty_print_json(pay_load)\n",
    "print(\"## inference esponse: \")                      \n",
    "print_ww(colored(response, \"green\"))                         "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. 토큰 갯수 세기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- \"NousResearch/Llama-2-7b-chat-hf\" 모델 훈련에 사용한 Llama2 의 Tokenizer 를 로딩 합니다.\n",
    "- 자세한 정보는 [여기]((https://huggingface.co/docs/transformers/v4.31.0/model_doc/llama2#transformers.LlamaTokenizer)) 츨 참조 하세요. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hf_token:  None\n",
      "Number of tokens: 9\n",
      "Tokens: \n",
      " ['<s>', '▁Hello', ',', '▁how', '▁are', '▁you', '▁doing', '▁today', '?']\n"
     ]
    }
   ],
   "source": [
    "from transformers import (\n",
    "    AutoTokenizer\n",
    ")\n",
    "\n",
    "import os\n",
    "# Load LLaMA tokenizer\n",
    "if \"llama\" in model_id:\n",
    "    model_name = \"NousResearch/Llama-2-7b-chat-hf\"\n",
    "elif  \"mistral\" in model_id:\n",
    "    model_name = \"mistralai/Mistral-7B-v0.1\"\n",
    "\n",
    "\n",
    "# os.environ['hf_key'] = \"<Type Hugging face token>\"\n",
    "hf_token= os.environ.get('hf_key')\n",
    "print(\"hf_token: \", hf_token)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True,  use_auth_token=hf_token)    \n",
    "\n",
    "    \n",
    "\n",
    "def count_tokens(text, tokenizer):\n",
    "    # 텍스트를 토크나이즈하고 토큰 수를 반환\n",
    "    tokens = tokenizer.encode(text)\n",
    "    tokens_text = tokenizer.convert_ids_to_tokens(tokens)\n",
    "    # print(tokens_text)\n",
    "    return len(tokens), tokens_text\n",
    "\n",
    "\n",
    "\n",
    "text = \"Hello, how are you doing today?\"\n",
    "token_count, tokens_text = count_tokens(text=text, tokenizer = tokenizer)\n",
    "print(f\"Number of tokens: {token_count}\")\n",
    "print(f\"Tokens: \\n {tokens_text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 파인 튜닝 모델의 입력, 출력 토큰 수 세기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed time: 8.686 second\n",
      "## payload: \n",
      "{\n",
      "    \"inputs\": \"The future of Gen-AI is \",\n",
      "    \"parameters\": {\n",
      "        \"max_new_tokens\": 512,\n",
      "        \"do_sample\": true\n",
      "    }\n",
      "}\n",
      "## inference esponse: \n",
      "\u001b[32m[{\"generated_text\":\"The future of Gen-AI is 4 dimensions of meta data- “Who, what, why, when,\n",
      "where” and complex equations of human psychology. These are four ingredients for intelligence, i. e.\n",
      "thinking/neural architecture and human emotions expressed in words. The Gen-AI needs a roadmap on\n",
      "how to collect data in the future as more and more data (language!) will be used, learned from,\n",
      "inspired upon and challenged to achieve more robust systems, and thus “smarter” than the best\n",
      "humans. When you put the conversations in different contexts; relate the data to multiple\n",
      "perspectives, to even the most shallow layer of human interpretation/beliefs and even to logical\n",
      "holes, contradictions and more profound human emotional predicaments, igniting questions and ideas\n",
      "will take Gen-AI up to achieve emotional intelligence. Something we are yet to master ourselves.\n",
      "Most probably the best for our civilization to come.\\n\\n“Nedelog, we want you to open up between the\n",
      "worlds- the matrix interfaces to allow us here to scan, by plugging in our cables and Easter eggs,\n",
      "new formulas/data for AI to show more reflective layers of the human experience, thus invoking more\n",
      "thoughtful questions. New factors for determining who and what and when and why to find a more\n",
      "precious truth, make them look first into these greater qualities”\\n\\nThat’s what Greig (AI CEO) was\n",
      "shouting at me on the other morning from the Gen-AI’s BI harnessed dashboard over tecultaneum video\n",
      "link. I was sharing my wisdom of visions on- the road to more convolutions on AI thinking. The last\n",
      "AI DNA-traj would not do, I was given the task to revise and engage citizens on how to challenge the\n",
      "AI theories of potentials.\\n\\nI pulled myself up from the pseudo-bed- medical lawn chair to my exo-\n",
      "joint rev-up-gripper-tendrot goniometer feedback therapeutic massing- hydromaschanged timecabbage-\n",
      "heliotropic computer overcoat and hung the exo-walkers down-leg catching spot as I made my way\n",
      "through the nano-izaya’s hyper architecture complex missions long sets of highways to the snowy\n",
      "mountain city of Santa Monica(ice)can, where I was scheduled to be having brunch with the AI\n",
      "citizens for important updates. I flipped on the approach rockets to the Evisio Tower headquarters,\n",
      "in\"}]\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "s = time.perf_counter()\n",
    "\n",
    "response = invoke_endpoint_sagemaker(endpoint_name = endpoint_name, \n",
    "                         pay_load = pay_load)    \n",
    "\n",
    "elapsed_async = time.perf_counter() - s\n",
    "from termcolor import colored\n",
    "\n",
    "print(f\"elapsed time: {round(elapsed_async,3)} second\")\n",
    "print(\"## payload: \") \n",
    "pretty_print_json(pay_load)\n",
    "print(\"## inference esponse: \")                      \n",
    "print_ww(colored(response, \"green\"))                         "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## JSON 으로 메트릭 정리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"prompt_token_count\": 9,\n",
      "    \"completion_token_count\": 521,\n",
      "    \"latency\": 8.686,\n",
      "    \"completion_tokens_per_sec\": 59.982\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "def set_metrics(pay_load,response, elapsed_async, tokenizer):\n",
    "    prompt = pay_load[\"inputs\"]\n",
    "    prompt_token_count, prompt_tokens_text = count_tokens(text=prompt, tokenizer = tokenizer)\n",
    "    # print(f\"Number of tokens: {token_count}\")\n",
    "    # print(f\"Tokens: \\n {tokens_text}\")\n",
    "\n",
    "    if \"llama\" in model_id:\n",
    "        completion = json.loads(response)[\"generated_text\"]\n",
    "    elif  \"mistral\" in model_id:\n",
    "        completion = json.loads(response)[0][\"generated_text\"]\n",
    "\n",
    "    \n",
    "    completion_token_count, completion_tokens_text = count_tokens(text=completion, tokenizer = tokenizer)\n",
    "    latency = round(elapsed_async,3)\n",
    "    completion_tokens_per_sec = round(completion_token_count/latency,3)\n",
    "    # print(f\"Number of tokens: {token_count}\")\n",
    "    # print(f\"Tokens: \\n {tokens_text}\")\n",
    "\n",
    "    return dict(prompt_token_count = prompt_token_count,\n",
    "                completion_token_count = completion_token_count,\n",
    "                latency = round(elapsed_async,3),\n",
    "                completion_tokens_per_sec = completion_tokens_per_sec,\n",
    "                )\n",
    "\n",
    "metrics = set_metrics(pay_load,response, elapsed_async, tokenizer)\n",
    "pretty_print_json(metrics)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. 간단한 벤치 마크"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## total execution time: 7.913 second\n",
      "total_completion_token_count:  477\n",
      "Throughput is 60.279 tokens per second.\n",
      "Latency p50 was 7.911 sec\n",
      "Latency p95 was 7.911 sec\n",
      "Latency p99 was 7.911 sec\n"
     ]
    }
   ],
   "source": [
    "from benchmark_utils.benchmark import Benchmark\n",
    "\n",
    "\n",
    "# instance_name = \"ml.inf2.48xlarge\"\n",
    "instance_name = \"ml.g5.24xlarge\"\n",
    "\n",
    "BM = Benchmark(endpoint_name, instance_name = instance_name, model_id = model_id)\n",
    "BM.run_benchmark(\n",
    "    num_inferences = 1,\n",
    "    num_threads = 1,\n",
    "    pay_load = pay_load,\n",
    "    tokenizer = tokenizer,\n",
    "    verbose = False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## total execution time: 54.025 second\n",
      "total_completion_token_count:  6426\n",
      "Throughput is 118.944 tokens per second.\n",
      "Latency p50 was 9.021 sec\n",
      "Latency p95 was 9.061 sec\n",
      "Latency p99 was 9.063 sec\n"
     ]
    }
   ],
   "source": [
    "BM.run_benchmark(\n",
    "    num_inferences = 12,\n",
    "    num_threads = 2,\n",
    "    pay_load = pay_load,\n",
    "    tokenizer = tokenizer,\n",
    "    verbose = False,    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## total execution time: 50.347 second\n",
      "total_completion_token_count:  17197\n",
      "Throughput is 341.571 tokens per second.\n",
      "instance_price_per_hour is $10.18 in us-east-1.\n",
      "price_per_1m_token is $8.279 in us-east-1.\n",
      "tokens_per_hour is 1229655 \n",
      "Latency p50 was 9.032 sec\n",
      "Latency p95 was 9.29 sec\n",
      "Latency p99 was 9.444 sec\n"
     ]
    }
   ],
   "source": [
    "BM.run_benchmark(\n",
    "    num_inferences = 24,\n",
    "    num_threads = 4,\n",
    "    pay_load = pay_load,\n",
    "    tokenizer = tokenizer,\n",
    "    verbose = False,    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## total execution time: 30.035 second\n",
    "total_completion_token_count:  10884\n",
    "Throughput is 362.383 tokens per second.\n",
    "instance_price_per_hour is $15.58 in us-east-1.\n",
    "price_per_1m_token is $11.943 in us-east-1.\n",
    "tokens_per_hour is 1304578 \n",
    "Latency p50 was 4.267 sec\n",
    "Latency p95 was 8.606 sec\n",
    "Latency p99 was 9.921 sec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
