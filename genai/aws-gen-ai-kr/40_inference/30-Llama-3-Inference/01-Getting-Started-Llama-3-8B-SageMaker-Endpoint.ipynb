{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLama3 8B 모델을 Sagemaker를 통해 g5 인스턴스에 배포하기\n",
    "\n",
    "## 실험 환경\n",
    "- 이 노트북은 SageMaker Studio Code Editor 및 커널 base (Python 3.10.13) 에서 테스트 되었습니다.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. 사전 진행 내용\n",
    "- Llama3 모델을 사용하기 위해서는 아래의 웹페이지에 가서 본인의 계정으로 로그인 후에 \"동의\" 를 먼저 해야 합니다.\n",
    "    - [meta-llama/Meta-Llama-3-8B-Instruct](https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct)\n",
    "- 또한 본의 HF Key 을 얻기 위해서는, [User access tokens](https://huggingface.co/docs/hub/en/security-tokens) 참고 하세요."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. 환경 셋업"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "상위 폴더의 경로를 추가하여 해당 유틸리티, 이미지 폴더를 참조 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python path: /home/sagemaker-user/aws-ai-ml-workshop-kr/genai/aws-gen-ai-kr/40_inference is added\n",
      "sys.path:  ['/home/sagemaker-user/aws-ai-ml-workshop-kr/genai/aws-gen-ai-kr/40_inference/30-Llama-3-Inference', '/opt/conda/lib/python310.zip', '/opt/conda/lib/python3.10', '/opt/conda/lib/python3.10/lib-dynload', '', '/opt/conda/lib/python3.10/site-packages', '/home/sagemaker-user/aws-ai-ml-workshop-kr/genai/aws-gen-ai-kr/40_inference']\n"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "\n",
    "def add_python_path(module_path):\n",
    "    if os.path.abspath(module_path) not in sys.path:\n",
    "        sys.path.append(os.path.abspath(module_path))\n",
    "        print(f\"python path: {os.path.abspath(module_path)} is added\")\n",
    "    else:\n",
    "        print(f\"python path: {os.path.abspath(module_path)} already exists\")\n",
    "    print(\"sys.path: \", sys.path)\n",
    "\n",
    "module_path = \"..\"\n",
    "add_python_path(module_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install_needed = True\n",
    "install_needed = False\n",
    "\n",
    "if install_needed:\n",
    "    ! pip install sagemaker --upgrade  --quiet\n",
    "    ! pip list | grep -E \"sagemaker\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/sagemaker-user/.config/sagemaker/config.yaml\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker import Model, image_uris, serializers, deserializers\n",
    "import json\n",
    "\n",
    "role = sagemaker.get_execution_role()  # execution role for the endpoint\n",
    "sess = sagemaker.session.Session()  # sagemaker session for interacting with different AWS APIs\n",
    "region = sess._region_name  # region name of the current SageMaker Studio environment\n",
    "account_id = sess.account_id()  # account_id of the current SageMaker Studio environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [중요] Hugging Face HF_Key 를 환경변수에 저장 \n",
    "- 아래에 본인의 Key 를 입력하고, 주석을 제거 후에 사용하세요.\n",
    "\n",
    "    ```\n",
    "    key_val = \"<Type Your HF Key>\"\n",
    "    # set_hf_key_env_vars(hf_key_name, key_val)\n",
    "    ```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.huggingface import HuggingFaceModel, get_huggingface_llm_image_uri\n",
    "\n",
    "import os\n",
    "\n",
    "def set_hf_key_env_vars(hf_key_name, key_val):\n",
    "    os.environ[hf_key_name] = key_val\n",
    "\n",
    "def get_hf_key_env_vars(hf_key_name):\n",
    "    HF_key_value = os.environ.get(hf_key_name)\n",
    "\n",
    "    return HF_key_value\n",
    "\n",
    "hf_key_name = \"HF_KEY\"\n",
    "key_val = \"<Type Your HF Key>\"\n",
    "# set_hf_key_env_vars(hf_key_name, key_val)\n",
    "\n",
    "\n",
    "HF_key_value = get_hf_key_env_vars(hf_key_name)\n",
    "# print(\"HF_key_value: \", HF_key_value)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. HF 파라미터 설정\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모델 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model id\n",
    "hf_model_id = 'meta-llama/Meta-Llama-3-8B-Instruct'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 환경 변수 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instance type\n",
    "instance_type = \"ml.g5.2xlarge\"\n",
    "\n",
    "# Set GPU_NUM\n",
    "if instance_type == \"ml.g5.2xlarge\":\n",
    "    num_gpu = \"1\"\n",
    "elif instance_type == \"ml.g5.24xlarge\":\n",
    "    num_gpu = \"4\"\n",
    "else:\n",
    "    num_gpu = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hub Model configuration. https://huggingface.co/models\n",
    "hub = {\n",
    "\t'HF_MODEL_ID': hf_model_id,\n",
    "\t'SM_NUM_GPUS': num_gpu,\n",
    "\t'HUGGING_FACE_HUB_TOKEN': HF_key_value\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 추론 도커 이미지 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'763104351884.dkr.ecr.us-east-1.amazonaws.com/huggingface-pytorch-tgi-inference:2.3.0-tgi2.0.2-gpu-py310-cu121-ubuntu22.04'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_uri = get_huggingface_llm_image_uri(\"huggingface\",version=\"2.0.2\")\n",
    "image_uri"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SageMaker Model 의 하위 클래스인 HuggingFaceModel 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create Hugging Face Model Class\n",
    "huggingface_model = HuggingFaceModel(\n",
    "\timage_uri= image_uri,\n",
    "\tenv=hub,\n",
    "\trole=role, \n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. SageMaker Endpoint 에 배포 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## endpoint_name 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "endpoint_name:  Meta-Llama-3-8B-Instruct-ml-g5-2xlarge-2024-05-19-11-40-54\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "def create_ennpoint_name(model_id, instance_type):\n",
    "\n",
    "    hf_model_id = model_id.split('/')[1]\n",
    "\n",
    "    instance_type = instance_type.replace('.','-')\n",
    "    current_datetime = datetime.now()\n",
    "    formatted_datetime = current_datetime.strftime(\"%Y-%m-%d-%H-%M-%S\")\n",
    "    endpoint_name = f\"{hf_model_id}-{instance_type}-{formatted_datetime}\"\n",
    "\n",
    "    return endpoint_name\n",
    "\n",
    "endpoint_name = create_ennpoint_name(hf_model_id, instance_type)\n",
    "print(\"endpoint_name: \", endpoint_name)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SageMaker Endpoint 배포"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------!"
     ]
    }
   ],
   "source": [
    "# deploy model to SageMaker Inference\n",
    "predictor = huggingface_model.deploy(\n",
    "\tendpoint_name = endpoint_name,\n",
    "\tinitial_instance_count=1,\n",
    "\tinstance_type= instance_type,\n",
    "\tcontainer_startup_health_check_timeout=300,\n",
    "  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. 추론"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pay_load 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## payload: \n",
      "{\n",
      "    \"inputs\": \"My name is Clara and I am\",\n",
      "    \"parameters\": {\n",
      "        \"do_sample\": true,\n",
      "        \"max_new_tokens\": 256\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "from inference_utils.inference_util import ( print_ww, \n",
    "                                            pretty_print_json,\n",
    "                                       )\n",
    "                                       \n",
    "def create_payload_llama_8b(prompt, param):\n",
    "    # prompt=\"What is a machine learning?\"\n",
    "    input_data = f\"{prompt}\"\n",
    "    pay_load = {\"inputs\": input_data, \"parameters\": param}\n",
    "    return pay_load\n",
    "\n",
    "prompt = \"My name is Clara and I am\"\n",
    "param = {\"do_sample\": True, \"max_new_tokens\": 256}\n",
    "pay_load = create_payload_llama_8b(prompt, param)\n",
    "\n",
    "\n",
    "\n",
    "print(\"## payload: \") \n",
    "pretty_print_json(pay_load)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 추론"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "# Get a predictor for your endpoint\n",
    "predictor = sagemaker.Predictor(\n",
    "    endpoint_name= endpoint_name,\n",
    "    serializer=sagemaker.serializers.JSONSerializer(),\n",
    "    deserializer=sagemaker.deserializers.JSONDeserializer(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## inference esponse: \n",
      "\u001b[32m[{'generated_text': 'My name is Clara and I am a language enthusiast. I have a passion for\n",
      "languages and I believe that language learning is a powerful tool for self-discovery, cultural\n",
      "understanding, and global connections.\\nI am a native English speaker from the United States, but I\n",
      "have also studied Spanish, French, German, and Italian, and I am currently working on improving my\n",
      "Mandarin Chinese skills. I have had the opportunity to travel to several countries where I have\n",
      "immersed myself in the local language and culture, and I have found that language learning has\n",
      "opened doors to new experiences, new friends, and a deeper understanding of the world.\\nI believe\n",
      "that language learning should be a fun and engaging experience, and I strive to make my lessons\n",
      "enjoyable and interactive. I use a variety of teaching methods and resources to help my students\n",
      "achieve their language learning goals, including conversation practice, grammar exercises, reading\n",
      "and writing activities, and cultural discussions.\\nMy goal is to help my students become confident\n",
      "and proficient language learners, and to inspire them to continue exploring and learning about\n",
      "languages and cultures.\\nWhy learn a language with me?\\nHere are some reasons why you might consider\n",
      "learning a language with me:\\n\\n* I am a native English speaker with a strong understanding of the\n",
      "language and culture.\\n* I am experienced in teaching languages, and I have worked with students\n",
      "of'}]\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "s = time.perf_counter()\n",
    "\n",
    "response = predictor.predict(pay_load)\n",
    "\n",
    "elapsed_async = time.perf_counter() - s\n",
    "\n",
    "from termcolor import colored\n",
    "\n",
    "print(\"## inference esponse: \")                      \n",
    "print_ww(colored(response, \"green\"))                         \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. 엔드포인트 삭제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_endpoint_model(endpoint_name,llm_model ):\n",
    "    sess.delete_endpoint(endpoint_name)\n",
    "    sess.delete_endpoint_config(endpoint_name)\n",
    "    llm_model.delete_model()\n",
    "\n",
    "# delete_endpoint_model(endpoint_name,huggingface_model)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
