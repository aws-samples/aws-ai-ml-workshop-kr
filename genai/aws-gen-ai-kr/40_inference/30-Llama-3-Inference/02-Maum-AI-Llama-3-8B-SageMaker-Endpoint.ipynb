{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Maum-AI LLama3 8B 모델을 Sagemaker를 통해 g5 인스턴스에 배포하기 \n",
    "\n",
    "---\n",
    "\n",
    "## Ref\n",
    "- [maum-ai/Llama-3-MAAL-8B-Instruct-v0.1](https://huggingface.co/maum-ai/Llama-3-MAAL-8B-Instruct-v0.1)\n",
    "\n",
    "---\n",
    "\n",
    "## 실험 환경\n",
    "- 이 노트북은 SageMaker Studio Code Editor 및 커널 base (Python 3.10.13) 에서 테스트 되었습니다.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. 환경 셋업"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "상위 폴더의 경로를 추가하여 해당 유틸리티, 이미지 폴더를 참조 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python path: /home/ec2-user/SageMaker/aws-ai-ml-workshop-kr/genai/aws-gen-ai-kr/40_inference is added\n",
      "sys.path:  ['/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python310.zip', '/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10', '/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/lib-dynload', '', '/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages', '/home/ec2-user/SageMaker/aws-ai-ml-workshop-kr/genai/aws-gen-ai-kr/40_inference']\n"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "\n",
    "def add_python_path(module_path):\n",
    "    if os.path.abspath(module_path) not in sys.path:\n",
    "        sys.path.append(os.path.abspath(module_path))\n",
    "        print(f\"python path: {os.path.abspath(module_path)} is added\")\n",
    "    else:\n",
    "        print(f\"python path: {os.path.abspath(module_path)} already exists\")\n",
    "    print(\"sys.path: \", sys.path)\n",
    "\n",
    "module_path = \"..\"\n",
    "add_python_path(module_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# install_needed = True\n",
    "install_needed = False\n",
    "\n",
    "if install_needed:\n",
    "    ! pip install sagemaker --upgrade  --quiet\n",
    "    ! pip list | grep -E \"sagemaker\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/ec2-user/.config/sagemaker/config.yaml\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker import Model, image_uris, serializers, deserializers\n",
    "import json\n",
    "\n",
    "role = sagemaker.get_execution_role()  # execution role for the endpoint\n",
    "sess = sagemaker.session.Session()  # sagemaker session for interacting with different AWS APIs\n",
    "region = sess._region_name  # region name of the current SageMaker Studio environment\n",
    "account_id = sess.account_id()  # account_id of the current SageMaker Studio environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. HF 파라미터 설정\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모델 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# model id\n",
    "hf_model_id = 'maum-ai/Llama-3-MAAL-8B-Instruct-v0.1'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 환경 변수 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# instance type\n",
    "instance_type = \"ml.g5.2xlarge\"\n",
    "\n",
    "# Set GPU_NUM\n",
    "if instance_type == \"ml.g5.2xlarge\":\n",
    "    num_gpu = \"1\"\n",
    "elif instance_type == \"ml.g5.24xlarge\":\n",
    "    num_gpu = \"4\"\n",
    "else:\n",
    "    num_gpu = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "hub = {\n",
    "\t'HF_MODEL_ID': hf_model_id,\n",
    "\t'SM_NUM_GPUS': num_gpu\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 추론 도커 이미지 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'763104351884.dkr.ecr.us-east-1.amazonaws.com/huggingface-pytorch-tgi-inference:2.3.0-tgi2.0.2-gpu-py310-cu121-ubuntu22.04'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sagemaker.huggingface import HuggingFaceModel, get_huggingface_llm_image_uri\n",
    "\n",
    "image_uri = get_huggingface_llm_image_uri(\"huggingface\",version=\"2.0.2\")\n",
    "image_uri"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SageMaker Model 의 하위 클래스인 HuggingFaceModel 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create Hugging Face Model Class\n",
    "huggingface_model = HuggingFaceModel(\n",
    "\timage_uri= image_uri,\n",
    "\tenv=hub,\n",
    "\trole=role, \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. SageMaker Endpoint 에 배포 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## endpoint_name 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "endpoint_name:  Llama-3-MAAL-8B-Instruct-v0-1-ml-g5-2xlarge-2024-06-04-12-17-17\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "def create_ennpoint_name(model_id, instance_type):\n",
    "\n",
    "    hf_model_id = model_id.split('/')[1]\n",
    "\n",
    "    instance_type = instance_type.replace('.','-')\n",
    "    hf_model_id = hf_model_id.replace('.','-')\n",
    "    current_datetime = datetime.now()\n",
    "    formatted_datetime = current_datetime.strftime(\"%Y-%m-%d-%H-%M-%S\")\n",
    "    endpoint_name = f\"{hf_model_id}-{instance_type}-{formatted_datetime}\"\n",
    "\n",
    "    return endpoint_name\n",
    "\n",
    "endpoint_name = create_ennpoint_name(hf_model_id, instance_type)\n",
    "print(\"endpoint_name: \", endpoint_name)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SageMaker Endpoint 배포"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------!CPU times: user 165 ms, sys: 3.9 ms, total: 169 ms\n",
      "Wall time: 7min 2s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "predictor = huggingface_model.deploy(\n",
    "\tendpoint_name = endpoint_name,\n",
    "\tinitial_instance_count=1,\n",
    "\tinstance_type= instance_type,\n",
    "\tcontainer_startup_health_check_timeout=300,\n",
    "  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. 추론"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pay_load 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from inference_utils.inference_util import ( print_ww, \n",
    "                                            pretty_print_json,\n",
    "                                            invoke_endpoint_sagemaker\n",
    "                                       )\n",
    "                                       \n",
    "def create_payload_llama_8b(prompt, param):\n",
    "    # prompt=\"What is a machine learning?\"\n",
    "    input_data = f\"{prompt}\"\n",
    "    pay_load = {\"inputs\": input_data, \"parameters\": param}\n",
    "\n",
    "    # payload_str = json.dumps(pay_load)\n",
    "    return pay_load\n",
    "    # return payload_str.encode(\"utf-8\")\n",
    "    \n",
    "def llama3_output_parser(response):\n",
    "    completion = json.loads(response)\n",
    "    completion = completion[0]\n",
    "    completion = completion['generated_text']\n",
    "\n",
    "    return completion\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt = \"Generative AI is\"\n",
    "param = {\"do_sample\": True, \"max_new_tokens\": 256}\n",
    "pay_load = create_payload_llama_8b(prompt, param)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## payload: \n",
      "{\n",
      "    \"inputs\": \"Generative AI is\",\n",
      "    \"parameters\": {\n",
      "        \"do_sample\": true,\n",
      "        \"max_new_tokens\": 256\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(\"## payload: \") \n",
    "pretty_print_json(pay_load)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 추론"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generative AI is the process by which computer systems create new content or results autonomously.\n",
      "It uses deep learning algorithms and neural networks to analyze data and generate new, original\n",
      "content that can be customized to human preferences based on patterns in the input data.\n",
      "Generative AI is widely used in many industries, including:\n",
      "Content creation and publishing\n",
      "Creating business logos\n",
      "Developing video game assets\n",
      "Augmenting product photos\n",
      "Developing virtual and augmented reality environments\n",
      "Creating interactive and personalized advertising\n",
      "Your enterprises can use NetOn's generative AI platform to unlock autonomous content creation and\n",
      "maximize your business potential. Let's schedule a call to discuss how NetOn can support your\n",
      "enterprise's growth.\n"
     ]
    }
   ],
   "source": [
    "response = invoke_endpoint_sagemaker(endpoint_name = endpoint_name, \n",
    "                         pay_load = pay_load)    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "response = llama3_output_parser(response)\n",
    "print_ww(response)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 한글 추론"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## payload: \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'inputs': '생성형 AI 는',\n",
       " 'parameters': {'do_sample': True, 'max_new_tokens': 256}}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"생성형 AI 는\"\n",
    "param = {\"do_sample\": True, \"max_new_tokens\": 256}\n",
    "pay_load = create_payload_llama_8b(prompt, param)\n",
    "\n",
    "\n",
    "\n",
    "print(\"## payload: \") \n",
    "# pretty_print_json(pay_load)\n",
    "pay_load\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## inference esponse: \n",
      "\u001b[32m생성형 AI 는 기계가 스스로 학습하고 인간과 같은 추론 능력을 갖추는 것을 돕는 교육 모델입니다. 생성형 AI 알고리즘은 컴퓨터에게 데이터를 활용하여 환경을 이해하도록\n",
      "가르치는 것입니다. 개인이 새로운 것을 요구하고, 이 데이터를 기반으로 결과를 생성함으로써 컴퓨터는 경험을 통해 계속 학습하며 복잡한 패턴을 인식하는 방법을 배우게 됩니다. 이를\n",
      "통해 데이터가 광범위하고 희귀한 경우에도 알고리즘은 새로운 장면, 객체 및 행동을 보는 것이 가능해집니다.\n",
      "궁금한 점이 있으면 언제든지 문의해 주세요.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "s = time.perf_counter()\n",
    "\n",
    "response = invoke_endpoint_sagemaker(endpoint_name = endpoint_name, \n",
    "                         pay_load = pay_load)    \n",
    "\n",
    "elapsed_async = time.perf_counter() - s\n",
    "\n",
    "\n",
    "response = llama3_output_parser(response)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from termcolor import colored\n",
    "\n",
    "print(\"## inference esponse: \")                      \n",
    "print_ww(colored(response, \"green\"))                         \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 추론 요약 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## payload: \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'inputs': '다음 글을 30자 아내로 요약해줘.\\n, 대학교육협의회는 이르면 이번 주 내년도 대학입학 전형시행계획을 심의하고 최종 의대 모집 규모를 승인합니다.\\n교육부 관계자는 수험생과 학부모를 위해 가능한 빨리 모집 규모를 확정할 필요가 있다며 구체적인 심사 일정을 논의하겠다고 설명했습니다.최종 승인 결과에 따라 내년도 의대 정원은 올해보다 천500명 안팎으로 늘어납니다.대통령실은 의대 증원이 일단락됐다며 각 대학이 의대 증원을 담은 학칙 개정을 조속히 완료해달라고 당부했습니다.동시에 의료계를 향해선 의대 증원 유예와 백지화라는 실현 불가능한 전제조건을 접고 대화하자고 제안했습니다.그러나 의료계 반발은 여전합니다.당장 의대생 단체는 정부가 귀를 닫고 복귀만을 호소하는 오만한 태도를 보이고 있다고 비판했습니다.전공의들 역시 복귀하지 않겠다는 뜻을 분명히 하고 있습니다.대한의사협회도 오는 22일 비공개 긴급회의를 열고 의대 교수 등과 구체적인 대응 수단을 논의합니다.의료계가 기댔던 의대 증원 집행정지 신청이 기각·각하된 만큼 다음 단계 대응 로드맵을 마련하겠다는 겁니다.\\n\\n',\n",
       " 'parameters': {'do_sample': True, 'max_new_tokens': 512}}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"다음 글을 30자 아내로 요약해줘.\\n, 대학교육협의회는 이르면 이번 주 내년도 대학입학 전형시행계획을 심의하고 최종 의대 모집 규모를 승인합니다.\\n\\\n",
    "교육부 관계자는 수험생과 학부모를 위해 가능한 빨리 모집 규모를 확정할 필요가 있다며 구체적인 심사 일정을 논의하겠다고 설명했습니다.\\\n",
    "최종 승인 결과에 따라 내년도 의대 정원은 올해보다 천500명 안팎으로 늘어납니다.\\\n",
    "대통령실은 의대 증원이 일단락됐다며 각 대학이 의대 증원을 담은 학칙 개정을 조속히 완료해달라고 당부했습니다.\\\n",
    "동시에 의료계를 향해선 의대 증원 유예와 백지화라는 실현 불가능한 전제조건을 접고 대화하자고 제안했습니다.\\\n",
    "그러나 의료계 반발은 여전합니다.\\\n",
    "당장 의대생 단체는 정부가 귀를 닫고 복귀만을 호소하는 오만한 태도를 보이고 있다고 비판했습니다.\\\n",
    "전공의들 역시 복귀하지 않겠다는 뜻을 분명히 하고 있습니다.\\\n",
    "대한의사협회도 오는 22일 비공개 긴급회의를 열고 의대 교수 등과 구체적인 대응 수단을 논의합니다.\\\n",
    "의료계가 기댔던 의대 증원 집행정지 신청이 기각·각하된 만큼 다음 단계 대응 로드맵을 마련하겠다는 겁니다.\\n\\n\"\n",
    "\n",
    "param = {\"do_sample\": True, \"max_new_tokens\": 512}\n",
    "pay_load = create_payload_llama_8b(prompt, param)\n",
    "\n",
    "print(\"## payload: \") \n",
    "# pretty_print_json(pay_load)\n",
    "\n",
    "pay_load\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## inference esponse: \n",
      "\u001b[32m다음 글을 30자 아내로 요약해줘.\n",
      ", 대학교육협의회는 이르면 이번 주 내년도 대학입학 전형시행계획을 심의하고 최종 의대 모집 규모를 승인합니다.\n",
      "교육부 관계자는 수험생과 학부모를 위해 가능한 빨리 모집 규모를 확정할 필요가 있다며 구체적인 심사 일정을 논의하겠다고 설명했습니다.최종 승인 결과에 따라 내년도 의대 정원은\n",
      "올해보다 천500명 안팎으로 늘어납니다.대통령실은 의대 증원이 일단락됐다며 각 대학이 의대 증원을 담은 학칙 개정을 조속히 완료해달라고 당부했습니다.동시에 의료계를 향해선 의대\n",
      "증원 유예와 백지화라는 실현 불가능한 전제조건을 접고 대화하자고 제안했습니다.그러나 의료계 반발은 여전합니다.당장 의대생 단체는 정부가 귀를 닫고 복귀만을 호소하는 오만한 태도를\n",
      "보이고 있다고 비판했습니다.전공의들 역시 복귀하지 않겠다는 뜻을 분명히 하고 있습니다.대한의사협회도 오는 22일 비공개 긴급회의를 열고 의대 교수 등과 구체적인 대응 수단을\n",
      "논의합니다.의료계가 기댔던 의대 증원 집행정지 신청이 기각·각하된 만큼 다음 단계 대응 로드맵을 마련하겠다는 겁니다.\n",
      "\n",
      "요약 하나: 대학교육협의회는 내년도 의대 입학 전형 시행 계획과 의대 모집 규모 확정을 앞두고 있다. 이번 확대는 기존의 오류와 지연을 설명하기 하였으며, 의사협회와의 대화 제안에\n",
      "대해 의료계는 아직 반발하고 있다.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "s = time.perf_counter()\n",
    "\n",
    "response = invoke_endpoint_sagemaker(endpoint_name = endpoint_name, \n",
    "                         pay_load = pay_load)    \n",
    "\n",
    "elapsed_async = time.perf_counter() - s\n",
    "\n",
    "\n",
    "response = llama3_output_parser(response)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from termcolor import colored\n",
    "\n",
    "print(\"## inference esponse: \")                      \n",
    "print_ww(colored(response, \"green\"))                         \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. 엔드포인트 삭제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def delete_endpoint_model(endpoint_name,llm_model ):\n",
    "    sess.delete_endpoint(endpoint_name)\n",
    "    sess.delete_endpoint_config(endpoint_name)\n",
    "    llm_model.delete_model()\n",
    "\n",
    "delete_endpoint_model(endpoint_name,huggingface_model)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
