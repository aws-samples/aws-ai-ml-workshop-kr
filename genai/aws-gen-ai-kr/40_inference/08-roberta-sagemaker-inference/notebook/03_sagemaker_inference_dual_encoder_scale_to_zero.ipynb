{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# SageMaker Inference: BiEncoder RoBerta with Scale-to-Zero Scheduling\n",
    "\n",
    "[KLUE RoBERTa](https://huggingface.co/klue/roberta-base) ëª¨ë¸ì„ SageMaker Inference Componentë¡œ ë°°í¬í•˜ê³  ì¶”ë¡ í•©ë‹ˆë‹¤.\n",
    "\n",
    "ì´ ë…¸íŠ¸ë¶ì€ **Scale-to-Zero** ê¸°ëŠ¥ì„ ì‚¬ìš©í•˜ì—¬ ì£¼ë§ì—ëŠ” ì¸ìŠ¤í„´ìŠ¤ë¥¼ 0ìœ¼ë¡œ ì¤„ì´ê³  í‰ì¼ì—ëŠ” ë‹¤ì‹œ ë³µì›í•˜ëŠ” ìŠ¤ì¼€ì¤„ë§ì„ êµ¬í˜„í•©ë‹ˆë‹¤.\n",
    "\n",
    "**ì£¼ìš” ê¸°ëŠ¥:**\n",
    "- Inference Componentë¥¼ ì‚¬ìš©í•œ ëª¨ë¸ ë°°í¬\n",
    "- ManagedInstanceScalingìœ¼ë¡œ MinInstanceCount=0 ì„¤ì •\n",
    "- EventBridge Schedulerë¥¼ ì‚¬ìš©í•œ ìë™ ìŠ¤ì¼€ì¼ë§\n",
    "- ì£¼ë§(ê¸ˆìš”ì¼ ì €ë…) Scale-in, í‰ì¼(ì›”ìš”ì¼ ì•„ì¹¨) Scale-out\n",
    "\n",
    "#### ì°¸ì¡° ë¸”ë¡œê·¸\n",
    "- [Unlock cost savings with the new scale down to zero feature in SageMaker Inference](https://aws.amazon.com/blogs/machine-learning/unlock-cost-savings-with-the-new-scale-down-to-zero-feature-in-amazon-sagemaker-inference/)\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## [ì„ ìˆ˜ ì‘ì—…] AWS Role ì •ë³´ë¥¼ .env íŒŒì¼ì— ì•„ë˜ì™€ ê°™ì´ ì €ì¥\n",
    "```\n",
    "SAGEMAKER_ROLE_ARN=arn:aws:iam::XXXXXX:role/gonsoomoon-sm-inference\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": [
    "## 0. í™˜ê²½ í™•ì¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cell-3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/lab/16-robert-sagemaker-inference/setup/.venv/bin/python\n",
      "\u001b[2mUsing Python 3.11.0rc1 environment at: /home/ubuntu/lab/16-robert-sagemaker-inference/setup/.venv\u001b[0m\n",
      "torch                            2.5.0+cu121\n"
     ]
    }
   ],
   "source": [
    "! which python\n",
    "! uv pip list | grep torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv('../.env')\n",
    "SAGEMAKER_ROLE_ARN = os.getenv('SAGEMAKER_ROLE_ARN')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-6",
   "metadata": {},
   "source": [
    "## 1. í™˜ê²½ ì„¤ì •"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cell-7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/ubuntu/.config/sagemaker/config.yaml\n",
      "Bucket: sagemaker-us-east-1-057716757052\n",
      "Region: us-east-1\n",
      "Role: arn:aws:iam::057716757052:role/gonsoomoon-sm-inference\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import time\n",
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker.pytorch.model import PyTorchModel\n",
    "from sagemaker.serializers import JSONSerializer\n",
    "from sagemaker.deserializers import JSONDeserializer\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "sagemaker_client = boto3.client(\"sagemaker\")\n",
    "role = SAGEMAKER_ROLE_ARN\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "region = sagemaker_session._region_name\n",
    "\n",
    "print(f\"Bucket: {bucket}\")\n",
    "print(f\"Region: {region}\")\n",
    "print(f\"Role: {role}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-8",
   "metadata": {},
   "source": [
    "## 2. ëª¨ë¸ ì•„í‹°íŒ©íŠ¸ ìƒì„± ë° S3 ì—…ë¡œë“œ\n",
    "\n",
    "model.tar.gz êµ¬ì¡°ë¡œ ìƒì„±ì„ í•˜ë©´, SageMaker ê°€ ì´ë¥¼ ì¸ì§€ í•©ë‹ˆë‹¤.\n",
    "\n",
    "model.tar.gz êµ¬ì¡°:\n",
    "```\n",
    "model.tar.gz/\n",
    "â”œâ”€â”€ config.json\n",
    "â”œâ”€â”€ model.safetensors\n",
    "â”œâ”€â”€ special_tokens_map.json\n",
    "â”œâ”€â”€ tokenizer.json\n",
    "â”œâ”€â”€ tokenizer_config.json\n",
    "â”œâ”€â”€ vocab.txt\n",
    "â””â”€â”€ code/\n",
    "    â”œâ”€â”€ inference.py\n",
    "    â””â”€â”€ requirements.txt\n",
    "```\n",
    "\n",
    "ì°¸ì¡°: [SageMaker PyTorch Documentation](https://sagemaker.readthedocs.io/en/stable/frameworks/pytorch/using_pytorch.html#deploy-pytorch-models)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-9",
   "metadata": {},
   "source": [
    "### model.tar.gz íŒŒì¼ ìƒì„±"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cell-10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "upload: ../model.tar.gz to s3://sagemaker-us-east-1-057716757052/klue-roberta-inference/model/model.tar.gz\n",
      "Model uploaded to: s3://sagemaker-us-east-1-057716757052/klue-roberta-inference/model/model.tar.gz\n"
     ]
    }
   ],
   "source": [
    "!rm -rf ../model_artifact\n",
    "!mkdir -p ../model_artifact/code\n",
    "!cp ../src/inference.py ../model_artifact/code/\n",
    "!cp ../src/requirements.txt ../model_artifact/code/\n",
    "!cp ../model/* ../model_artifact/\n",
    "\n",
    "!cd ../model_artifact && tar -czf ../model.tar.gz *\n",
    "\n",
    "model_artifact_s3_uri = f's3://{bucket}/klue-roberta-inference/model/model.tar.gz'\n",
    "!aws s3 cp ../model.tar.gz {model_artifact_s3_uri}\n",
    "\n",
    "print(f\"Model uploaded to: {model_artifact_s3_uri}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-11",
   "metadata": {},
   "source": [
    "## 3. SageMaker Model ìƒì„±\n",
    "\n",
    "Inference Componentë¥¼ ì‚¬ìš©í•˜ê¸° ìœ„í•´ì„œëŠ” ë¨¼ì € SageMaker Modelì„ ìƒì„±í•´ì•¼ í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cell-12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch Image URI: 763104351884.dkr.ecr.us-east-1.amazonaws.com/pytorch-inference:2.5-gpu-py311\n"
     ]
    }
   ],
   "source": [
    "# PyTorch ì´ë¯¸ì§€ URI ê°€ì ¸ì˜¤ê¸°\n",
    "from sagemaker import image_uris\n",
    "\n",
    "pytorch_image_uri = image_uris.retrieve(\n",
    "    framework=\"pytorch\",\n",
    "    region=region,\n",
    "    version=\"2.5\",\n",
    "    py_version=\"py311\",\n",
    "    instance_type=\"ml.g4dn.xlarge\",\n",
    "    image_scope=\"inference\"\n",
    ")\n",
    "\n",
    "print(f\"PyTorch Image URI: {pytorch_image_uri}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7le84fseqmo",
   "metadata": {},
   "source": [
    "#### SageMaker Model ìƒì„±"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cell-13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Model created: roberta-dual-encoder-1760700181-8454-model\n",
      "Model Name: roberta-dual-encoder-1760700181-8454-model\n"
     ]
    }
   ],
   "source": [
    "# SageMaker Model ìƒì„±\n",
    "prefix = sagemaker.utils.unique_name_from_base(\"roberta-dual-encoder\")\n",
    "model_name = f\"{prefix}-model\"\n",
    "\n",
    "# ëª¨ë¸ì´ ì´ë¯¸ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸\n",
    "try:\n",
    "    sagemaker_client.describe_model(ModelName=model_name)\n",
    "    print(f\"âœ… Model already exists: {model_name}\")\n",
    "except sagemaker_client.exceptions.ClientError as e:\n",
    "    if \"Could not find model\" in str(e):\n",
    "        # ëª¨ë¸ì´ ì¡´ì¬í•˜ì§€ ì•Šìœ¼ë©´ ìƒì„±\n",
    "        create_model_response = sagemaker_client.create_model(\n",
    "            ModelName=model_name,\n",
    "            ExecutionRoleArn=role,\n",
    "            PrimaryContainer={\n",
    "                \"Image\": pytorch_image_uri,\n",
    "                \"ModelDataUrl\": model_artifact_s3_uri,\n",
    "                \"Environment\": {\n",
    "                    \"SAGEMAKER_PROGRAM\": \"inference.py\",\n",
    "                    \"SAGEMAKER_SUBMIT_DIRECTORY\": model_artifact_s3_uri,\n",
    "                    \"SAGEMAKER_CONTAINER_LOG_LEVEL\": \"20\",\n",
    "                    \"SAGEMAKER_REGION\": region,\n",
    "                    \"MMS_DEFAULT_WORKERS_PER_MODEL\": \"1\"\n",
    "                }\n",
    "            }\n",
    "        )\n",
    "        print(f\"âœ… Model created: {model_name}\")\n",
    "    else:\n",
    "        raise\n",
    "\n",
    "print(f\"Model Name: {model_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-14",
   "metadata": {},
   "source": [
    "## 4. Endpoint Configuration ìƒì„± (Scale-to-Zero ì§€ì›)\n",
    "\n",
    "**ManagedInstanceScaling**ì„ í™œì„±í™”í•˜ê³  **MinInstanceCount=0**ìœ¼ë¡œ ì„¤ì •í•˜ì—¬ ì¸ìŠ¤í„´ìŠ¤ë¥¼ 0ìœ¼ë¡œ ì¤„ì¼ ìˆ˜ ìˆë„ë¡ í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cell-15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Endpoint Config created: roberta-dual-encoder-1760700181-8454-scale-to-zero-config\n",
      "Endpoint Config Name: roberta-dual-encoder-1760700181-8454-scale-to-zero-config\n"
     ]
    }
   ],
   "source": [
    "# Endpoint Configuration ì„¤ì •\n",
    "endpoint_config_name = f\"{prefix}-scale-to-zero-config\"\n",
    "variant_name = \"AllTraffic\"\n",
    "instance_type = \"ml.g4dn.xlarge\"\n",
    "model_data_download_timeout_in_seconds = 3600\n",
    "container_startup_health_check_timeout_in_seconds = 3600\n",
    "\n",
    "min_instance_count = 0  # Scale-to-Zeroë¥¼ ìœ„í•´ 0ìœ¼ë¡œ ì„¤ì •\n",
    "max_instance_count = 2\n",
    "\n",
    "# Endpoint Configê°€ ì´ë¯¸ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸\n",
    "try:\n",
    "    sagemaker_client.describe_endpoint_config(EndpointConfigName=endpoint_config_name)\n",
    "    print(f\"âœ… Endpoint Config already exists: {endpoint_config_name}\")\n",
    "except sagemaker_client.exceptions.ClientError as e:\n",
    "    if \"Could not find endpoint configuration\" in str(e):\n",
    "        # Endpoint Configê°€ ì¡´ì¬í•˜ì§€ ì•Šìœ¼ë©´ ìƒì„±\n",
    "        sagemaker_client.create_endpoint_config(\n",
    "            EndpointConfigName=endpoint_config_name,\n",
    "            ExecutionRoleArn=role,\n",
    "            ProductionVariants=[\n",
    "                {\n",
    "                    \"VariantName\": variant_name,\n",
    "                    \"InstanceType\": instance_type,\n",
    "                    \"InitialInstanceCount\": 1,\n",
    "                    \"ModelDataDownloadTimeoutInSeconds\": model_data_download_timeout_in_seconds,\n",
    "                    \"ContainerStartupHealthCheckTimeoutInSeconds\": container_startup_health_check_timeout_in_seconds,\n",
    "                    \"ManagedInstanceScaling\": {\n",
    "                        \"Status\": \"ENABLED\",\n",
    "                        \"MinInstanceCount\": min_instance_count,\n",
    "                        \"MaxInstanceCount\": max_instance_count,\n",
    "                    },\n",
    "                    \"RoutingConfig\": {\"RoutingStrategy\": \"LEAST_OUTSTANDING_REQUESTS\"},\n",
    "                }\n",
    "            ],\n",
    "        )\n",
    "        print(f\"âœ… Endpoint Config created: {endpoint_config_name}\")\n",
    "    else:\n",
    "        raise\n",
    "\n",
    "print(f\"Endpoint Config Name: {endpoint_config_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-16",
   "metadata": {},
   "source": [
    "## 5. SageMaker Endpoint ìƒì„±"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cell-17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Endpoint creation initiated: roberta-dual-encoder-1760700181-8454-scale-to-zero-endpoint\n",
      "Endpoint Name: roberta-dual-encoder-1760700181-8454-scale-to-zero-endpoint\n"
     ]
    }
   ],
   "source": [
    "# Endpoint ìƒì„±\n",
    "endpoint_name = f\"{prefix}-scale-to-zero-endpoint\"\n",
    "\n",
    "# Endpointê°€ ì´ë¯¸ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸\n",
    "try:\n",
    "    endpoint_desc = sagemaker_client.describe_endpoint(EndpointName=endpoint_name)\n",
    "    print(f\"âœ… Endpoint already exists: {endpoint_name}\")\n",
    "    print(f\"   Status: {endpoint_desc['EndpointStatus']}\")\n",
    "except sagemaker_client.exceptions.ClientError as e:\n",
    "    if \"Could not find endpoint\" in str(e):\n",
    "        # Endpointê°€ ì¡´ì¬í•˜ì§€ ì•Šìœ¼ë©´ ìƒì„±\n",
    "        sagemaker_client.create_endpoint(\n",
    "            EndpointName=endpoint_name,\n",
    "            EndpointConfigName=endpoint_config_name,\n",
    "        )\n",
    "        print(f\"âœ… Endpoint creation initiated: {endpoint_name}\")\n",
    "    else:\n",
    "        raise\n",
    "\n",
    "print(f\"Endpoint Name: {endpoint_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating\n",
      "Creating\n",
      "Creating\n",
      "Creating\n",
      "InService\n",
      "\n",
      "Total time taken: 150.48 seconds (2.51 minutes)\n"
     ]
    }
   ],
   "source": [
    "# Endpointê°€ InService ìƒíƒœê°€ ë  ë•Œê¹Œì§€ ëŒ€ê¸° (~3-5ë¶„ ì†Œìš”)\n",
    "import time\n",
    "import sys\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "while True:\n",
    "    desc = sagemaker_client.describe_endpoint(EndpointName=endpoint_name)\n",
    "    status = desc[\"EndpointStatus\"]\n",
    "    print(status)\n",
    "    sys.stdout.flush()\n",
    "    if status in [\"InService\", \"Failed\"]:\n",
    "        break\n",
    "    time.sleep(30)\n",
    "\n",
    "total_time = time.time() - start_time\n",
    "print(f\"\\nTotal time taken: {total_time:.2f} seconds ({total_time/60:.2f} minutes)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-19",
   "metadata": {},
   "source": [
    "## 6. Inference Component ìƒì„±\n",
    "\n",
    "Inference Componentë¥¼ ìƒì„±í•˜ì—¬ ëª¨ë¸ì„ Endpointì— ë°°í¬í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cell-20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Inference Component creation initiated: roberta-dual-encoder-1760700181-8454-inference-component\n",
      "Inference Component Name: roberta-dual-encoder-1760700181-8454-inference-component\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.compute_resource_requirements.resource_requirements import ResourceRequirements\n",
    "\n",
    "inference_component_name = f\"{prefix}-inference-component\"\n",
    "\n",
    "# Inference Componentê°€ ì´ë¯¸ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸\n",
    "try:\n",
    "    ic_desc = sagemaker_client.describe_inference_component(\n",
    "        InferenceComponentName=inference_component_name\n",
    "    )\n",
    "    print(f\"âœ… Inference Component already exists: {inference_component_name}\")\n",
    "    print(f\"   Status: {ic_desc['InferenceComponentStatus']}\")\n",
    "except sagemaker_client.exceptions.ClientError as e:\n",
    "    if \"Could not find inference component\" in str(e):\n",
    "        # Inference Componentê°€ ì¡´ì¬í•˜ì§€ ì•Šìœ¼ë©´ ìƒì„±\n",
    "        sagemaker_client.create_inference_component(\n",
    "            InferenceComponentName=inference_component_name,\n",
    "            EndpointName=endpoint_name,\n",
    "            VariantName=variant_name,\n",
    "            Specification={\n",
    "                \"ModelName\": model_name,\n",
    "                \"StartupParameters\": {\n",
    "                    \"ModelDataDownloadTimeoutInSeconds\": model_data_download_timeout_in_seconds,\n",
    "                    \"ContainerStartupHealthCheckTimeoutInSeconds\": container_startup_health_check_timeout_in_seconds,\n",
    "                },\n",
    "                \"ComputeResourceRequirements\": {\n",
    "                    \"MinMemoryRequiredInMb\": 4096,\n",
    "                    \"NumberOfAcceleratorDevicesRequired\": 1,\n",
    "                },\n",
    "            },\n",
    "            RuntimeConfig={\"CopyCount\": 1},\n",
    "        )\n",
    "        print(f\"âœ… Inference Component creation initiated: {inference_component_name}\")\n",
    "    else:\n",
    "        raise\n",
    "\n",
    "print(f\"Inference Component Name: {inference_component_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cell-21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating\n",
      "Creating\n",
      "Creating\n",
      "Creating\n",
      "Creating\n",
      "Creating\n",
      "Creating\n",
      "Creating\n",
      "Creating\n",
      "Creating\n",
      "InService\n",
      "\n",
      "Total time taken: 332.07 seconds (5.53 minutes)\n"
     ]
    }
   ],
   "source": [
    "# Inference Componentê°€ InService ìƒíƒœê°€ ë  ë•Œê¹Œì§€ ëŒ€ê¸°\n",
    "start_time = time.time()\n",
    "\n",
    "while True:\n",
    "    desc = sagemaker_client.describe_inference_component(\n",
    "        InferenceComponentName=inference_component_name\n",
    "    )\n",
    "    status = desc[\"InferenceComponentStatus\"]\n",
    "    print(status)\n",
    "    sys.stdout.flush()\n",
    "    if status in [\"InService\", \"Failed\"]:\n",
    "        break\n",
    "    time.sleep(30)\n",
    "\n",
    "total_time = time.time() - start_time\n",
    "print(f\"\\nTotal time taken: {total_time:.2f} seconds ({total_time/60:.2f} minutes)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-22",
   "metadata": {},
   "source": [
    "## 7. Endpoint ì¶”ë¡  í…ŒìŠ¤íŠ¸"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-23",
   "metadata": {},
   "source": [
    "### ì‹±ê¸€ ìƒ˜í”Œ ì¶”ë¡ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e5ae4f35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ§ª Testing inference component...\n",
      "   Queries: 1\n",
      "   Documents: 1\n",
      "âœ… Success! Response time: 0.53s\n",
      "   Embedding dimension: 768\n",
      "\n",
      "Query embeddings shape: (1, 768)\n",
      "Document embeddings shape: (1, 768)\n",
      "\n",
      "Cosine similarity:\n",
      "   Pair 1: 0.8666\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def test_inference_component(endpoint_name, inference_component_name, payload):\n",
    "      \"\"\"\n",
    "      ê°„ë‹¨í•œ ì¶”ë¡  í…ŒìŠ¤íŠ¸ - Inference Componentê°€ ì‘ë™í•˜ëŠ”ì§€ í™•ì¸\n",
    "      \n",
    "      Args:\n",
    "          endpoint_name: SageMaker Endpoint ì´ë¦„\n",
    "          inference_component_name: Inference Component ì´ë¦„\n",
    "          payload: ì¶”ë¡  ìš”ì²­ í˜ì´ë¡œë“œ (dict with 'queries' and 'documents')\n",
    "      \"\"\"\n",
    "      import boto3\n",
    "      import json\n",
    "      import time\n",
    "      import numpy as np\n",
    "\n",
    "      runtime_client = boto3.Session().client('sagemaker-runtime')\n",
    "\n",
    "      try:\n",
    "          print(\"ğŸ§ª Testing inference component...\")\n",
    "          print(f\"   Queries: {len(payload['queries'])}\")\n",
    "          print(f\"   Documents: {len(payload['documents'])}\")\n",
    "\n",
    "          start_time = time.time()\n",
    "\n",
    "          response = runtime_client.invoke_endpoint(\n",
    "              EndpointName=endpoint_name,\n",
    "              InferenceComponentName=inference_component_name,\n",
    "              ContentType='application/json',\n",
    "              Body=json.dumps(payload)\n",
    "          )\n",
    "\n",
    "          # ì‘ë‹µ ë³¸ë¬¸ ì½ê¸°\n",
    "          response_body = response['Body'].read().decode()\n",
    "\n",
    "          # ë””ë²„ê¹…: ì‘ë‹µ ë‚´ìš© í™•ì¸\n",
    "          if not response_body:\n",
    "              print(f\"âš ï¸  Warning: Empty response body\")\n",
    "              return False\n",
    "\n",
    "          # JSON íŒŒì‹±\n",
    "          result = json.loads(response_body)\n",
    "          elapsed_time = time.time() - start_time\n",
    "\n",
    "          print(f\"âœ… Success! Response time: {elapsed_time:.2f}s\")\n",
    "          print(f\"   Embedding dimension: {result['embedding_dim']}\")\n",
    "\n",
    "          # Embedding shapes ì¶œë ¥\n",
    "          print(f\"\\nQuery embeddings shape: ({result['num_queries']}, {result['embedding_dim']})\")\n",
    "          print(f\"Document embeddings shape: ({result['num_documents']}, {result['embedding_dim']})\")\n",
    "\n",
    "          # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚° ë° ì¶œë ¥\n",
    "          query_embs = np.array(result[\"query_embeddings\"])\n",
    "          doc_embs = np.array(result[\"doc_embeddings\"])\n",
    "\n",
    "          print(f\"\\nCosine similarity:\")\n",
    "          for i in range(len(query_embs)):\n",
    "              similarity = np.dot(query_embs[i], doc_embs[i])\n",
    "              print(f\"   Pair {i+1}: {similarity:.4f}\")\n",
    "\n",
    "          return True\n",
    "\n",
    "      except json.JSONDecodeError as e:\n",
    "          print(f\"âŒ JSON parsing failed: {str(e)}\")\n",
    "          print(f\"   Response body (first 200 chars): {response_body[:200] if 'response_body' in locals() else 'N/A'}\")\n",
    "          return False\n",
    "      except Exception as e:\n",
    "          print(f\"âŒ Failed: {str(e)}\")\n",
    "          print(f\"   Error type: {type(e).__name__}\")\n",
    "          return False\n",
    "\n",
    "# ì‚¬ìš© ì˜ˆì‹œ 1: ë‹¨ì¼ ì¿¼ë¦¬-ë¬¸ì„œ ìŒ\n",
    "payload1 = {\n",
    "    \"queries\": [\"ë§›ìˆëŠ” í•œêµ­ ì „í†µ ìŒì‹ ê¹€ì¹˜ì°Œê°œ\"],\n",
    "    \"documents\": [\"ê¹€ì¹˜ì°Œê°œì™€ ëœì¥ì°Œê°œëŠ” í•œêµ­ì˜ ëŒ€í‘œ ì „í†µ ìŒì‹ì…ë‹ˆë‹¤.\"]\n",
    "}\n",
    "  \n",
    "test_inference_component(endpoint_name, inference_component_name, payload1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-25",
   "metadata": {},
   "source": [
    "### 8ê°œ ì¿¼ë¦¬-ë¬¸ì„œ ìŒ ë°°ì¹˜ ì¶”ë¡ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cell-26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch inference completed:\n",
      "  Queries: 8\n",
      "  Documents: 8\n",
      "  Embedding dim: 768\n",
      "\n",
      "Pair-wise cosine similarities:\n",
      "  Pair 1: 0.8666\n",
      "  Pair 2: 0.7164\n",
      "  Pair 3: 0.4745\n",
      "  Pair 4: 0.6969\n",
      "  Pair 5: 0.6220\n",
      "  Pair 6: 0.5832\n",
      "  Pair 7: 0.6482\n",
      "  Pair 8: 0.6714\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import json\n",
    "\n",
    "# Create the runtime client\n",
    "runtime_client = boto3.Session().client('sagemaker-runtime')\n",
    "\n",
    "batch_payload = {\n",
    "    \"queries\": [\n",
    "        \"ë§›ìˆëŠ” í•œêµ­ ì „í†µ ìŒì‹ ê¹€ì¹˜ì°Œê°œ\",\n",
    "        \"ìµœì‹  ê¸°ìˆ  ë°œì „\",\n",
    "        \"ìƒ‰ê¹”\",\n",
    "        \"ì—¬í–‰ ê³„íš\",\n",
    "        \"ìŠ¤í¬ì¸  ê²½ê¸°\",\n",
    "        \"ì˜í™” ì¶”ì²œ\",\n",
    "        \"ë‚ ì”¨ ì •ë³´\",\n",
    "        \"ê±´ê°• ê´€ë¦¬\"\n",
    "    ],\n",
    "    \"documents\": [\n",
    "        \"ê¹€ì¹˜ì°Œê°œì™€ ëœì¥ì°Œê°œëŠ” í•œêµ­ì˜ ëŒ€í‘œ ì „í†µ ìŒì‹ì…ë‹ˆë‹¤.\",\n",
    "        \"ì¸ê³µì§€ëŠ¥ ê¸°ìˆ ì´ ë¹ ë¥´ê²Œ ë°œì „í•˜ê³  ìˆìŠµë‹ˆë‹¤.\",\n",
    "        \"íŒŒë¦¬ì˜ ì—í íƒ‘ì€ í”„ë‘ìŠ¤ì˜ ìƒì§•ì…ë‹ˆë‹¤.\",\n",
    "        \"ì œì£¼ë„ëŠ” í•œêµ­ì˜ ì¸ê¸° ì—¬í–‰ì§€ì…ë‹ˆë‹¤.\",\n",
    "        \"ì¶•êµ¬ ê²½ê¸°ê°€ ì˜¤ëŠ˜ ì €ë…ì— ìˆìŠµë‹ˆë‹¤.\",\n",
    "        \"ìµœê·¼ ê°œë´‰í•œ ì˜í™”ê°€ ì¢‹ì€ í‰ê°€ë¥¼ ë°›ê³  ìˆìŠµë‹ˆë‹¤.\",\n",
    "        \"ë‚´ì¼ì€ ë§‘ì€ ë‚ ì”¨ê°€ ì˜ˆìƒë©ë‹ˆë‹¤.\",\n",
    "        \"ê·œì¹™ì ì¸ ìš´ë™ì´ ê±´ê°•ì— ì¢‹ìŠµë‹ˆë‹¤.\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "response = runtime_client.invoke_endpoint(\n",
    "    EndpointName=endpoint_name,\n",
    "    InferenceComponentName=inference_component_name,\n",
    "    ContentType='application/json',\n",
    "    Body=json.dumps(batch_payload)\n",
    ")\n",
    "\n",
    "batch_result = json.loads(response['Body'].read().decode())\n",
    "\n",
    "print(f\"Batch inference completed:\")\n",
    "print(f\"  Queries: {batch_result['num_queries']}\")\n",
    "print(f\"  Documents: {batch_result['num_documents']}\")\n",
    "print(f\"  Embedding dim: {batch_result['embedding_dim']}\\n\")\n",
    "\n",
    "# ê° ìŒì˜ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°\n",
    "query_embs = np.array(batch_result['query_embeddings'])\n",
    "doc_embs = np.array(batch_result['doc_embeddings'])\n",
    "\n",
    "print(\"Pair-wise cosine similarities:\")\n",
    "for i in range(len(query_embs)):\n",
    "    similarity = np.dot(query_embs[i], doc_embs[i])\n",
    "    print(f\"  Pair {i+1}: {similarity:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f40f2ee5",
   "metadata": {},
   "source": [
    "## ìˆ˜ë™ìœ¼ë¡œ Scale-to-Zero í…ŒìŠ¤íŠ¸ \n",
    "\n",
    "ìŠ¤ì¼€ì¤„ì„ ê¸°ë‹¤ë¦¬ì§€ ì•Šê³  ì¦‰ì‹œ í…ŒìŠ¤íŠ¸í•˜ë ¤ë©´ ì•„ë˜ ì½”ë“œë¥¼ ì‹¤í–‰í•˜ì„¸ìš”."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c0004a2",
   "metadata": {},
   "source": [
    "### inference_component ë¥¼ 0ê°œë¡œ ì¤„ì´ê¸° (Scale In)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a2e32508",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Inference Component scaled down to 0 copies\n",
      "ì¸ìŠ¤í„´ìŠ¤ê°€ ì¢…ë£Œë˜ëŠ” ë° ëª‡ ë¶„ì´ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n"
     ]
    }
   ],
   "source": [
    "# CopyCountë¥¼ 0ìœ¼ë¡œ ì„¤ì •í•˜ì—¬ ìŠ¤ì¼€ì¼ ë‹¤ìš´\n",
    "sagemaker_client.update_inference_component_runtime_config(\n",
    "    InferenceComponentName=inference_component_name,\n",
    "    DesiredRuntimeConfig={'CopyCount': 0}\n",
    ")\n",
    "\n",
    "print(\"âœ… Inference Component scaled down to 0 copies\")\n",
    "print(\"ì¸ìŠ¤í„´ìŠ¤ê°€ ì¢…ë£Œë˜ëŠ” ë° ëª‡ ë¶„ì´ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e12d4b7b",
   "metadata": {},
   "source": [
    "### ìƒíƒœ í™•ì¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8d63e8e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Inference Component scaled down to 0 copies ìš”ì²­ ì™„ë£Œ\n",
      "â³ ì¸ìŠ¤í„´ìŠ¤ê°€ ì¢…ë£Œë˜ëŠ” ê³¼ì •ì„ ëª¨ë‹ˆí„°ë§í•©ë‹ˆë‹¤...\n",
      "\n",
      "[0s] ğŸ“Š Status Update:\n",
      "   IC Status: Updating\n",
      "   Current CopyCount: 0\n",
      "   Desired CopyCount: 0\n",
      "   RuntimeConfig: {\"DesiredCopyCount\": 0, \"CurrentCopyCount\": 0}\n",
      "\n",
      "âœ… Scale-down ì™„ë£Œ! CopyCountê°€ 0ì´ ë˜ì—ˆìŠµë‹ˆë‹¤.\n",
      "   ì´ ì†Œìš” ì‹œê°„: 0ì´ˆ (0.0ë¶„)\n",
      "\n",
      "ğŸ¯ ëª¨ë‹ˆí„°ë§ ì™„ë£Œ\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import json\n",
    "\n",
    "# CopyCountë¥¼ 0ìœ¼ë¡œ ì„¤ì •í•˜ì—¬ ìŠ¤ì¼€ì¼ ë‹¤ìš´\n",
    "sagemaker_client.update_inference_component_runtime_config(\n",
    "    InferenceComponentName=inference_component_name,\n",
    "    DesiredRuntimeConfig={'CopyCount': 0}\n",
    ")\n",
    "\n",
    "print(\"âœ… Inference Component scaled down to 0 copies ìš”ì²­ ì™„ë£Œ\")\n",
    "print(\"â³ ì¸ìŠ¤í„´ìŠ¤ê°€ ì¢…ë£Œë˜ëŠ” ê³¼ì •ì„ ëª¨ë‹ˆí„°ë§í•©ë‹ˆë‹¤...\\n\")\n",
    "\n",
    "# ìŠ¤ì¼€ì¼ ë‹¤ìš´ ëª¨ë‹ˆí„°ë§\n",
    "max_wait_time = 600  # 10ë¶„ íƒ€ì„ì•„ì›ƒ\n",
    "start_time = time.time()\n",
    "previous_copy_count = None\n",
    "\n",
    "while True:\n",
    "    try:\n",
    "        elapsed = time.time() - start_time\n",
    "\n",
    "        # íƒ€ì„ì•„ì›ƒ ì²´í¬\n",
    "        if elapsed > max_wait_time:\n",
    "            print(f\"\\nâš ï¸ Timeout: {max_wait_time}ì´ˆ ê²½ê³¼. ëª¨ë‹ˆí„°ë§ ì¢…ë£Œ.\")\n",
    "            break\n",
    "\n",
    "        # Inference Component ìƒíƒœ í™•ì¸\n",
    "        ic_desc = sagemaker_client.describe_inference_component(\n",
    "            InferenceComponentName=inference_component_name\n",
    "        )\n",
    "\n",
    "        status = ic_desc['InferenceComponentStatus']\n",
    "        runtime_config = ic_desc.get('RuntimeConfig', {})\n",
    "        current_copy_count = runtime_config.get('CurrentCopyCount', runtime_config.get('CopyCount', 'N/A'))\n",
    "        desired_copy_count = runtime_config.get('DesiredCopyCount', 0)\n",
    "\n",
    "        # ìƒíƒœ ë³€ê²½ ì‹œì—ë§Œ ì¶œë ¥\n",
    "        if current_copy_count != previous_copy_count:\n",
    "            print(f\"[{elapsed:.0f}s] ğŸ“Š Status Update:\")\n",
    "            print(f\"   IC Status: {status}\")\n",
    "            print(f\"   Current CopyCount: {current_copy_count}\")\n",
    "            print(f\"   Desired CopyCount: {desired_copy_count}\")\n",
    "\n",
    "            # Full RuntimeConfig (ë””ë²„ê¹…ìš©)\n",
    "            print(f\"   RuntimeConfig: {json.dumps(runtime_config, default=str)}\")\n",
    "            print()\n",
    "\n",
    "            previous_copy_count = current_copy_count\n",
    "\n",
    "        # CopyCountê°€ 0ì´ ë˜ë©´ ì¢…ë£Œ\n",
    "        if current_copy_count == 0:\n",
    "            print(f\"âœ… Scale-down ì™„ë£Œ! CopyCountê°€ 0ì´ ë˜ì—ˆìŠµë‹ˆë‹¤.\")\n",
    "            print(f\"   ì´ ì†Œìš” ì‹œê°„: {elapsed:.0f}ì´ˆ ({elapsed/60:.1f}ë¶„)\")\n",
    "            break\n",
    "\n",
    "        # 10ì´ˆ ëŒ€ê¸°\n",
    "        time.sleep(10)\n",
    "\n",
    "    except sagemaker_client.exceptions.ClientError as e:\n",
    "        if \"Could not find inference component\" in str(e):\n",
    "            print(f\"âŒ Inference Component not found: {inference_component_name}\")\n",
    "            break\n",
    "        else:\n",
    "            print(f\"âŒ Error: {str(e)}\")\n",
    "            break\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Unexpected error: {str(e)}\")\n",
    "        break\n",
    "\n",
    "print(\"\\nğŸ¯ ëª¨ë‹ˆí„°ë§ ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "561353de",
   "metadata": {},
   "source": [
    "#### ì‹¤ì œ í…ŒìŠ¤íŠ¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "05e4edd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ§ª Testing inference component...\n",
      "   Queries: 1\n",
      "   Documents: 1\n",
      "âŒ Failed: An error occurred (ValidationError) when calling the InvokeEndpoint operation: Inference Component has no capacity to process this request. ApplicationAutoScaling may be in-progress (if configured) or try to increase the capacity by invoking UpdateInferenceComponentRuntimeConfig API.\n",
      "   Error type: ValidationError\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ì‚¬ìš© ì˜ˆì‹œ 1: ë‹¨ì¼ ì¿¼ë¦¬-ë¬¸ì„œ ìŒ\n",
    "payload1 = {\n",
    "    \"queries\": [\"ë§›ìˆëŠ” í•œêµ­ ì „í†µ ìŒì‹ ê¹€ì¹˜ì°Œê°œ\"],\n",
    "    \"documents\": [\"ê¹€ì¹˜ì°Œê°œì™€ ëœì¥ì°Œê°œëŠ” í•œêµ­ì˜ ëŒ€í‘œ ì „í†µ ìŒì‹ì…ë‹ˆë‹¤.\"]\n",
    "}\n",
    "  \n",
    "test_inference_component(endpoint_name, inference_component_name, payload1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57671ff6",
   "metadata": {},
   "source": [
    "### inference_component 1 ê°œ ìƒì„±í•˜ì—¬ Scale Out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f0bd64d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Inference Component scaled up to 1 copy\n",
      "ì¸ìŠ¤í„´ìŠ¤ê°€ ì‹œì‘ë˜ëŠ” ë° ëª‡ ë¶„ì´ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n"
     ]
    }
   ],
   "source": [
    "# CopyCountë¥¼ 1ë¡œ ì„¤ì •í•˜ì—¬ ìŠ¤ì¼€ì¼ ì—…\n",
    "sagemaker_client.update_inference_component_runtime_config(\n",
    "    InferenceComponentName=inference_component_name,\n",
    "    DesiredRuntimeConfig={'CopyCount': 1}\n",
    ")\n",
    "\n",
    "print(\"âœ… Inference Component scaled up to 1 copy\")\n",
    "print(\"ì¸ìŠ¤í„´ìŠ¤ê°€ ì‹œì‘ë˜ëŠ” ë° ëª‡ ë¶„ì´ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f92ef6a0",
   "metadata": {},
   "source": [
    "### inference_component ì—…ë°ì´íŠ¸ ìƒíƒœ í™•ì¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6d74b54d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating\n",
      "Updating\n",
      "Updating\n",
      "Updating\n",
      "Updating\n",
      "Updating\n",
      "Updating\n",
      "Updating\n",
      "Updating\n",
      "Updating\n",
      "InService\n",
      "\n",
      "Total time taken: 331.88 seconds (5.53 minutes)\n"
     ]
    }
   ],
   "source": [
    "# Inference Componentê°€ InService ìƒíƒœê°€ ë  ë•Œê¹Œì§€ ëŒ€ê¸°\n",
    "start_time = time.time()\n",
    "\n",
    "while True:\n",
    "    desc = sagemaker_client.describe_inference_component(\n",
    "        InferenceComponentName=inference_component_name\n",
    "    )\n",
    "    status = desc[\"InferenceComponentStatus\"]\n",
    "    print(status)\n",
    "    sys.stdout.flush()\n",
    "    if status in [\"InService\", \"Failed\"]:\n",
    "        break\n",
    "    time.sleep(30)\n",
    "\n",
    "total_time = time.time() - start_time\n",
    "print(f\"\\nTotal time taken: {total_time:.2f} seconds ({total_time/60:.2f} minutes)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fb559a20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ§ª Testing inference component...\n",
      "   Queries: 1\n",
      "   Documents: 1\n",
      "âœ… Success! Response time: 0.49s\n",
      "   Embedding dimension: 768\n",
      "\n",
      "Query embeddings shape: (1, 768)\n",
      "Document embeddings shape: (1, 768)\n",
      "\n",
      "Cosine similarity:\n",
      "   Pair 1: 0.8666\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ì‚¬ìš© ì˜ˆì‹œ 1: ë‹¨ì¼ ì¿¼ë¦¬-ë¬¸ì„œ ìŒ\n",
    "payload1 = {\n",
    "    \"queries\": [\"ë§›ìˆëŠ” í•œêµ­ ì „í†µ ìŒì‹ ê¹€ì¹˜ì°Œê°œ\"],\n",
    "    \"documents\": [\"ê¹€ì¹˜ì°Œê°œì™€ ëœì¥ì°Œê°œëŠ” í•œêµ­ì˜ ëŒ€í‘œ ì „í†µ ìŒì‹ì…ë‹ˆë‹¤.\"]\n",
    "}\n",
    "  \n",
    "test_inference_component(endpoint_name, inference_component_name, payload1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-27",
   "metadata": {},
   "source": [
    "## 8. EventBridge Schedulerë¥¼ ì‚¬ìš©í•œ Scale-to-Zero ìŠ¤ì¼€ì¤„ë§\n",
    "\n",
    "### ì‚¬ì „ ìš”êµ¬ ì‚¬í•­: \n",
    "- Role ì— ì•„ë˜ì™€ ê°™ì€ ì •ì±…ì´ ì¶”ê°€ ë˜ì–´ ìˆì–´ì•¼ í•©ë‹ˆë‹¤.\n",
    "    - AmazonEC2ContainerRegistryFullAccess\n",
    "    - AmazonEventBridgeFullAccess\n",
    "    - AmazonS3FullAccess\n",
    "    - AmazonSageMakerFullAccess\n",
    "\n",
    "### ë°©ë²• 1: UpdateInferenceComponentRuntimeConfig API ì‚¬ìš©\n",
    "\n",
    "Inference Componentì˜ CopyCountë¥¼ 0ìœ¼ë¡œ ì„¤ì •í•˜ì—¬ ìŠ¤ì¼€ì¼ ë‹¤ìš´í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-28",
   "metadata": {},
   "source": [
    "### Weekend Scale-in (ê¸ˆìš”ì¼ ì €ë…)\n",
    "\n",
    "ë§¤ì£¼ ê¸ˆìš”ì¼ 18:00 UTC+1ì— CopyCountë¥¼ 0ìœ¼ë¡œ ì„¤ì •í•˜ëŠ” ìŠ¤ì¼€ì¤„ì„ ìƒì„±í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ee1025a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'arn:aws:iam::057716757052:role/gonsoomoon-sm-inference'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "role"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cell-29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Scale-in schedule created: roberta-dual-encoder-1760700181-8454-scale-to-zero-schedule\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import json\n",
    "\n",
    "scheduler = boto3.client('scheduler')\n",
    "\n",
    "flex_window = {\"Mode\": \"OFF\"}\n",
    "\n",
    "# Scale-in ìŠ¤ì¼€ì¤„ íƒ€ê²Ÿ ì„¤ì •\n",
    "scale_in_target = {\n",
    "    \"RoleArn\": role,\n",
    "    \"Arn\": \"arn:aws:scheduler:::aws-sdk:sagemaker:updateInferenceComponentRuntimeConfig\",\n",
    "    \"Input\": json.dumps({\n",
    "        \"DesiredRuntimeConfig\": {\"CopyCount\": 0},\n",
    "        \"InferenceComponentName\": inference_component_name\n",
    "    })\n",
    "}\n",
    "\n",
    "# ë§¤ì£¼ ê¸ˆìš”ì¼ 18:00 UTC+9 (í•œêµ­ ì‹œê°„)ì— ìŠ¤ì¼€ì¼ ë‹¤ìš´\n",
    "update_IC_scale_in_schedule = f\"{prefix}-scale-to-zero-schedule\"\n",
    "\n",
    "try:\n",
    "    scheduler.create_schedule(\n",
    "        Name=update_IC_scale_in_schedule,\n",
    "        ScheduleExpression=\"cron(00 18 ? * 6 *)\",  # ê¸ˆìš”ì¼ 18:00\n",
    "        ScheduleExpressionTimezone=\"Asia/Seoul\",  # í•œêµ­ ì‹œê°„ëŒ€\n",
    "        Target=scale_in_target,\n",
    "        FlexibleTimeWindow=flex_window,\n",
    "        ActionAfterCompletion=\"NONE\",  # ê³„ì† ìœ ì§€\n",
    "    )\n",
    "    print(f\"âœ… Scale-in schedule created: {update_IC_scale_in_schedule}\")\n",
    "except scheduler.exceptions.ConflictException:\n",
    "    print(f\"Schedule {update_IC_scale_in_schedule} already exists\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-30",
   "metadata": {},
   "source": [
    "### Workweek Scale-out (ì›”ìš”ì¼ ì•„ì¹¨)\n",
    "\n",
    "ë§¤ì£¼ ì›”ìš”ì¼ 07:00 UTC+9ì— CopyCountë¥¼ 1ë¡œ ë³µì›í•˜ëŠ” ìŠ¤ì¼€ì¤„ì„ ìƒì„±í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cell-31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Scale-out schedule created: roberta-dual-encoder-1760700181-8454-scale-out-schedule\n"
     ]
    }
   ],
   "source": [
    "# Scale-out ìŠ¤ì¼€ì¤„ íƒ€ê²Ÿ ì„¤ì •\n",
    "scale_out_target = {\n",
    "    \"RoleArn\": role,\n",
    "    \"Arn\": \"arn:aws:scheduler:::aws-sdk:sagemaker:updateInferenceComponentRuntimeConfig\",\n",
    "    \"Input\": json.dumps({\n",
    "        \"DesiredRuntimeConfig\": {\"CopyCount\": 1},\n",
    "        \"InferenceComponentName\": inference_component_name\n",
    "    })\n",
    "}\n",
    "\n",
    "# ë§¤ì£¼ ì›”ìš”ì¼ 07:00 UTC+9 (í•œêµ­ ì‹œê°„)ì— ìŠ¤ì¼€ì¼ ì—…\n",
    "update_IC_scale_out_schedule = f\"{prefix}-scale-out-schedule\"\n",
    "\n",
    "try:\n",
    "    scheduler.create_schedule(\n",
    "        Name=update_IC_scale_out_schedule,\n",
    "        ScheduleExpression=\"cron(00 07 ? * 2 *)\",  # ì›”ìš”ì¼ 07:00\n",
    "        ScheduleExpressionTimezone=\"Asia/Seoul\",  # í•œêµ­ ì‹œê°„ëŒ€\n",
    "        Target=scale_out_target,\n",
    "        FlexibleTimeWindow=flex_window,\n",
    "        ActionAfterCompletion=\"NONE\",  # ê³„ì† ìœ ì§€\n",
    "    )\n",
    "    print(f\"âœ… Scale-out schedule created: {update_IC_scale_out_schedule}\")\n",
    "except scheduler.exceptions.ConflictException:\n",
    "    print(f\"Schedule {update_IC_scale_out_schedule} already exists\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-32",
   "metadata": {},
   "source": [
    "### ìƒì„±ëœ ìŠ¤ì¼€ì¤„ í™•ì¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "cell-33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“… Schedule: roberta-dual-encoder-1760700181-8454-scale-to-zero-schedule\n",
      "   Expression: cron(00 18 ? * 6 *)\n",
      "   Timezone: Asia/Seoul\n",
      "   State: ENABLED\n",
      "\n",
      "ğŸ“… Schedule: roberta-dual-encoder-1760700181-8454-scale-out-schedule\n",
      "   Expression: cron(00 07 ? * 2 *)\n",
      "   Timezone: Asia/Seoul\n",
      "   State: ENABLED\n"
     ]
    }
   ],
   "source": [
    "# ìƒì„±ëœ ìŠ¤ì¼€ì¤„ ëª©ë¡ í™•ì¸\n",
    "try:\n",
    "    schedules_to_check = [update_IC_scale_in_schedule, update_IC_scale_out_schedule]\n",
    "    \n",
    "    for schedule_name in schedules_to_check:\n",
    "        try:\n",
    "            schedule = scheduler.get_schedule(Name=schedule_name)\n",
    "            print(f\"\\nğŸ“… Schedule: {schedule_name}\")\n",
    "            print(f\"   Expression: {schedule['ScheduleExpression']}\")\n",
    "            print(f\"   Timezone: {schedule['ScheduleExpressionTimezone']}\")\n",
    "            print(f\"   State: {schedule['State']}\")\n",
    "        except scheduler.exceptions.ResourceNotFoundException:\n",
    "            print(f\"\\nâŒ Schedule not found: {schedule_name}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error checking schedules: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-37",
   "metadata": {},
   "source": [
    "## 9. ë¦¬ì†ŒìŠ¤ ì •ë¦¬\n",
    "\n",
    "í…ŒìŠ¤íŠ¸ê°€ ì™„ë£Œë˜ë©´ ë¦¬ì†ŒìŠ¤ë¥¼ ì •ë¦¬í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-38",
   "metadata": {},
   "source": [
    "### ìŠ¤ì¼€ì¤„ ì‚­ì œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e971638b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Deleted schedule: roberta-dual-encoder-1760700181-8454-scale-to-zero-schedule\n",
      "âœ… Deleted schedule: roberta-dual-encoder-1760700181-8454-scale-out-schedule\n"
     ]
    }
   ],
   "source": [
    "# ìƒì„±ëœ ìŠ¤ì¼€ì¤„ ì‚­ì œ\n",
    "schedules = [update_IC_scale_in_schedule, update_IC_scale_out_schedule]\n",
    "\n",
    "for schedule in schedules:\n",
    "    try:\n",
    "        scheduler.delete_schedule(Name=schedule)\n",
    "        print(f\"âœ… Deleted schedule: {schedule}\")\n",
    "    except scheduler.exceptions.ResourceNotFoundException:\n",
    "        print(f\"Schedule {schedule} not found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-40",
   "metadata": {},
   "source": [
    "### Inference Component ë° ì—”ë“œí¬ì¸íŠ¸ ë“±ì˜ ë¦¬ì†ŒìŠ¤ ì‚­ì œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "db25e9f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference Component ì‚­ì œ ì¤‘...\n",
      "âœ… Inference Component ì‚­ì œ ì‹œì‘: roberta-dual-encoder-1760700181-8454-inference-component\n",
      "Inference Component ì‚­ì œ ëŒ€ê¸° ì¤‘...\n",
      "   Status: Deleting\n",
      "   Status: Deleting\n",
      "   Status: Deleting\n",
      "âœ… Inference Component ì‚­ì œ ì™„ë£Œ\n",
      "\n",
      "Endpoint ì‚­ì œ ì¤‘...\n",
      "âœ… Endpoint ì‚­ì œ ì‹œì‘: roberta-dual-encoder-1760700181-8454-scale-to-zero-endpoint\n",
      "   Endpoint Status: Deleting\n",
      "âœ… Endpoint ì‚­ì œ ì™„ë£Œ\n",
      "\n",
      "Endpoint Config ì‚­ì œ ì¤‘...\n",
      "âœ… Endpoint Config ì‚­ì œ ì™„ë£Œ: roberta-dual-encoder-1760700181-8454-scale-to-zero-config\n",
      "\n",
      "Model ì‚­ì œ ì¤‘...\n",
      "âœ… Model ì‚­ì œ ì™„ë£Œ: roberta-dual-encoder-1760700181-8454-model\n",
      "\n",
      "ğŸ‰ ëª¨ë“  ë¦¬ì†ŒìŠ¤ ì‚­ì œ ì™„ë£Œ!\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# Inference Component ì‚­ì œ\n",
    "print(\"Inference Component ì‚­ì œ ì¤‘...\")\n",
    "try:\n",
    "    sagemaker_client.describe_inference_component(\n",
    "        InferenceComponentName=inference_component_name\n",
    "    )\n",
    "    sagemaker_client.delete_inference_component(\n",
    "        InferenceComponentName=inference_component_name\n",
    "    )\n",
    "    print(f\"âœ… Inference Component ì‚­ì œ ì‹œì‘: {inference_component_name}\")\n",
    "except sagemaker_client.exceptions.ClientError as e:\n",
    "    if \"Could not find inference component\" in str(e):\n",
    "        print(f\"â„¹ï¸ Inference Componentê°€ ì´ë¯¸ ì‚­ì œë˜ì—ˆê±°ë‚˜ ì¡´ì¬í•˜ì§€ ì•ŠìŒ: {inference_component_name}\")\n",
    "    else:\n",
    "        raise\n",
    "\n",
    "# Inference Componentê°€ ì‚­ì œë  ë•Œê¹Œì§€ ëŒ€ê¸°\n",
    "print(\"Inference Component ì‚­ì œ ëŒ€ê¸° ì¤‘...\")\n",
    "max_wait_time = 300  # 5ë¶„ íƒ€ì„ì•„ì›ƒ\n",
    "start_time = time.time()\n",
    "\n",
    "while True:\n",
    "    try:\n",
    "        if time.time() - start_time > max_wait_time:\n",
    "            print(\"âš ï¸ Timeout: Inference Component ì‚­ì œ ëŒ€ê¸° ì‹œê°„ ì´ˆê³¼\")\n",
    "            break\n",
    "\n",
    "        desc = sagemaker_client.describe_inference_component(\n",
    "            InferenceComponentName=inference_component_name\n",
    "        )\n",
    "        status = desc.get('InferenceComponentStatus', 'Unknown')\n",
    "        print(f\"   Status: {status}\")\n",
    "        time.sleep(10)\n",
    "\n",
    "    except sagemaker_client.exceptions.ClientError as e:\n",
    "        if \"Could not find inference component\" in str(e):\n",
    "            print(\"âœ… Inference Component ì‚­ì œ ì™„ë£Œ\")\n",
    "            break\n",
    "        else:\n",
    "            print(f\"âš ï¸ Unexpected error: {str(e)}\")\n",
    "            break\n",
    "\n",
    "# Endpoint ì‚­ì œ\n",
    "print(\"\\nEndpoint ì‚­ì œ ì¤‘...\")\n",
    "try:\n",
    "    sagemaker_client.describe_endpoint(EndpointName=endpoint_name)\n",
    "    sagemaker_client.delete_endpoint(EndpointName=endpoint_name)\n",
    "    print(f\"âœ… Endpoint ì‚­ì œ ì‹œì‘: {endpoint_name}\")\n",
    "\n",
    "    # Endpoint ì‚­ì œ ëŒ€ê¸°\n",
    "    max_wait_time = 300\n",
    "    start_time = time.time()\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            if time.time() - start_time > max_wait_time:\n",
    "                print(\"âš ï¸ Timeout: Endpoint ì‚­ì œ ëŒ€ê¸° ì‹œê°„ ì´ˆê³¼\")\n",
    "                break\n",
    "\n",
    "            desc = sagemaker_client.describe_endpoint(EndpointName=endpoint_name)\n",
    "            status = desc.get('EndpointStatus', 'Unknown')\n",
    "            print(f\"   Endpoint Status: {status}\")\n",
    "            time.sleep(10)\n",
    "\n",
    "        except sagemaker_client.exceptions.ClientError as e:\n",
    "            if \"Could not find endpoint\" in str(e):\n",
    "                print(\"âœ… Endpoint ì‚­ì œ ì™„ë£Œ\")\n",
    "                break\n",
    "            else:\n",
    "                print(f\"âš ï¸ Unexpected error: {str(e)}\")\n",
    "                break\n",
    "\n",
    "except sagemaker_client.exceptions.ClientError as e:\n",
    "    if \"Could not find endpoint\" in str(e):\n",
    "        print(f\"â„¹ï¸ Endpointê°€ ì´ë¯¸ ì‚­ì œë˜ì—ˆê±°ë‚˜ ì¡´ì¬í•˜ì§€ ì•ŠìŒ: {endpoint_name}\")\n",
    "    else:\n",
    "        raise\n",
    "\n",
    "# Endpoint Config ì‚­ì œ\n",
    "print(\"\\nEndpoint Config ì‚­ì œ ì¤‘...\")\n",
    "try:\n",
    "    sagemaker_client.describe_endpoint_config(EndpointConfigName=endpoint_config_name)\n",
    "    sagemaker_client.delete_endpoint_config(EndpointConfigName=endpoint_config_name)\n",
    "    print(f\"âœ… Endpoint Config ì‚­ì œ ì™„ë£Œ: {endpoint_config_name}\")\n",
    "except sagemaker_client.exceptions.ClientError as e:\n",
    "    if \"Could not find endpoint configuration\" in str(e):\n",
    "        print(f\"â„¹ï¸ Endpoint Configê°€ ì´ë¯¸ ì‚­ì œë˜ì—ˆê±°ë‚˜ ì¡´ì¬í•˜ì§€ ì•ŠìŒ: {endpoint_config_name}\")\n",
    "    else:\n",
    "        raise\n",
    "\n",
    "# Model ì‚­ì œ\n",
    "print(\"\\nModel ì‚­ì œ ì¤‘...\")\n",
    "try:\n",
    "    sagemaker_client.describe_model(ModelName=model_name)\n",
    "    sagemaker_client.delete_model(ModelName=model_name)\n",
    "    print(f\"âœ… Model ì‚­ì œ ì™„ë£Œ: {model_name}\")\n",
    "except sagemaker_client.exceptions.ClientError as e:\n",
    "    if \"Could not find model\" in str(e):\n",
    "        print(f\"â„¹ï¸ Modelì´ ì´ë¯¸ ì‚­ì œë˜ì—ˆê±°ë‚˜ ì¡´ì¬í•˜ì§€ ì•ŠìŒ: {model_name}\")\n",
    "    else:\n",
    "        raise\n",
    "\n",
    "print(\"\\nğŸ‰ ëª¨ë“  ë¦¬ì†ŒìŠ¤ ì‚­ì œ ì™„ë£Œ!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-44",
   "metadata": {},
   "source": [
    "## ìš”ì•½\n",
    "\n",
    "ì´ ë…¸íŠ¸ë¶ì—ì„œëŠ” ë‹¤ìŒì„ êµ¬í˜„í–ˆìŠµë‹ˆë‹¤:\n",
    "\n",
    "1. **RoBERTa Dual Encoder ëª¨ë¸**ì„ Inference Componentë¡œ ë°°í¬\n",
    "2. **ManagedInstanceScaling**ì„ í™œì„±í™”í•˜ì—¬ MinInstanceCount=0 ì„¤ì •\n",
    "3. **EventBridge Scheduler**ë¥¼ ì‚¬ìš©í•˜ì—¬:\n",
    "   - ê¸ˆìš”ì¼ 18:00ì— ìë™ìœ¼ë¡œ CopyCountë¥¼ 0ìœ¼ë¡œ ì„¤ì • (Scale-in)\n",
    "   - ì›”ìš”ì¼ 07:00ì— ìë™ìœ¼ë¡œ CopyCountë¥¼ 1ë¡œ ë³µì› (Scale-out)\n",
    "4. **ë¹„ìš© ì ˆê°**: ì£¼ë§ ë™ì•ˆ ì¸ìŠ¤í„´ìŠ¤ê°€ 0ê°œë¡œ ì¤„ì–´ë“¤ì–´ ì»´í“¨íŒ… ë¹„ìš© ì ˆê°\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58c669c9",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
