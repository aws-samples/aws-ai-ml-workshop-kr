{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "88feef7d-68ef-47f3-9446-e7d3bab80db3",
   "metadata": {},
   "source": [
    "# Lab 1: Korean QnA (Question Answering) Training on AWS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a562ce3-e032-489c-a5cc-03d9904e053f",
   "metadata": {},
   "source": [
    "\n",
    "## Introduction\n",
    "---\n",
    "\n",
    "ë³¸ ëª¨ë“ˆì—ì„œëŠ” í—ˆê¹…í˜ì´ìŠ¤ íŠ¸ëœìŠ¤í¬ë¨¸(Hugging Face transformers) ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì‚¬ìš©í•˜ì—¬ í•œêµ­ì–´ ì§ˆì˜ì‘ë‹µ(Korean QnA; Question Answering) ìŒì„ í›ˆë ¨í•©ë‹ˆë‹¤. ì§ˆì˜ì‘ë‹µì€ ë³¸ë¬¸ì—ì„œ íŠ¹ì • ì§ˆì˜ì— ëŒ€í•œ ë‹µë³€ì„ ì°¾ëŠ” ë‹¤ìš´ìŠ¤íŠ¸ë¦¼ íƒœìŠ¤í¬ì…ë‹ˆë‹¤. ë°ì´í„°ëŠ” ë³¸ë¬¸(context)/ì§ˆì˜(question) ìŒì˜ í˜•íƒœë¡œ ì œê³µë˜ë©°, ì •ë‹µì€ ë³¸ë¬¸ ë‚´ì— í¬í•¨ëœ ì •ë‹µ(answer) ë¬¸êµ¬ì˜ ì‹œì‘ê³¼ ëì„ ìƒ‰ì¸í™”í•©ë‹ˆë‹¤. \n",
    "\n",
    "***[Note] SageMaker Studio Lab, SageMaker Studio, SageMaker ë…¸íŠ¸ë¶ ì¸ìŠ¤í„´ìŠ¤, ë˜ëŠ” ì—¬ëŸ¬ë¶„ì˜ ë¡œì»¬ ë¨¸ì‹ ì—ì„œ ì´ ë°ëª¨ë¥¼ ì‹¤í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. SageMaker Studio Labì„ ì‚¬ìš©í•˜ëŠ” ê²½ìš° GPUë¥¼ í™œì„±í™”í•˜ì„¸ìš”.***\n",
    "\n",
    "***[ì£¼ì˜] ë³¸ ë°ì´í„°ì…‹ì€ ìƒì—…ì ì¸ ëª©ì ìœ¼ë¡œ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë³¸ í•¸ì¦ˆì˜¨ì€ ì—°êµ¬/ì°¸ê³ ìš©ìœ¼ë¡œë§Œ í™œìš©í•˜ì‹œê³ , ëª¨ë¸ í›ˆë ¨ì€ ì—¬ëŸ¬ë¶„ë§Œì˜ ë°ì´í„°ì…‹ì„ ì§ì ‘ ìƒì„±í•˜ì…”ì•¼ í•©ë‹ˆë‹¤.***\n",
    "\n",
    "### References\n",
    "- Hugging Face Tutorial: https://huggingface.co/docs/transformers/tasks/question_answering\n",
    "- Fine-tuning with custom datasets: https://huggingface.co/transformers/v4.11.3/custom_datasets.html#question-answering-with-squad-2-0\n",
    "- KorQuAD 1.0: https://korquad.github.io/KorQuad%201.0/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fecc4a9e-0435-4356-8b4c-e34f7afdfe39",
   "metadata": {},
   "source": [
    "\n",
    "## 1. Setup Environments\n",
    "---\n",
    "\n",
    "### Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3a700dbe-e93a-4356-b6a1-f84a5081411b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import logging\n",
    "import argparse\n",
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import (\n",
    "    BertTokenizer, BertTokenizerFast, BertConfig, BertForTokenClassification, \n",
    "    Trainer, TrainingArguments, set_seed\n",
    ")\n",
    "from transformers.trainer_utils import get_last_checkpoint\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO, \n",
    "    format='[{%(filename)s:%(lineno)d} %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.StreamHandler(sys.stdout)\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce03ca7f-41b6-4479-b4bc-965069f35b2d",
   "metadata": {},
   "source": [
    "### Argument parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dbe51e4c-095f-4c1e-9d76-745febcd35c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parser_args(train_notebook=False):\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    # Default Setting\n",
    "    parser.add_argument(\"--epochs\", type=int, default=3)\n",
    "    parser.add_argument(\"--seed\", type=int, default=42)\n",
    "    parser.add_argument(\"--train_batch_size\", type=int, default=32)\n",
    "    parser.add_argument(\"--eval_batch_size\", type=int, default=64)\n",
    "    parser.add_argument(\"--max_length\", type=int, default=384)\n",
    "    parser.add_argument(\"--stride\", type=int, default=64)\n",
    "    parser.add_argument(\"--warmup_steps\", type=int, default=100)\n",
    "    parser.add_argument(\"--logging_steps\", type=int, default=100)\n",
    "    parser.add_argument(\"--learning_rate\", type=str, default=5e-5)\n",
    "    parser.add_argument(\"--disable_tqdm\", type=bool, default=False)\n",
    "    parser.add_argument(\"--fp16\", type=bool, default=True)\n",
    "    parser.add_argument(\"--tokenizer_id\", type=str, default='salti/bert-base-multilingual-cased-finetuned-squad')\n",
    "    parser.add_argument(\"--model_id\", type=str, default='salti/bert-base-multilingual-cased-finetuned-squad')\n",
    "    \n",
    "    # SageMaker Container environment\n",
    "    parser.add_argument(\"--output_data_dir\", type=str, default=os.environ[\"SM_OUTPUT_DATA_DIR\"])\n",
    "    parser.add_argument(\"--model_dir\", type=str, default=os.environ[\"SM_MODEL_DIR\"])\n",
    "    parser.add_argument(\"--n_gpus\", type=str, default=os.environ[\"SM_NUM_GPUS\"])\n",
    "    parser.add_argument(\"--train_dir\", type=str, default=os.environ[\"SM_CHANNEL_TRAIN\"])\n",
    "    parser.add_argument(\"--valid_dir\", type=str, default=os.environ[\"SM_CHANNEL_VALID\"])\n",
    "    parser.add_argument('--chkpt_dir', type=str, default='/opt/ml/checkpoints')     \n",
    "\n",
    "    if train_notebook:\n",
    "        args = parser.parse_args([])\n",
    "    else:\n",
    "        args = parser.parse_args()\n",
    "    return args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1b088282-0c75-4e8a-9e17-57949c5e03ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = 'qna_train'\n",
    "valid_dir = 'qna_valid'\n",
    "!rm -rf {train_dir} {valid_dir}\n",
    "os.makedirs(train_dir, exist_ok=True)\n",
    "os.makedirs(valid_dir, exist_ok=True) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76fc19c4-c663-420c-86f9-74d698582309",
   "metadata": {},
   "source": [
    "### Load Arguments\n",
    "\n",
    "ì£¼í”¼í„° ë…¸íŠ¸ë¶ì—ì„œ ê³§ë°”ë¡œ ì‹¤í–‰í•  ìˆ˜ ìˆë„ë¡ ì„¤ì •ê°’ë“¤ì„ ë¡œë“œí•©ë‹ˆë‹¤. ë¬¼ë¡  ë…¸íŠ¸ë¶ í™˜ê²½ì´ ì•„ë‹Œ ì»¤ë§¨ë“œë¼ì¸ì—ì„œë„ `cd scripts & python3 train.py` ì»¤ë§¨ë“œë¡œ í›ˆë ¨ ìŠ¤í¬ë¦½íŠ¸ë¥¼ ì‹¤í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "51d77fcd-9a07-4343-b74f-917e18673657",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{204499775.py:21} INFO - ***** Arguments *****\n",
      "[{204499775.py:22} INFO - epochs=3\n",
      "seed=42\n",
      "train_batch_size=32\n",
      "eval_batch_size=64\n",
      "max_length=384\n",
      "stride=64\n",
      "warmup_steps=100\n",
      "logging_steps=100\n",
      "learning_rate=5e-05\n",
      "disable_tqdm=False\n",
      "fp16=True\n",
      "tokenizer_id=salti/bert-base-multilingual-cased-finetuned-squad\n",
      "model_id=salti/bert-base-multilingual-cased-finetuned-squad\n",
      "output_data_dir=/home/ec2-user/SageMaker/sm-kornlp-usecases/question-answering/data\n",
      "model_dir=/home/ec2-user/SageMaker/sm-kornlp-usecases/question-answering/model\n",
      "n_gpus=4\n",
      "train_dir=/home/ec2-user/SageMaker/sm-kornlp-usecases/question-answering/qna_train\n",
      "valid_dir=/home/ec2-user/SageMaker/sm-kornlp-usecases/question-answering/qna_valid\n",
      "chkpt_dir=chkpt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "chkpt_dir = 'chkpt'\n",
    "model_dir = 'model'\n",
    "output_data_dir = 'data'\n",
    "num_gpus = torch.cuda.device_count()\n",
    "\n",
    "!rm -rf {chkpt_dir} {model_dir} {output_data_dir} \n",
    "\n",
    "if os.environ.get('SM_CURRENT_HOST') is None:\n",
    "    is_sm_container = False\n",
    "\n",
    "    #src_dir = '/'.join(os.getcwd().split('/')[:-1])\n",
    "    src_dir = os.getcwd()\n",
    "    os.environ['SM_MODEL_DIR'] = f'{src_dir}/{model_dir}'\n",
    "    os.environ['SM_OUTPUT_DATA_DIR'] = f'{src_dir}/{output_data_dir}'\n",
    "    os.environ['SM_NUM_GPUS'] = str(num_gpus)\n",
    "    os.environ['SM_CHANNEL_TRAIN'] = f'{src_dir}/{train_dir}'\n",
    "    os.environ['SM_CHANNEL_VALID'] = f'{src_dir}/{valid_dir}'\n",
    "\n",
    "args = parser_args(train_notebook=True) \n",
    "args.chkpt_dir = chkpt_dir\n",
    "logger.info(\"***** Arguments *****\")\n",
    "logger.info(''.join(f'{k}={v}\\n' for k, v in vars(args).items()))\n",
    "\n",
    "os.makedirs(args.chkpt_dir, exist_ok=True) \n",
    "os.makedirs(args.model_dir, exist_ok=True)\n",
    "os.makedirs(args.output_data_dir, exist_ok=True) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4fcc91c-8c1f-4cae-a63a-791791600e00",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## 2. Preparation\n",
    "---\n",
    "\n",
    "### Dataset\n",
    "\n",
    "ë³¸ í•¸ì¦ˆì˜¨ì—ì„œ ì‚¬ìš©í•  ë°ì´í„°ì…‹ì€ KorQuAD (The Korean Question Answering Dataset) 1.0 ì…ë‹ˆë‹¤. 1,560ê°œì˜ í•œêµ­ì–´ ìœ„í‚¤í”¼ë””ì•„ ê¸€ë“¤ì— ëŒ€í•´ 66,181ê°œì˜ ì§ˆì˜ì‘ë‹µ ìŒ(60,407ê°œ í›ˆë ¨ ë°ì´í„°, 5,774ê°œ ê²€ì¦ ë°ì´í„°)ìœ¼ë¡œ êµ¬ì„±ëœ ë°ì´í„°ì…‹ìœ¼ë¡œ SQuAD (Stanford Question Answering Dataset) 1.0ê³¼ ë™ì¼í•œ í¬ë§·ì…ë‹ˆë‹¤.\n",
    "- KorQuad 1.0: https://korquad.github.io/KorQuad%201.0/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cd57c85b-a8ec-4ade-bd9b-7137ca37ab4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2022-07-06 04:57:46--  https://korquad.github.io/dataset/KorQuAD_v1.0_train.json\n",
      "Resolving korquad.github.io (korquad.github.io)... 185.199.108.153, 185.199.110.153, 185.199.109.153, ...\n",
      "Connecting to korquad.github.io (korquad.github.io)|185.199.108.153|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 38527475 (37M) [application/json]\n",
      "Saving to: â€˜qna_train/KorQuAD_v1.0_train.jsonâ€™\n",
      "\n",
      "100%[======================================>] 38,527,475  --.-K/s   in 0.1s    \n",
      "\n",
      "2022-07-06 04:57:46 (269 MB/s) - â€˜qna_train/KorQuAD_v1.0_train.jsonâ€™ saved [38527475/38527475]\n",
      "\n",
      "--2022-07-06 04:57:46--  https://korquad.github.io/dataset/KorQuAD_v1.0_dev.json\n",
      "Resolving korquad.github.io (korquad.github.io)... 185.199.111.153, 185.199.108.153, 185.199.110.153, ...\n",
      "Connecting to korquad.github.io (korquad.github.io)|185.199.111.153|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 3881058 (3.7M) [application/json]\n",
      "Saving to: â€˜qna_valid/KorQuAD_v1.0_dev.jsonâ€™\n",
      "\n",
      "100%[======================================>] 3,881,058   --.-K/s   in 0.02s   \n",
      "\n",
      "2022-07-06 04:57:46 (219 MB/s) - â€˜qna_valid/KorQuAD_v1.0_dev.jsonâ€™ saved [3881058/3881058]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://korquad.github.io/dataset/KorQuAD_v1.0_train.json -O {train_dir}/KorQuAD_v1.0_train.json\n",
    "!wget https://korquad.github.io/dataset/KorQuAD_v1.0_dev.json -O {valid_dir}/KorQuAD_v1.0_dev.json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00dd7ce1-72ee-40cf-8487-c487c39c33c6",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## 3. Construct Feature set\n",
    "---\n",
    "\n",
    "ì§ˆì˜ì‘ë‹µìŒì— ëŒ€í•œ ëª¨ë¸ì„ í›ˆë ¨í•˜ë ¤ë©´ í† í°í™”ëœ ë³¸ë¬¸/ì§ˆë¬¸ ìŒê³¼ ë‹µë³€ ë²”ìœ„ë¥¼ ì•Œ ìˆ˜ ìˆëŠ” í† í° ì¸ë±ìŠ¤ê°€ í•„ìš”í•©ë‹ˆë‹¤. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "346510d8-311b-4ca2-a876-d81d13249458",
   "metadata": {},
   "source": [
    "### Load raw data\n",
    "\n",
    "ì›ë³¸ ë°ì´í„°ë¡œë¶€í„° ë³¸ë¬¸ ë° ì§ˆì˜ì‘ë‹µ ì •ë³´ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "733b628e-e9ef-490e-8ef1-7ccd31a0ae6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "def read_squad(path):\n",
    "    path = Path(path)\n",
    "    with open(path, 'rb') as f:\n",
    "        squad_dict = json.load(f)\n",
    "\n",
    "    contexts = []\n",
    "    questions = []\n",
    "    answers = []\n",
    "    for group in squad_dict['data']:\n",
    "        for passage in group['paragraphs']:\n",
    "            context = passage['context']\n",
    "            for qa in passage['qas']:\n",
    "                question = qa['question']\n",
    "                for answer in qa['answers']:\n",
    "                    contexts.append(context)\n",
    "                    questions.append(question)\n",
    "                    answers.append(answer)\n",
    "\n",
    "    return contexts, questions, answers\n",
    "\n",
    "train_contexts, train_questions, train_answers = read_squad(f'{train_dir}/KorQuAD_v1.0_train.json')\n",
    "val_contexts, val_questions, val_answers = read_squad(f'{valid_dir}/KorQuAD_v1.0_dev.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0eb36a9-7c8d-4319-8e8b-7d8758e83bcd",
   "metadata": {},
   "source": [
    "### Add End index\n",
    "\n",
    "ë¬¸ì¥ì—ì„œ ë‹µë³€ì´ ëë‚˜ëŠ” ë¬¸ì ìœ„ì¹˜ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤. ë•Œë¡œ, ë‹µë³€ì´ í•œë‘ ê¸€ì§œì”© ì°¨ì´ê°€ ë‚˜ëŠ” ê²½ìš°ê°€ ìˆìœ¼ë¯€ë¡œ ì´ì— ëŒ€í•œ ì˜ˆì™¸ì²˜ë¦¬ë¥¼ ì¶”ê°€í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "73a65ac2-6234-4cf4-9a77-550ceb94d04b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_end_idx(answers, contexts):\n",
    "    for answer, context in zip(answers, contexts):\n",
    "        gold_text = answer['text']\n",
    "        start_idx = answer['answer_start']\n",
    "        end_idx = start_idx + len(gold_text)\n",
    "\n",
    "        # sometimes squad answers are off by a character or two â€“ fix this\n",
    "        if context[start_idx:end_idx] == gold_text:\n",
    "            answer['answer_end'] = end_idx\n",
    "        elif context[start_idx-1:end_idx-1] == gold_text:\n",
    "            answer['answer_start'] = start_idx - 1\n",
    "            answer['answer_end'] = end_idx - 1     # When the gold label is off by one character\n",
    "        elif context[start_idx-2:end_idx-2] == gold_text:\n",
    "            answer['answer_start'] = start_idx - 2\n",
    "            answer['answer_end'] = end_idx - 2     # When the gold label is off by two characters\n",
    "    return answers\n",
    "\n",
    "train_answers = add_end_idx(train_answers, train_contexts)\n",
    "val_answers = add_end_idx(val_answers, val_contexts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ffaa50c-3f86-45ae-89a7-edb17b9e2459",
   "metadata": {},
   "source": [
    "### Tokenization \n",
    "\n",
    "ì—°ì–´ ì²˜ë¦¬ ëª¨ë¸ì„ í›ˆë ¨í•˜ë ¤ë©´, í† í°í™”(Tokenization)ë¥¼ í†µí•´ ë§ë­‰ì¹˜(corpus; ìì—°ì–´ ì²˜ë¦¬ë¥¼ ìœ„í•œ ëŒ€ëŸ‰ì˜ í…ìŠ¤íŠ¸ ë°ì´í„°)ë¥¼ í† í° ì‹œí€€ìŠ¤ë¡œ ë‚˜ëˆ„ëŠ” ê³¼ì •ì´ í•„ìš”í•©ë‹ˆë‹¤. BERT ì´ì „ì˜ ìì—°ì–´ ì²˜ë¦¬ ëª¨ë¸ì€ ì£¼ë¡œ ë„ë©”ì¸ ì „ë¬¸ê°€ë“¤ì´ ì§ì ‘ í† í°í™”í•´ë†“ì€ í† í¬ì•„ë‹ˆì €(Mecab, Kkma ë“±)ë“¤ì„ ì‚¬ìš©í–ˆì§€ë§Œ, BERTë¥¼ í›ˆë ¨í•˜ê¸° ìœ„í•œ í† í¬ë‚˜ì´ì €ëŠ” ë„ë©”ì¸ ì§€ì‹ í•„ìš” ì—†ì´ ë§ë­‰ì¹˜ì—ì„œ ìì£¼ ë“±ì¥í•˜ëŠ” ì„œë¸Œì›Œë“œ(subword)ë¥¼ í† í°í™”í•©ë‹ˆë‹¤. GPT ê¸°ë°˜ ëª¨ë¸ì€ BPE(Byte-pair Encoding)ë¼ëŠ” í†µê³„ì  ê¸°ë²•ì„ ì‚¬ìš©í•˜ë©°, BERT ë° ELECTRA ê¸°ë°˜ ëª¨ë¸ì€ BPEì™€ ìœ ì‚¬í•œ Wordpieceë¥¼ í† í¬ë‚˜ì´ì €ë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "26b2d33e-ff1f-42a5-bc0a-bb7117a3f9a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, BertTokenizerFast\n",
    "#tokenizer = AutoTokenizer.from_pretrained(args.tokenizer_id)\n",
    "tokenizer = BertTokenizerFast.from_pretrained(args.tokenizer_id)\n",
    "\n",
    "train_encodings = tokenizer(\n",
    "    train_contexts, \n",
    "    train_questions, \n",
    "    truncation=True, \n",
    "    max_length=args.max_length,\n",
    "    stride=args.stride, \n",
    "    padding=\"max_length\"\n",
    ")\n",
    "val_encodings = tokenizer(\n",
    "    val_contexts, \n",
    "    val_questions, \n",
    "    truncation=True, \n",
    "    max_length=args.max_length,\n",
    "    stride=args.stride, \n",
    "    padding=\"max_length\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aea9ecd3-01fd-44be-be3b-27810b29f95c",
   "metadata": {},
   "source": [
    "### Covert to token positions for answers\n",
    "\n",
    "ë‹µë³€ì˜ ì‹œì‘/ë ì¸ë±ìŠ¤ë¥¼ í† í° ì‹œì‘/ë ì¸ë±ìŠ¤ë¡œ ë³€í™˜í•©ë‹ˆë‹¤. ì›ë˜ì˜ ë¬¸ìì—´ì—ì„œ í† í° ì¸ë±ìŠ¤ë¥¼ ë°˜í™˜í•˜ëŠ” `char_to_token()` ë©”ì†Œë“œë¡œ í¸í•˜ê²Œ ìˆ˜í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ee29c4b6-eba7-4440-b66c-9e560e0bc1a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_token_positions(encodings, answers, tokenizer):\n",
    "    start_positions = []\n",
    "    end_positions = []\n",
    "    for i in range(len(answers)):\n",
    "        start_positions.append(encodings.char_to_token(i, answers[i]['answer_start']))\n",
    "        end_positions.append(encodings.char_to_token(i, answers[i]['answer_end'] - 1))\n",
    "        # if None, the answer passage has been truncated\n",
    "        if start_positions[-1] is None:\n",
    "            start_positions[-1] = tokenizer.model_max_length\n",
    "        if end_positions[-1] is None:\n",
    "            end_positions[-1] = tokenizer.model_max_length\n",
    "            \n",
    "        # If the start and end positions are greater than max_length, both must be changed to max_length.\n",
    "        if start_positions[-1] is None or start_positions[-1] > tokenizer.model_max_length:\n",
    "            start_positions[-1] = tokenizer.model_max_length\n",
    "        \n",
    "        if end_positions[-1] is None or end_positions[-1] > tokenizer.model_max_length:\n",
    "            end_positions[-1] = tokenizer.model_max_length\n",
    "\n",
    "    encodings.update({'start_positions': start_positions, 'end_positions': end_positions})   \n",
    "    return encodings\n",
    "\n",
    "train_encodings = add_token_positions(train_encodings, train_answers, tokenizer)\n",
    "val_encodings = add_token_positions(val_encodings, val_answers, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e44a8ac-c722-4088-9c1c-cb05366a2167",
   "metadata": {},
   "source": [
    "### Custom Dataset\n",
    "\n",
    "í›ˆë ¨/ê²€ì¦ ì‹œì— ì‚¬ìš©í•  ì»¤ìŠ¤í…€ ë°ì´í„°ì…‹ì„ ìƒì„±í•˜ê¸° ìœ„í•œ í´ë˜ìŠ¤ë¥¼ ìƒì„±í•©ë‹ˆë‹¤. BERT ì§ˆì˜ì‘ë‹µìŒ ëª¨ë¸ì€ ë³´í†µ ì•„ë˜ì˜ ì…ë ¥ê°’ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.\n",
    "- `input_ids`: ë¬¸ì¥ì´ ì¸ë±ìŠ¤(íŠ¹ì • vocabì— ë§¤í•‘í•˜ëŠ” ìˆ«ìê°’)ë¡œ êµ¬ì„±ëœ í† í° ì‹œí€€ìŠ¤ë¡œ ë³€í™˜ëœ ê²°ê´ê°’\n",
    "- `attention_mask` : í•´ë‹¹ í† í°ì´ íŒ¨ë”© í† í°ì¸ì§€, ì•„ë‹Œì§€ë¥¼ ë§ˆìŠ¤í‚¹\n",
    "- `token_type_ids`: ì„¸ê·¸ë¨¼íŠ¸ (ë‘ ë¬¸ì¥ ì…ë ¥ ì‹œ, ì²«ë²ˆì§¸ ë¬¸ì¥ì¸ì§€ ì•„ë‹Œì§€ë¥¼ ë§ˆìŠ¤í‚¹)\n",
    "- `start_positions`: ë‹µë³€ì´ ì‹œì‘í•˜ëŠ” í† í° ìœ„ì¹˜\n",
    "- `end_positions`: ë‹µë³€ì´ ëë‚˜ëŠ” í† í° ìœ„ì¹˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dfca2c12-3b5b-483a-86c7-bbfe462eaf0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SquadDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings):\n",
    "        self.encodings = encodings\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encodings.input_ids)\n",
    "\n",
    "train_dataset = SquadDataset(train_encodings)\n",
    "val_dataset = SquadDataset(val_encodings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fbbc4dc-a38b-4407-b411-2a2870d8561c",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## 4. Training\n",
    "---\n",
    "\n",
    "### Training Preparation\n",
    "\n",
    "ë³¸ í•¸ì¦ˆì˜¨ì€ í—ˆê¹…í˜ì´ìŠ¤ì˜ íŠ¸ëœìŠ¤í¬ë¨¸ ë¼ì´ë¸ŒëŸ¬ë¦¬ì— í¬í•¨ëœ BertForQuestionAnswering ëª¨ë¸ì„ ì‚¬ìš©í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f87dd1c8-0f3f-45dd-b5ab-4ca975778b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    BertForQuestionAnswering,\n",
    "    Trainer, TrainingArguments, set_seed\n",
    ")\n",
    "from transformers.trainer_utils import get_last_checkpoint\n",
    "\n",
    "model = BertForQuestionAnswering.from_pretrained(args.model_id)\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='chkpt',\n",
    "    overwrite_output_dir=True if get_last_checkpoint('chkpt') is not None else False,\n",
    "    num_train_epochs=args.epochs,\n",
    "    per_device_train_batch_size=args.train_batch_size, \n",
    "    per_device_eval_batch_size=args.eval_batch_size, \n",
    "    warmup_steps=args.warmup_steps, \n",
    "    weight_decay=0.01,    \n",
    "    logging_dir=\"logs\", \n",
    "    logging_steps=args.logging_steps,\n",
    "    learning_rate=float(args.learning_rate),\n",
    "    save_total_limit=5,\n",
    "    save_strategy=\"epoch\",\n",
    "    fp16=args.fp16,\n",
    "    gradient_accumulation_steps=4,\n",
    "    #evaluation_strategy=\"steps\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9aee250-b610-49bf-b15d-41bb54ff2c0e",
   "metadata": {},
   "source": [
    "í›ˆë ¨ì„ ìˆ˜í–‰í•˜ê¸° ìœ„í•œ `Trainer` í´ë˜ìŠ¤ë¥¼ ì¸ìŠ¤í„´ìŠ¤í™”í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3fd7b703-aff7-4d90-ae13-2474c64ff303",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cuda_amp half precision backend\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,                         # the instantiated ğŸ¤— Transformers model to be trained\n",
    "    args=training_args,                  # training arguments, defined above\n",
    "    train_dataset=train_dataset,         # training dataset\n",
    "    eval_dataset=val_dataset             # evaluation dataset\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e83de449-0749-4e97-ae08-8e1612559a74",
   "metadata": {},
   "source": [
    "### Training\n",
    "í›ˆë ¨ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤. ë”¥ëŸ¬ë‹ ê¸°ë°˜ ìì—°ì–´ ì²˜ë¦¬ ëª¨ë¸ í›ˆë ¨ì—ëŠ” GPUê°€ í•„ìˆ˜ì´ë©°, ë³¸ê²©ì ì¸ í›ˆë ¨ì„ ìœ„í•´ì„œëŠ” ë©€í‹° GPU ë° ë¶„ì‚° í›ˆë ¨ì„ ê¶Œì¥í•©ë‹ˆë‹¤. ë§Œì•½ ë©€í‹° GPUê°€ ì¥ì°©ë˜ì–´ ìˆë‹¤ë©´ Trainerì—ì„œ ì´ ë°°ì¹˜ í¬ê¸° = ë°°ì¹˜ í¬ê¸° x GPU ê°œìˆ˜ë¡œ ì§€ì •í•œ ë‹¤ìŒ ë°ì´í„° ë³‘ë ¬í™”ë¥¼ ìë™ìœ¼ë¡œ ìˆ˜í–‰í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1558ebc8-325c-4053-9f1c-a713abdfe337",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 60407\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 512\n",
      "  Gradient Accumulation steps = 4\n",
      "  Total optimization steps = 354\n",
      "/home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='354' max='354' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [354/354 17:44, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.463100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.481400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.364300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to chkpt/checkpoint-118\n",
      "Configuration saved in chkpt/checkpoint-118/config.json\n",
      "Model weights saved in chkpt/checkpoint-118/pytorch_model.bin\n",
      "/home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to chkpt/checkpoint-236\n",
      "Configuration saved in chkpt/checkpoint-236/config.json\n",
      "Model weights saved in chkpt/checkpoint-236/pytorch_model.bin\n",
      "/home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to chkpt/checkpoint-354\n",
      "Configuration saved in chkpt/checkpoint-354/config.json\n",
      "Model weights saved in chkpt/checkpoint-354/pytorch_model.bin\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 41min 50s, sys: 5min 59s, total: 47min 50s\n",
      "Wall time: 17min 54s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# train model\n",
    "if get_last_checkpoint(args.chkpt_dir) is not None:\n",
    "    logger.info(\"***** Continue Training *****\")\n",
    "    last_checkpoint = get_last_checkpoint(args.chkpt_dir)\n",
    "    trainer.train(resume_from_checkpoint=last_checkpoint)\n",
    "else:\n",
    "    trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "64101d48-abd8-4c07-82eb-6b04666780c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer config file saved in /home/ec2-user/SageMaker/sm-kornlp-usecases/question-answering/model/tokenizer_config.json\n",
      "Special tokens file saved in /home/ec2-user/SageMaker/sm-kornlp-usecases/question-answering/model/special_tokens_map.json\n",
      "Saving model checkpoint to /home/ec2-user/SageMaker/sm-kornlp-usecases/question-answering/model\n",
      "Configuration saved in /home/ec2-user/SageMaker/sm-kornlp-usecases/question-answering/model/config.json\n",
      "Model weights saved in /home/ec2-user/SageMaker/sm-kornlp-usecases/question-answering/model/pytorch_model.bin\n"
     ]
    }
   ],
   "source": [
    "tokenizer.save_pretrained(args.model_dir)                \n",
    "trainer.save_model(args.model_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aacbc71d-bea9-48c2-bcf2-3b0895668ffc",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## 5. Evaluation\n",
    "---\n",
    "\n",
    "í‰ê°€ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤. ì§ˆì˜ì‘ë‹µ íƒœìŠ¤í¬ì˜ í‰ê°€ëŠ” EM(Exact Match)ì™€ F1 scoreë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.\n",
    "- EM: ì˜ˆì¸¡í•œ ì •ë‹µì˜ í† í°ì´ ì •í™•íˆ ì¼ì¹˜í•˜ë©´ 1, ê·¸ë ‡ì§€ ì•Šìœ¼ë©´ 0\n",
    "- F1: ì˜ˆì¸¡í•œ ì •ë‹µê³¼ ì‹¤ì œ ì •ë‹µì˜ ì¤‘ì²© í† í°ìœ¼ë¡œ ìŠ¤ì½”ì–´ ì‚°ì¶œ\n",
    "\n",
    "Reference: https://qa.fastforwardlabs.com/no%20answer/null%20threshold/bert/distilbert/exact%20match/f1/robust%20predictions/2020/06/09/Evaluating_BERT_on_SQuAD.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5ec00b58-ade4-4547-99fb-dc5cd5d2eb5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 140/140 [00:01<00:00, 116.52it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5774/5774 [01:00<00:00, 96.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{3242478017.py:4} INFO - EM = 0.627641149982681\n",
      "[{3242478017.py:5} INFO - F1 = 0.6994331158010804\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# compute metrics (EM and F1)\n",
    "from scripts.evaluate import get_metrics_korquadv1, get_prediction\n",
    "em, f1 = get_metrics_korquadv1(args.valid_dir, tokenizer, model)\n",
    "logger.info(f\"EM = {em}\")\n",
    "logger.info(f\"F1 = {f1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "526d05ac-5abd-4b56-b06b-9ea1780a05cf",
   "metadata": {},
   "source": [
    "### Example\n",
    "ì—¬ëŸ¬ë¶„ë§Œì˜ ìƒ˜í”Œ ë¬¸ì¥ì„ ë§Œë“¤ì–´ì„œ ììœ ë¡­ê²Œ ì¶”ë¡ ì„ ìˆ˜í–‰í•´ ë³´ì„¸ìš”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8de3787e-1aa7-4457-bffb-131ce58442e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'score': 0.9967060089111328, 'start': 29, 'end': 36, 'answer': 'ì¹´ì¹´ì˜¤ê²Œì„ì¦ˆê°€'}\n",
      "{'score': 0.8259254097938538, 'start': 263, 'end': 269, 'answer': 'ì•„ë§ˆì¡´ S3'}\n",
      "{'score': 0.879924476146698, 'start': 514, 'end': 521, 'answer': 'ì•„ë§ˆì¡´ ì˜¤ë¡œë¼'}\n",
      "{'score': 0.9202089905738831, 'start': 626, 'end': 636, 'answer': 'ì•„ë§ˆì¡´ ì„¸ì´ì§€ë©”ì´ì»¤'}\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "nlp = pipeline(\"question-answering\", model=model, tokenizer=tokenizer, device=0)\n",
    "\n",
    "context = r\"\"\"\n",
    "ì•„ë§ˆì¡´ì›¹ì„œë¹„ìŠ¤(AWS)ëŠ” ì¹´ì¹´ì˜¤ ê²Œì„ ì „ë¬¸ ê³„ì—´ì‚¬ ì¹´ì¹´ì˜¤ê²Œì„ì¦ˆê°€ ìì‚¬ ë¨¸ì‹ ëŸ¬ë‹(ML), ë°ì´í„°ë² ì´ìŠ¤(DB) ë° ë°ì´í„° ë¶„ì„ ë“± ì„œë¹„ìŠ¤ë¥¼ í†µí•´ ì‚¬ìš©ì ê²½í—˜ì„ ì œê³ í–ˆë‹¤ê³  7ì¼ ë°í˜”ë‹¤.\n",
    "AWSëŠ” ì¹´ì¹´ì˜¤ê²Œì„ì¦ˆê°€ AWSí´ë¼ìš°ë“œ ì—­ëŸ‰ì„ í™œìš©í•´ ê²Œì„ ë°ì´í„° ë¶„ì„ ì†”ë£¨ì…˜ì„ ì‹¤í–‰í•˜ê³ , ëŒ€ëŸ‰ì˜ ê²Œì„ ë°ì´í„°ì™€ ì„¤ì¹˜ ê±´ìˆ˜, ì‚¬ìš©ì ìœ ì§€ìœ¨ê³¼ ê°™ì€ ì„±ê³¼ ì§€í‘œë¥¼ ë¶„ì„í•˜ê³  ìˆë‹¤ê³  ì„¤ëª…í–ˆë‹¤. \n",
    "í˜„ì¬ ì¹´ì¹´ì˜¤ê²Œì„ì¦ˆëŠ” í­ì¦í•˜ëŠ” ë°ì´í„°ë¥¼ ì €ì¥Â·ë¶„ì„í•˜ê¸° ìœ„í•œ ë°©ë²•ìœ¼ë¡œ í´ë¼ìš°ë“œ ì˜¤ë¸Œì íŠ¸ ìŠ¤í† ë¦¬ì§€ ì„œë¹„ìŠ¤ 'ì•„ë§ˆì¡´ S3(Amazon Simple Storage Service)' ê¸°ë°˜ ë°ì´í„° ë ˆì´í¬(Data Lake)ë¥¼ êµ¬ì¶•í–ˆë‹¤. ë˜ ë°ì´í„° ë¶„ì„ì„ ìš©ì´í•˜ê²Œ í•´ì£¼ëŠ” ëŒ€í™”í˜• ì¿¼ë¦¬ ì„œë¹„ìŠ¤ 'ì•„ë§ˆì¡´ ì•„í…Œë‚˜(Amazon Athena)'ë¥¼ ë„ì…í•´ ë°ì´í„° ë ˆì´í¬ë¡œë¶€í„° ê²Œì„ ë°ì´í„°ë¥¼ í†µí•©í•˜ê³ , ê²Œì„ ì‚¬ìš©ì í–‰ë™ê³¼ ê´€ë ¨ëœ ì¸ì‚¬ì´íŠ¸ë¥¼ í™•ë³´ ì¤‘ì´ë‹¤. \n",
    "ì´ë¥¼ í†µí•´ ì¹´ì¹´ì˜¤ê²Œì„ì¦ˆëŠ” ê²Œì„ ë´‡ì„ íƒì§€í•˜ê³  ì œê±°í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ ì‚¬ìš©ì ê²½í—˜ì„ ì œê³ í–ˆë‹¤. ë˜í•œ ê´€ê³„í˜• ë°ì´í„°ë² ì´ìŠ¤ ì„œë¹„ìŠ¤ 'ì•„ë§ˆì¡´ ì˜¤ë¡œë¼(Amazon Aurora)'ë¥¼ í™œìš©í•´ ê²Œì„ ë‚´ êµ¬ë§¤ì™€ ê°™ì€ ëŒ€ê·œëª¨ ë°ì´í„°ë² ì´ìŠ¤ ê±°ë˜ë¥¼ ì²˜ë¦¬í•˜ê³  ìˆë‹¤. ì´ë°–ì—ë„ ì¹´ì¹´ì˜¤ê²Œì„ì¦ˆëŠ” ML ëª¨ë¸ êµ¬ì¶•, êµìœ¡ ë° ë°°í¬ë¥¼ ìœ„í•œ ì™„ì „ ê´€ë¦¬í˜• ì„œë¹„ìŠ¤ 'ì•„ë§ˆì¡´ ì„¸ì´ì§€ë©”ì´ì»¤(Amazon SageMaker)'ë¥¼ í™œìš©í•  ì˜ˆì •ì´ë‹¤.\n",
    "\"\"\"\n",
    "\n",
    "print(nlp(question=\"ì¹´ì¹´ì˜¤ ê²Œì„ ì „ë¬¸ ê³„ì—´ì‚¬ëŠ”?\", context=context))\n",
    "print(nlp(question=\"AWSì˜ í´ë¼ìš°ë“œ ì˜¤ë¸Œì íŠ¸ ìŠ¤í† ë¦¬ì§€ ì„œë¹„ìŠ¤ëŠ”?\", context=context))\n",
    "print(nlp(question=\"AWSì˜ ê´€ê³„í˜• ë°ì´í„°ë² ì´ìŠ¤ ì„œë¹„ìŠ¤ëŠ”?\", context=context))\n",
    "print(nlp(question=\"AWSì˜ ML ëª¨ë¸ ì™„ì „ ê´€ë¦¬í˜• ì„œë¹„ìŠ¤ëŠ”?\", context=context))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p38",
   "language": "python",
   "name": "conda_pytorch_p38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
